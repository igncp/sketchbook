var d = diagrams.box.generateDefinition,
    c = diagrams.box.generateContainer,
    s = diagrams.shared.get;

  diagrams.box({
    name: s('project') + ' generated api overview',
    body: [
c("cluster", "/cluster<br>", [
c("tests", "/cluster/tests<br>", [
c("common.py", "/cluster/tests/common.py<br> Common utilities for testing clustering.", [
d("generate_clustered_data(seed, n_clusters, n_features, n_samples_per_cluster, std)"),]),
c("test_affinity_propagation.py", "/cluster/tests/test_affinity_propagation.py<br> Testing for Clustering methods", [
d("test_affinity_propagation()"),
d("test_affinity_propagation_predict()"),
d("test_affinity_propagation_predict_error()"),]),
c("test_bicluster.py", "/cluster/tests/test_bicluster.py<br> Testing for Spectral Biclustering methods", [
c("MockBiclustering(BaseEstimator, BiclusterMixin)", "/cluster/tests/test_bicluster.py<br>", [
d("__init__(self)"),
d("get_indices(self, i)"),]),
d("_do_bistochastic_test(scaled)", "Check that rows and columns sum to the same constant."),
d("_do_scale_test(scaled)", "Check that rows sum to one constant, and columns to another."),
d("_test_shape_indices(model)"),
d("test_bistochastic_normalize()"),
d("test_errors()"),
d("test_fit_best_piecewise()"),
d("test_get_submatrix()"),
d("test_log_normalize()"),
d("test_perfect_checkerboard()"),
d("test_project_and_cluster()"),
d("test_scale_normalize()"),
d("test_spectral_biclustering()"),
d("test_spectral_coclustering()"),]),
c("test_birch.py", "/cluster/tests/test_birch.py<br> Tests for the birch clustering algorithm.", [
d("check_branching_factor(node, branching_factor)"),
d("check_threshold(birch_instance, threshold)", "Use the leaf linked list for traversal"),
d("test_birch_predict()"),
d("test_branching_factor()"),
d("test_n_clusters()"),
d("test_n_samples_leaves_roots()"),
d("test_partial_fit()"),
d("test_sparse_X()"),
d("test_threshold()"),]),
c("test_dbscan.py", "/cluster/tests/test_dbscan.py<br> Tests for DBSCAN clustering algorithm", [
d("test_boundaries()"),
d("test_dbscan_badargs()"),
d("test_dbscan_balltree()"),
d("test_dbscan_callable()"),
d("test_dbscan_core_samples_toy()"),
d("test_dbscan_feature()"),
d("test_dbscan_no_core_samples()"),
d("test_dbscan_precomputed_metric_with_degenerate_input_arrays()"),
d("test_dbscan_similarity()"),
d("test_dbscan_sparse()"),
d("test_input_validation()"),
d("test_pickle()"),
d("test_weighted_dbscan()"),]),
c("test_hierarchical.py", "/cluster/tests/test_hierarchical.py<br> Several basic tests for hierarchical clustering procedures", [
d("assess_same_labelling(cut1, cut2)", "Util for comparison with scipy"),
d("test_agg_n_clusters()"),
d("test_agglomerative_clustering()"),
d("test_compute_full_tree()"),
d("test_connectivity_callable()"),
d("test_connectivity_fixing_non_lil()"),
d("test_connectivity_ignores_diagonal()"),
d("test_connectivity_propagation()"),
d("test_height_linkage_tree()"),
d("test_int_float_dict()"),
d("test_linkage_misc()"),
d("test_n_components()"),
d("test_scikit_vs_scipy()"),
d("test_structured_linkage_tree()"),
d("test_unstructured_linkage_tree()"),
d("test_ward_agglomeration()"),
d("test_ward_linkage_tree_return_distance()"),
d("test_ward_tree_children_order()"),]),
c("test_k_means.py", "/cluster/tests/test_k_means.py<br> Testing for K-means", [
d("_check_fitted_model(km)"),
d("_has_blas_lib(libname)"),
d("test_fit_transform()"),
d("test_input_dtypes()"),
d("test_k_means_copyx()"),
d("test_k_means_fortran_aligned_data()"),
d("test_k_means_function()"),
d("test_k_means_invalid_init()"),
d("test_k_means_n_init()"),
d("test_k_means_new_centers()"),
d("test_k_means_non_collapsed()"),
d("test_k_means_perfect_init()"),
d("test_k_means_plus_plus_init()"),
d("test_k_means_plus_plus_init_2_jobs()"),
d("test_k_means_plus_plus_init_not_precomputed()"),
d("test_k_means_plus_plus_init_sparse()"),
d("test_k_means_precompute_distances_flag()"),
d("test_k_means_random_init()"),
d("test_k_means_random_init_not_precomputed()"),
d("test_k_means_random_init_sparse()"),
d("test_kmeans_dtype()"),
d("test_labels_assignment_and_inertia()"),
d("test_mb_k_means_plus_plus_init_dense_array()"),
d("test_mb_k_means_plus_plus_init_sparse_matrix()"),
d("test_mb_kmeans_verbose()"),
d("test_mini_batch_k_means_random_init_partial_fit()"),
d("test_mini_match_k_means_invalid_init()"),
d("test_minibatch_default_init_size()"),
d("test_minibatch_init_with_large_k()"),
d("test_minibatch_k_means_init_multiple_runs_with_explicit_centers()"),
d("test_minibatch_k_means_perfect_init_dense_array()"),
d("test_minibatch_k_means_perfect_init_sparse_csr()"),
d("test_minibatch_k_means_random_init_dense_array()"),
d("test_minibatch_k_means_random_init_sparse_csr()"),
d("test_minibatch_reassign()"),
d("test_minibatch_sensible_reassign_fit()"),
d("test_minibatch_sensible_reassign_partial_fit()"),
d("test_minibatch_set_init_size()"),
d("test_minibatch_tol()"),
d("test_minibatch_update_consistency()"),
d("test_minibatch_with_many_reassignments()"),
d("test_n_init()"),
d("test_predict()"),
d("test_predict_minibatch_dense_input()"),
d("test_predict_minibatch_kmeanspp_init_sparse_input()"),
d("test_predict_minibatch_random_init_sparse_input()"),
d("test_score()"),
d("test_sparse_mb_k_means_callable_init()"),
d("test_transform()"),]),
c("test_mean_shift.py", "/cluster/tests/test_mean_shift.py<br> Testing for mean shift clustering methods", [
d("test_bin_seeds()"),
d("test_estimate_bandwidth()"),
d("test_mean_shift()"),
d("test_meanshift_all_orphans()"),
d("test_meanshift_predict()"),
d("test_unfitted()"),]),
c("test_spectral.py", "/cluster/tests/test_spectral.py<br> Testing for Spectral Clustering methods", [
d("test_affinities()"),
d("test_discretize(seed)"),
d("test_spectral_amg_mode()"),
d("test_spectral_clustering()"),
d("test_spectral_clustering_sparse()"),
d("test_spectral_unknown_assign_labels()"),
d("test_spectral_unknown_mode()"),]),]),
c("affinity_propagation_.py", "/cluster/affinity_propagation_.py<br> Algorithms for clustering : Meanshift,  Affinity propagation and spectral clustering.", [
c("AffinityPropagation(BaseEstimator, ClusterMixin)", "/cluster/affinity_propagation_.py<br>Perform Affinity Propagation Clustering of data.  Read more in the :ref:`User Guide <affinity_propagation>`.  Parameters ---------- damping : float, optional, default: 0.5     Damping factor between 0.5 and 1.  convergence_iter : int, optional, default: 15     Number of iterations with no change in ...", [
d("__init__(self, damping, max_iter, convergence_iter, copy, preference, affinity, verbose)"),
d("_pairwise(self)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
d("affinity_propagation(S, preference, convergence_iter, max_iter, damping, copy, verbose, return_n_iter)", "Perform Affinity Propagation Clustering of data  Read more in the :ref:`User Guide <affinity_propagation>`.  Parameters ----------  S : array-like, shape (n_samples, n_samples)     Matrix of similarities between points  preference : array-like, shape (n_samples,) or float, optional     Preferences f..."),]),
c("bicluster.py", "/cluster/bicluster.py<br> Spectral biclustering algorithms.  Authors : Kemal Eren License: BSD 3 clause", [
c("BaseSpectral()", "/cluster/bicluster.py<br>Base class for spectral biclustering.", [
d("__init__(self, n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, n_jobs, random_state)"),
d("_check_parameters(self)"),
d("_k_means(self, data, n_clusters)"),
d("_svd(self, array, n_components, n_discard)"),
d("fit(self, X)"),]),
c("SpectralBiclustering(BaseSpectral)", "/cluster/bicluster.py<br>Spectral biclustering (Kluger, 2003).  Partitions rows and columns under the assumption that the data has an underlying checkerboard structure. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two bicl...", [
d("__init__(self, n_clusters, method, n_components, n_best, svd_method, n_svd_vecs, mini_batch, init, n_init, n_jobs, random_state)"),
d("_check_parameters(self)"),
d("_fit(self, X)"),
d("_fit_best_piecewise(self, vectors, n_best, n_clusters)"),
d("_project_and_cluster(self, data, vectors, n_clusters)"),]),
c("SpectralCoclustering(BaseSpectral)", "/cluster/bicluster.py<br>Spectral Co-Clustering algorithm (Dhillon, 2001).  Clusters rows and columns of an array `X` to solve the relaxed normalized cut of the bipartite graph created from `X` as follows: the edge between row vertex `i` and column vertex `j` has weight `X[i, j]`.  The resulting bicluster structure is block...", [
d("__init__(self, n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, n_jobs, random_state)"),
d("_fit(self, X)"),]),
d("_bistochastic_normalize(X, max_iter, tol)", "Normalize rows and columns of ``X`` simultaneously so that all rows sum to one constant and all columns sum to a different constant."),
d("_log_normalize(X)", "Normalize ``X`` according to Kluger's log-interactions scheme."),
d("_scale_normalize(X)", "Normalize ``X`` by scaling rows and columns independently.  Returns the normalized matrix and the row and column scaling factors."),]),
c("birch.py", "/cluster/birch.py<br>", [
c("Birch(BaseEstimator, TransformerMixin, ClusterMixin)", "/cluster/birch.py<br>Implements the Birch clustering algorithm.  Every new sample is inserted into the root of the Clustering Feature Tree. It is then clubbed together with the subcluster that has the centroid closest to the new sample. This is done recursively till it ends up at the subcluster of the leaf of the tree h...", [
d("__init__(self, threshold, branching_factor, n_clusters, compute_labels, copy)"),
d("_check_fit(self, X)"),
d("_fit(self, X)"),
d("_get_leaves(self)"),
d("_global_clustering(self, X)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("predict(self, X)"),
d("transform(self, X, y)"),]),
c("_CFNode(object)", "/cluster/birch.py<br>Each node in a CFTree is called a CFNode.  The CFNode can have a maximum of branching_factor number of CFSubclusters.  Parameters ---------- threshold : float     Threshold needed for a new subcluster to enter a CFSubcluster.  branching_factor : int     Maximum number of CF subclusters in each node....", [
d("__init__(self, threshold, branching_factor, is_leaf, n_features)"),
d("append_subcluster(self, subcluster)"),
d("insert_cf_subcluster(self, subcluster)"),
d("update_split_subclusters(self, subcluster, new_subcluster1, new_subcluster2)"),]),
c("_CFSubcluster(object)", "/cluster/birch.py<br>Each subcluster in a CFNode is called a CFSubcluster.  A CFSubcluster can have a CFNode has its child.  Parameters ---------- linear_sum : ndarray, shape (n_features,), optional     Sample. This is kept optional to allow initialization of empty     subclusters.  Attributes ---------- n_samples_ : in...", [
d("__init__(self, linear_sum)"),
d("merge_subcluster(self, nominee_cluster, threshold)"),
d("radius(self)"),
d("update(self, subcluster)"),]),
d("_iterate_sparse_X(X)", "This little hack returns a densified row when iterating over a sparse matrix, insted of constructing a sparse matrix for every row that is expensive."),
d("_split_node(node, threshold, branching_factor)", "The node has to be split if there is no place for a new subcluster in the node. 1. Two empty nodes and two empty subclusters are initialized. 2. The pair of distant subclusters are found. 3. The properties of the empty subclusters and nodes are updated    according to the nearest distance between th..."),]),
c("dbscan_.py", "/cluster/dbscan_.py<br> DBSCAN: Density-Based Spatial Clustering of Applications with Noise", [
c("DBSCAN(BaseEstimator, ClusterMixin)", "/cluster/dbscan_.py<br>Perform DBSCAN clustering from vector array or distance matrix.  DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.  Read more in the :ref:`User Guide <dbsc...", [
d("__init__(self, eps, min_samples, metric, algorithm, leaf_size, p, random_state)"),
d("fit(self, X, y, sample_weight)"),
d("fit_predict(self, X, y, sample_weight)"),]),
d("dbscan(X, eps, min_samples, metric, algorithm, leaf_size, p, sample_weight, random_state)", "Perform DBSCAN clustering from vector array or distance matrix.  Read more in the :ref:`User Guide <dbscan>`.  Parameters ---------- X : array or sparse (CSR) matrix of shape (n_samples, n_features), or             array of shape (n_samples, n_samples)     A feature array, or array of distances betw..."),]),
c("hierarchical.py", "/cluster/hierarchical.py<br> Hierarchical Agglomerative Clustering  These routines perform some hierarchical agglomerative clustering of some input data.  Authors : Vincent Michel, Bertrand Thirion, Alexandre Gramfort,           Gael Varoquaux License: BSD 3 clause", [
c("AgglomerativeClustering(BaseEstimator, ClusterMixin)", "/cluster/hierarchical.py<br>Agglomerative Clustering  Recursively merges the pair of clusters that minimally increases a given linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int, default2     The number of clusters to find.  connectivity : array-like or call...", [
d("__init__(self, n_clusters, affinity, memory, connectivity, n_components, compute_full_tree, linkage, pooling_func)"),
d("fit(self, X, y)"),]),
c("FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform)", "/cluster/hierarchical.py<br>Agglomerate features.  Similar to AgglomerativeClustering, but recursively merges features instead of samples.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int, default 2     The number of clusters to find.  connectivity : array-like or callable,...", [
d("fit(self, X, y)"),
d("fit_predict(self)"),]),
d("_average_linkage()"),
d("_complete_linkage()"),
d("_fix_connectivity(X, connectivity, n_components, affinity)", "Fixes the connectivity matrix      - copies it     - makes it symmetric     - converts it to LIL if necessary     - completes it if necessary"),
d("_hc_cut(n_clusters, children, n_leaves)", "Function cutting the ward tree for a given number of clusters.  Parameters ---------- n_clusters : int or ndarray     The number of clusters to form.  children : list of pairs. Length of n_nodes     The children of each non-leaf node. Values less than `n_samples` refer     to leaves of the tree. A g..."),
d("linkage_tree(X, connectivity, n_components, n_clusters, linkage, affinity, return_distance)", "Linkage agglomerative clustering based on a Feature matrix.  The inertia matrix uses a Heapq-based representation.  This is the structured version, that takes into account some topological structure between samples.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------..."),
d("ward_tree(X, connectivity, n_components, n_clusters, return_distance)", "Ward clustering based on a Feature matrix.  Recursively merges the pair of clusters that minimally increases within-cluster variance.  The inertia matrix uses a Heapq-based representation.  This is the structured version, that takes into account some topological structure between samples.  Read more..."),]),
c("k_means_.py", "/cluster/k_means_.py<br> K-means clustering", [
c("KMeans(BaseEstimator, ClusterMixin, TransformerMixin)", "/cluster/k_means_.py<br>K-Means clustering  Read more in the :ref:`User Guide <k_means>`.  Parameters ----------  n_clusters : int, optional, default: 8     The number of clusters to form as well as the number of     centroids to generate.  max_iter : int, default: 300     Maximum number of iterations of the k-means algori...", [
d("__init__(self, n_clusters, init, n_init, max_iter, tol, precompute_distances, verbose, random_state, copy_x, n_jobs)"),
d("_check_fit_data(self, X)"),
d("_check_test_data(self, X)"),
d("_transform(self, X)"),
d("fit(self, X, y)"),
d("fit_predict(self, X, y)"),
d("fit_transform(self, X, y)"),
d("predict(self, X)"),
d("score(self, X, y)"),
d("transform(self, X, y)"),]),
c("MiniBatchKMeans(KMeans)", "/cluster/k_means_.py<br>Mini-Batch K-Means clustering  Parameters ----------  n_clusters : int, optional, default: 8     The number of clusters to form as well as the number of     centroids to generate.  max_iter : int, optional     Maximum number of iterations over the complete dataset before     stopping independently o...", [
d("__init__(self, n_clusters, init, max_iter, batch_size, verbose, compute_labels, random_state, tol, max_no_improvement, init_size, n_init, reassignment_ratio)"),
d("_labels_inertia_minibatch(self, X)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("predict(self, X)"),]),
d("_init_centroids(X, k, init, random_state, x_squared_norms, init_size)", "Compute the initial centroids  Parameters ----------  X: array, shape (n_samples, n_features)  k: int     number of centroids  init: {'k-means++', 'random' or ndarray or callable} optional     Method for initialization  random_state: integer or numpy.RandomState, optional     The generator used to i..."),
d("_k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials)", "Init n_clusters seeds according to k-means++  Parameters ----------- X: array or sparse matrix, shape (n_samples, n_features)     The data to pick seeds for. To avoid memory copy, the input data     should be double precision (dtypenp.float64).  n_clusters: integer     The number of seeds to choose ..."),
d("_kmeans_single(X, n_clusters, x_squared_norms, max_iter, init, verbose, random_state, tol, precompute_distances)", "A single run of k-means, assumes preparation completed prior.  Parameters ---------- X: array-like of floats, shape (n_samples, n_features)     The observations to cluster.  n_clusters: int     The number of clusters to form as well as the number of     centroids to generate.  max_iter: int, optiona..."),
d("_labels_inertia(X, x_squared_norms, centers, precompute_distances, distances)", "E step of the K-means EM algorithm.  Compute the labels and the inertia of the given samples and centers. This will compute the distances in-place.  Parameters ---------- X: float64 array-like or CSR sparse matrix, shape (n_samples, n_features)     The input samples to assign to the labels.  x_squar..."),
d("_labels_inertia_precompute_dense(X, x_squared_norms, centers, distances)", "Compute labels and inertia using a full distance matrix.  This will overwrite the 'distances' array in-place.  Parameters ---------- X : numpy array, shape (n_sample, n_features)     Input data.  x_squared_norms : numpy array, shape (n_samples,)     Precomputed squared norms of X.  centers : numpy a..."),
d("_mini_batch_convergence(model, iteration_idx, n_iter, tol, n_samples, centers_squared_diff, batch_inertia, context, verbose)", "Helper function to encapsulte the early stopping logic"),
d("_mini_batch_step(X, x_squared_norms, centers, counts, old_center_buffer, compute_squared_diff, distances, random_reassign, random_state, reassignment_ratio, verbose)", "Incremental update of the centers for the Minibatch K-Means algorithm.  Parameters ----------  X : array, shape (n_samples, n_features)     The original data array.  x_squared_norms : array, shape (n_samples,)     Squared euclidean norm of each data point.  centers : array, shape (k, n_features)    ..."),
d("_tolerance(X, tol)", "Return a tolerance which is independent of the dataset"),
d("k_means(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, return_n_iter)", "K-means clustering algorithm.  Read more in the :ref:`User Guide <k_means>`.  Parameters ---------- X : array-like or sparse matrix, shape (n_samples, n_features)     The observations to cluster.  n_clusters : int     The number of clusters to form as well as the number of     centroids to generate...."),]),
c("mean_shift_.py", "/cluster/mean_shift_.py<br> Mean shift clustering algorithm.  Mean shift clustering aims to discover *blobs* in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-process...", [
c("MeanShift(BaseEstimator, ClusterMixin)", "/cluster/mean_shift_.py<br>Mean shift clustering using a flat kernel.  Mean shift clustering aims to discover 'blobs' in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a po...", [
d("__init__(self, bandwidth, seeds, bin_seeding, min_bin_freq, cluster_all)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
d("estimate_bandwidth(X, quantile, n_samples, random_state)", "Estimate the bandwidth to use with the mean-shift algorithm.  That this function takes time at least quadratic in n_samples. For large datasets, it's wise to set that parameter to a small value.  Parameters ---------- X : array-like, shape[n_samples, n_features]     Input points.  quantile : float, ..."),
d("get_bin_seeds(X, bin_size, min_bin_freq)", "Finds seeds for mean_shift.  Finds seeds by first binning data onto a grid whose lines are spaced bin_size apart, and then choosing those bins with at least min_bin_freq points.  Parameters ----------  X : array-like, shape[n_samples, n_features]     Input points, the same points that will be used i..."),
d("mean_shift(X, bandwidth, seeds, bin_seeding, min_bin_freq, cluster_all, max_iter, max_iterations)", "Perform mean shift clustering of data using a flat kernel.  Read more in the :ref:`User Guide <mean_shift>`.  Parameters ----------  X : array-like, shape[n_samples, n_features]     Input data.  bandwidth : float, optional     Kernel bandwidth.      If bandwidth is not given, it is determined using ..."),]),
c("setup.py", "/cluster/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("spectral.py", "/cluster/spectral.py<br> Algorithms for spectral clustering", [
c("SpectralClustering(BaseEstimator, ClusterMixin)", "/cluster/spectral.py<br>Apply clustering to a projection to the normalized laplacian.  In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete clus...", [
d("__init__(self, n_clusters, eigen_solver, random_state, n_init, gamma, affinity, n_neighbors, eigen_tol, assign_labels, degree, coef0, kernel_params)"),
d("_pairwise(self)"),
d("fit(self, X, y)"),]),
d("discretize(vectors, copy, max_svd_restarts, n_iter_max, random_state)", "Search for a partition matrix (clustering) which is closest to the eigenvector embedding.  Parameters ---------- vectors : array-like, shape: (n_samples, n_clusters)     The embedding space of the samples.  copy : boolean, optional, default: True     Whether to copy vectors, or perform in-place norm..."),
d("spectral_clustering(affinity, n_clusters, n_components, eigen_solver, random_state, n_init, eigen_tol, assign_labels)", "Apply clustering to a projection to the normalized laplacian.  In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete clus..."),]),
c("_feature_agglomeration.py", "/cluster/_feature_agglomeration.py<br> Feature agglomeration. Base classes and functions for performing feature agglomeration.", [
c("AgglomerationTransform(TransformerMixin)", "/cluster/_feature_agglomeration.py<br>A class for feature agglomeration via the transform interface", [
d("inverse_transform(self, Xred)"),
d("transform(self, X, pooling_func)"),]),]),]),
c("covariance", "/covariance<br>", [
c("tests", "/covariance/tests<br>", [
c("test_covariance.py", "/covariance/tests/test_covariance.py<br>", [
d("test_covariance()"),
d("test_ledoit_wolf()"),
d("test_ledoit_wolf_large()"),
d("test_oas()"),
d("test_shrunk_covariance()"),]),
c("test_graph_lasso.py", "/covariance/tests/test_graph_lasso.py<br> Test the graph_lasso module.", [
d("test_graph_lasso(random_state)"),
d("test_graph_lasso_cv(random_state)"),
d("test_graph_lasso_iris()"),
d("test_graph_lasso_iris_singular()"),]),
c("test_robust_covariance.py", "/covariance/tests/test_robust_covariance.py<br>", [
d("launch_mcd_on_dataset(n_samples, n_features, n_outliers, tol_loc, tol_cov, tol_support)"),
d("test_mcd()"),
d("test_mcd_issue1127()"),
d("test_outlier_detection()"),]),]),
c("empirical_covariance_.py", "/covariance/empirical_covariance_.py<br> Maximum likelihood covariance estimator.", [
c("EmpiricalCovariance(BaseEstimator)", "/covariance/empirical_covariance_.py<br>Maximum likelihood covariance estimator  Read more in the :ref:`User Guide <covariance>`.  Parameters ---------- store_precision : bool     Specifies if the estimated precision is stored.  assume_centered : bool     If True, data are not centered before computation.     Useful when working with data...", [
d("__init__(self, store_precision, assume_centered)"),
d("_set_covariance(self, covariance)"),
d("error_norm(self, comp_cov, norm, scaling, squared)"),
d("fit(self, X, y)"),
d("get_precision(self)"),
d("mahalanobis(self, observations)"),
d("score(self, X_test, y)"),]),
d("empirical_covariance(X, assume_centered)", "Computes the Maximum likelihood covariance estimator   Parameters ---------- X : ndarray, shape (n_samples, n_features)     Data from which to compute the covariance estimate  assume_centered : Boolean     If True, data are not centered before computation.     Useful when working with data whose mea..."),
d("log_likelihood(emp_cov, precision)", "Computes the sample mean of the log_likelihood under a covariance model  computes the empirical expected log-likelihood (accounting for the normalization terms and scaling), allowing for universal comparison (beyond this software package)  Parameters ---------- emp_cov : 2D ndarray (n_features, n_fe..."),]),
c("graph_lasso_.py", "/covariance/graph_lasso_.py<br> GraphLasso: sparse inverse covariance estimation with an l1-penalized estimator.", [
c("GraphLasso(EmpiricalCovariance)", "/covariance/graph_lasso_.py<br>Sparse inverse covariance estimation with an l1-penalized estimator.  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  Parameters ---------- alpha : positive float, default 0.01     The regularization parameter: the higher alpha, the more     regularization, the sparser the inverse c...", [
d("__init__(self, alpha, mode, tol, enet_tol, max_iter, verbose, assume_centered)"),
d("fit(self, X, y)"),]),
c("GraphLassoCV(GraphLasso)", "/covariance/graph_lasso_.py<br>Sparse inverse covariance w/ cross-validated choice of the l1 penalty  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  Parameters ---------- alphas : integer, or list positive float, optional     If an integer is given, it fixes the number of points on the     grids of alpha to be u...", [
d("__init__(self, alphas, n_refinements, cv, tol, enet_tol, max_iter, mode, n_jobs, verbose, assume_centered)"),
d("fit(self, X, y)"),]),
d("_dual_gap(emp_cov, precision_, alpha)", "Expression of the dual gap convergence criterion  The specific definition is given in Duchi 'Projected Subgradient Methods for Learning Sparse Gaussians'."),
d("_objective(mle, precision_, alpha)", "Evaluation of the graph-lasso objective function  the objective function is made of a shifted scaled version of the normalized log-likelihood (i.e. its empirical mean over the samples) and a penalisation term to promote sparsity"),
d("alpha_max(emp_cov)", "Find the maximum alpha for which there are some non-zeros off-diagonal.  Parameters ---------- emp_cov : 2D array, (n_features, n_features)     The sample covariance matrix  Notes -----  This results from the bound for the all the Lasso that are solved in GraphLasso: each time, the row of cov corres..."),
d("graph_lasso(emp_cov, alpha, cov_init, mode, tol, enet_tol, max_iter, verbose, return_costs, eps, return_n_iter)", "l1-penalized covariance estimator  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  Parameters ---------- emp_cov : 2D ndarray, shape (n_features, n_features)     Empirical covariance from which to compute the covariance estimate.  alpha : positive float     The regularization parame..."),
d("graph_lasso_path(X, alphas, cov_init, X_test, mode, tol, enet_tol, max_iter, verbose)", "l1-penalized covariance estimator along a path of decreasing alphas  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  Parameters ---------- X : 2D ndarray, shape (n_samples, n_features)     Data from which to compute the covariance estimate.  alphas : list of positive floats     The ..."),]),
c("outlier_detection.py", "/covariance/outlier_detection.py<br> Class for outlier detection.  This class provides a framework for outlier detection. It consists in several methods that can be added to a covariance estimator in order to assess the outlying-ness of the observations of a data set. Such a 'outlier detector' object is proposed constructed from a robu...", [
c("EllipticEnvelope(ClassifierMixin, OutlierDetectionMixin, MinCovDet)", "/covariance/outlier_detection.py<br>An object for detecting outliers in a Gaussian distributed dataset.  Read more in the :ref:`User Guide <outlier_detection>`.  Attributes ---------- `contamination` : float, 0. < contamination < 0.5   The amount of contamination of the data set, i.e. the proportion of       outliers in the data set. ...", [
d("__init__(self, store_precision, assume_centered, support_fraction, contamination, random_state)"),
d("fit(self, X, y)"),]),
c("OutlierDetectionMixin(object)", "/covariance/outlier_detection.py<br>Set of methods for outliers detection with covariance estimators.  Parameters ---------- contamination : float, 0. < contamination < 0.5     The amount of contamination of the data set, i.e. the proportion     of outliers in the data set.  Notes ----- Outlier detection from covariance estimation may...", [
d("__init__(self, contamination)"),
d("decision_function(self, X, raw_values)"),
d("predict(self, X)"),
d("threshold(self)"),]),]),
c("robust_covariance.py", "/covariance/robust_covariance.py<br> Robust location and covariance estimators.  Here are implemented estimators that are resistant to outliers.", [
c("MinCovDet(EmpiricalCovariance)", "/covariance/robust_covariance.py<br>Minimum Covariance Determinant (MCD): robust estimator of covariance.  The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal...", [
d("__init__(self, store_precision, assume_centered, support_fraction, random_state)"),
d("correct_covariance(self, data)"),
d("fit(self, X, y)"),
d("reweight_covariance(self, data)"),]),
d("_c_step(X, n_support, random_state, remaining_iterations, initial_estimates, verbose, cov_computation_method)"),
d("c_step(X, n_support, remaining_iterations, initial_estimates, verbose, cov_computation_method, random_state)", "C_step procedure described in [Rouseeuw1984]_ aiming at computing MCD.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Data set in which we look for the n_support observations whose     scatter matrix has minimum determinant.  n_support : int, > n_samples / 2     Number of o..."),
d("fast_mcd(X, support_fraction, cov_computation_method, random_state)", "Estimates the Minimum Covariance Determinant matrix.  Read more in the :ref:`User Guide <robust_covariance>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)   The data matrix, with p features and n samples.  support_fraction : float, 0 < support_fraction < 1       The proportio..."),
d("select_candidates(X, n_support, n_trials, select, n_iter, verbose, cov_computation_method, random_state)", "Finds the best pure subset of observations to compute MCD from it.  The purpose of this function is to find the best sets of n_support observations with respect to a minimization of their covariance matrix determinant. Equivalently, it removes n_samples-n_support observations to construct what we ca..."),]),
c("shrunk_covariance_.py", "/covariance/shrunk_covariance_.py<br> Covariance estimators using shrinkage.  Shrinkage corresponds to regularising `cov` using a convex combination: shrunk_cov  (1-shrinkage)*cov + shrinkage*structured_estimate.", [
c("LedoitWolf(EmpiricalCovariance)", "/covariance/shrunk_covariance_.py<br>LedoitWolf Estimator  Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf's formula as described in 'A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices', Ledoit and Wolf, Journal of Multivariate Analysis, Volume 8...", [
d("__init__(self, store_precision, assume_centered, block_size)"),
d("fit(self, X, y)"),]),
c("OAS(EmpiricalCovariance)", "/covariance/shrunk_covariance_.py<br>Oracle Approximating Shrinkage Estimator  Read more in the :ref:`User Guide <shrunk_covariance>`.  OAS is a particular form of shrinkage described in 'Shrinkage Algorithms for MMSE Covariance Estimation' Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.  The formula used he...", [
d("fit(self, X, y)"),]),
c("ShrunkCovariance(EmpiricalCovariance)", "/covariance/shrunk_covariance_.py<br>Covariance estimator with shrinkage  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- store_precision : boolean, default True     Specify if the estimated precision is stored  shrinkage : float, 0 < shrinkage < 1, default 0.1     Coefficient in the convex combination us...", [
d("__init__(self, store_precision, assume_centered, shrinkage)"),
d("fit(self, X, y)"),]),
d("ledoit_wolf(X, assume_centered, block_size)", "Estimates the shrunk Ledoit-Wolf covariance matrix.  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Data from which to compute the covariance estimate  assume_centered : boolean, defaultFalse     If True, data are not ..."),
d("ledoit_wolf_shrinkage(X, assume_centered, block_size)", "Estimates the shrunk Ledoit-Wolf covariance matrix.  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.  assume_centered : Boolean     If True, data a..."),
d("oas(X, assume_centered)", "Estimate covariance with the Oracle Approximating Shrinkage algorithm.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Data from which to compute the covariance estimate.  assume_centered : boolean   If True, data are not centered before computation.   Useful to work with da..."),
d("shrunk_covariance(emp_cov, shrinkage)", "Calculates a covariance matrix shrunk on the diagonal  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- emp_cov : array-like, shape (n_features, n_features)     Covariance matrix to be shrunk  shrinkage : float, 0 < shrinkage < 1     Coefficient in the convex combinatio..."),]),]),
c("cross_decomposition", "/cross_decomposition<br>", [
c("tests", "/cross_decomposition/tests<br>", [
c("test_pls.py", "/cross_decomposition/tests/test_pls.py<br>", [
d("test_PLSSVD()"),
d("test_pls()"),
d("test_pls_errors()"),
d("test_predict_transform_copy()"),
d("test_scale()"),
d("test_univariate_pls_regression()"),]),]),
c("cca_.py", "/cross_decomposition/cca_.py<br>", [
c("CCA(_PLS)", "/cross_decomposition/cca_.py<br>CCA Canonical Correlation Analysis.  CCA inherits from PLS with mode'B' and deflation_mode'canonical'.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- n_components : int, (default 2).     number of components to keep.  scale : boolean, (default True)     whether to ...", [
d("__init__(self, n_components, scale, max_iter, tol, copy)"),]),]),
c("pls_.py", "/cross_decomposition/pls_.py<br> The sklearn.pls module implements Partial Least Squares (PLS).", [
c("PLSCanonical(_PLS)", "/cross_decomposition/pls_.py<br>PLSCanonical implements the 2 blocks canonical PLS of the original Wold algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].  This class inherits from PLS with mode'A' and deflation_mode'canonical', norm_y_weightsTrue and algorithm'nipals', but svd should provide similar results ...", [
d("__init__(self, n_components, scale, algorithm, max_iter, tol, copy)"),]),
c("PLSRegression(_PLS)", "/cross_decomposition/pls_.py<br>PLS regression  PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1 in case of one dimensional response. This class inherits from _PLS with mode'A', deflation_mode'regression', norm_y_weightsFalse and algorithm'nipals'.  Read more in the :ref:`User Guide <cross_decomposition>`...", [
d("__init__(self, n_components, scale, max_iter, tol, copy)"),]),
c("PLSSVD(BaseEstimator, TransformerMixin)", "/cross_decomposition/pls_.py<br>Partial Least Square SVD  Simply perform a svd on the crosscovariance matrix: X'Y There are no iterative deflation here.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- n_components : int, default 2     Number of components to keep.  scale : boolean, default True   ...", [
d("__init__(self, n_components, scale, copy)"),
d("fit(self, X, Y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X, Y)"),]),
c("_PLS(BaseEstimator, TransformerMixin, RegressorMixin)", "/cross_decomposition/pls_.py<br>Partial Least Squares (PLS)  This class implements the generic PLS algorithm, constructors' parameters allow to obtain a specific implementation such as:  - PLS2 regression, i.e., PLS 2 blocks, mode A, with asymmetric deflation   and unnormalized y weights such as defined by [Tenenhaus 1998] p. 132....", [
d("__init__(self, n_components, scale, deflation_mode, mode, algorithm, norm_y_weights, max_iter, tol, copy)"),
d("fit(self, X, Y)"),
d("fit_transform(self, X, y)"),
d("predict(self, X, copy)"),
d("transform(self, X, Y, copy)"),]),
d("_center_scale_xy(X, Y, scale)", "Center X, Y and scale if the scale parameterTrue  Returns -------     X, Y, x_mean, y_mean, x_std, y_std"),
d("_nipals_twoblocks_inner_loop(X, Y, mode, max_iter, tol, norm_y_weights)", "Inner loop of the iterative NIPALS algorithm.  Provides an alternative to the svd(X'Y); returns the first left and right singular vectors of X'Y.  See PLS for the meaning of the parameters.  It is similar to the Power method for determining the eigenvectors and eigenvalues of a X'Y."),
d("_svd_cross_product(X, Y)"),]),]),
c("datasets", "/datasets<br>", [
c("tests", "/datasets/tests<br>", [
c("test_20news.py", "/datasets/tests/test_20news.py<br> Test the 20news downloader, if the data is available.", [
d("test_20news()"),
d("test_20news_length_consistency()", "Checks the length consistencies within the bunch  This is a non-regression test for a bug present in 0.16.1."),
d("test_20news_vectorized()"),]),
c("test_base.py", "/datasets/tests/test_base.py<br>", [
d("_remove_dir(path)"),
d("setup_load_files()"),
d("teardown_load_files()"),
d("teardown_module()", "Test fixture (clean up) run once after all tests of this module"),
d("test_data_home()"),
d("test_default_empty_load_files()"),
d("test_default_load_files()"),
d("test_load_boston()"),
d("test_load_diabetes()"),
d("test_load_digits()"),
d("test_load_digits_n_class_lt_10()"),
d("test_load_files_w_categories_desc_and_encoding()"),
d("test_load_files_wo_load_content()"),
d("test_load_iris()"),
d("test_load_linnerud()"),
d("test_load_missing_sample_image_error()"),
d("test_load_sample_image()"),
d("test_load_sample_images()"),
d("test_loads_dumps_bunch()"),]),
c("test_covtype.py", "/datasets/tests/test_covtype.py<br> Test the covtype loader.  Skipped if covtype is not already downloaded to data_home.", [
d("fetch()"),
d("test_fetch()"),]),
c("test_lfw.py", "/datasets/tests/test_lfw.py<br> This test for the LFW require medium-size data dowloading and processing  If the data has not been already downloaded by running the examples, the tests won't run (skipped).  If the test are run, the first execution will be long (typically a bit more than a couple of minutes) but as the dataset load...", [
d("setup_module()", "Test fixture run once and common to all tests of this module"),
d("teardown_module()", "Test fixture (clean up) run once after all tests of this module"),
d("test_load_empty_lfw_pairs()"),
d("test_load_empty_lfw_people()"),
d("test_load_fake_lfw_pairs()"),
d("test_load_fake_lfw_people()"),
d("test_load_fake_lfw_people_too_restrictive()"),
d("test_load_lfw_pairs_deprecation()"),
d("test_load_lfw_people_deprecation()"),]),
c("test_mldata.py", "/datasets/tests/test_mldata.py<br> Test functionality of mldata fetching utilities.", [
d("setup_tmpdata()"),
d("teardown_tmpdata()"),
d("test_download()", "Test that fetch_mldata is able to download and cache a data set."),
d("test_fetch_multiple_column()"),
d("test_fetch_one_column()"),
d("test_mldata_filename()"),]),
c("test_rcv1.py", "/datasets/tests/test_rcv1.py<br> Test the rcv1 loader.  Skipped if rcv1 is not already downloaded to data_home.", [
d("test_fetch_rcv1()"),]),
c("test_samples_generator.py", "/datasets/tests/test_samples_generator.py<br>", [
d("test_make_biclusters()"),
d("test_make_blobs()"),
d("test_make_checkerboard()"),
d("test_make_classification()"),
d("test_make_classification_informative_features()", "Test the construction of informative features in make_classification  Also tests `n_clusters_per_class`, `n_classes`, `hypercube` and fully-specified `weights`."),
d("test_make_friedman1()"),
d("test_make_friedman2()"),
d("test_make_friedman3()"),
d("test_make_hastie_10_2()"),
d("test_make_low_rank_matrix()"),
d("test_make_multilabel_classification_return_indicator()"),
d("test_make_multilabel_classification_return_sequences()"),
d("test_make_regression()"),
d("test_make_regression_multitarget()"),
d("test_make_s_curve()"),
d("test_make_sparse_coded_signal()"),
d("test_make_sparse_uncorrelated()"),
d("test_make_spd_matrix()"),
d("test_make_swiss_roll()"),]),
c("test_svmlight_format.py", "/datasets/tests/test_svmlight_format.py<br>", [
d("test_dump()"),
d("test_dump_comment()"),
d("test_dump_concise()"),
d("test_dump_invalid()"),
d("test_dump_multilabel()"),
d("test_dump_query_id()"),
d("test_invalid_filename()"),
d("test_load_compressed()"),
d("test_load_invalid_file()"),
d("test_load_invalid_file2()"),
d("test_load_invalid_order_file()"),
d("test_load_svmlight_file()"),
d("test_load_svmlight_file_fd()"),
d("test_load_svmlight_file_multilabel()"),
d("test_load_svmlight_file_n_features()"),
d("test_load_svmlight_files()"),
d("test_load_with_qid()"),
d("test_load_zero_based()"),
d("test_load_zero_based_auto()"),
d("test_not_a_filename()"),]),]),
c("base.py", "/datasets/base.py<br> Base IO code for all datasets", [
c("Bunch(dict)", "/datasets/base.py<br>Container object for datasets  Dictionary-like object that exposes its keys as attributes.  >>> b  Bunch(a1, b2) >>> b['b'] 2 >>> b.b 2 >>> b.a  3 >>> b['a'] 3 >>> b.c  6 >>> b['c'] 6", [
d("__getattr__(self, key)"),
d("__getstate__(self)"),
d("__init__(self)"),
d("__setattr__(self, key, value)"),]),
d("clear_data_home(data_home)", "Delete all the content of the data home cache."),
d("get_data_home(data_home)", "Return the path of the scikit-learn data dir.  This folder is used by some large dataset loaders to avoid downloading the data several times.  By default the data dir is set to a folder named 'scikit_learn_data' in the user home folder.  Alternatively, it can be set by the 'SCIKIT_LEARN_DATA' enviro..."),
d("load_boston()", "Load and return the boston house-prices dataset (regression).        Samples total                 506 Dimensionality                 13 Features           real, positive Targets             real 5. - 50.        Returns ------- data : Bunch     Dictionary-like object, the interesting attributes are:..."),
d("load_diabetes()", "Load and return the diabetes dataset (regression).         Samples total       442 Dimensionality      10 Features            real, -.2 < x < .2 Targets             integer 25 - 346         Read more in the :ref:`User Guide <datasets>`.  Returns ------- data : Bunch     Dictionary-like object, the i..."),
d("load_digits(n_class)", "Load and return the digits dataset (classification).  Each datapoint is a 8x8 image of a digit.      Classes                         10 Samples per class             ~180 Samples total                 1797 Dimensionality                  64 Features             integers 0-16      Read more in the :r..."),
d("load_files(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)", "Load text files with categories as subfolder names.  Individual samples are assumed to be files stored a two levels folder structure such as the following:      container_folder/         category_1_folder/             file_1.txt             file_2.txt             ...             file_42.txt         ..."),
d("load_iris()", "Load and return the iris dataset (classification).  The iris dataset is a classic and very easy multi-class classification dataset.      Classes                          3 Samples per class               50 Samples total                  150 Dimensionality                   4 Features            rea..."),
d("load_linnerud()", "Load and return the linnerud dataset (multivariate regression).  Samples total: 20 Dimensionality: 3 for both data and targets Features: integer Targets: integer  Returns ------- data : Bunch     Dictionary-like object, the interesting attributes are: 'data' and     'targets', the two multivariate d..."),
d("load_sample_image(image_name)", "Load the numpy array of a single sample image  Parameters ----------- image_name: {`china.jpg`, `flower.jpg`}     The name of the sample image loaded  Returns ------- img: 3D array     The image as a numpy array: height x width x color  Examples ---------  >>> from sklearn.datasets import load_sampl..."),
d("load_sample_images()", "Load sample images for image manipulation. Loads both, ``china`` and ``flower``.  Returns ------- data : Bunch     Dictionary-like object with the following attributes :     'images', the two sample images, 'filenames', the file     names for the images, and 'DESCR'     the full description of the d..."),]),
c("california_housing.py", "/datasets/california_housing.py<br> California housing dataset.  The original database is available from StatLib      http://lib.stat.cmu.edu/  The data contains 20,640 observations on 9 variables.  This dataset contains the average house value as target variable and the following input variables (features): average income, housing av...", [
d("fetch_california_housing(data_home, download_if_missing)", "Loader for the California housing dataset from StatLib.  Read more in the :ref:`User Guide <datasets>`.  Parameters ---------- data_home : optional, default: None     Specify another download and cache folder for the datasets. By default     all scikit learn data is stored in '~/scikit_learn_data' s..."),]),
c("covtype.py", "/datasets/covtype.py<br> Forest covertype dataset.  A classic dataset for classification benchmarks, featuring categorical and real-valued features.  The dataset page is available from UCI Machine Learning Repository      http://archive.ics.uci.edu/ml/datasets/Covertype  Courtesy of Jock A. Blackard and Colorado State Unive...", [
d("fetch_covtype(data_home, download_if_missing, random_state, shuffle)", "Load the covertype dataset, downloading it if necessary.  Read more in the :ref:`User Guide <datasets>`.  Parameters ---------- data_home : string, optional     Specify another download and cache folder for the datasets. By default     all scikit learn data is stored in '~/scikit_learn_data' subfold..."),]),
c("lfw.py", "/datasets/lfw.py<br> Loader for the Labeled Faces in the Wild (LFW) dataset  This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:      http://vis-www.cs.umass.edu/lfw/  Each picture is centered on a single face. The typical task is...", [
d("_fetch_lfw_pairs(index_file_path, data_folder_path, slice_, color, resize)", "Perform the actual data loading for the LFW pairs dataset  This operation is meant to be cached by a joblib wrapper."),
d("_fetch_lfw_people(data_folder_path, slice_, color, resize, min_faces_per_person)", "Perform the actual data loading for the lfw people dataset  This operation is meant to be cached by a joblib wrapper."),
d("_load_imgs(file_paths, slice_, color, resize)", "Internally used to load images"),
d("check_fetch_lfw(data_home, funneled, download_if_missing)", "Helper function to download any missing LFW data"),
d("fetch_lfw_pairs(subset, data_home, funneled, resize, color, slice_, download_if_missing)", "Loader for the Labeled Faces in the Wild (LFW) pairs dataset  This dataset is a collection of JPEG pictures of famous people collected on the internet, all details are available on the official website:      http://vis-www.cs.umass.edu/lfw/  Each picture is centered on a single face. Each pixel of e..."),
d("fetch_lfw_people(data_home, funneled, resize, min_faces_per_person, color, slice_, download_if_missing)", "Loader for the Labeled Faces in the Wild (LFW) people dataset  This dataset is a collection of JPEG pictures of famous people collected on the internet, all details are available on the official website:      http://vis-www.cs.umass.edu/lfw/  Each picture is centered on a single face. Each pixel of ..."),
d("load_lfw_pairs(download_if_missing)", "Alias for fetch_lfw_pairs(download_if_missingFalse)  Check fetch_lfw_pairs.__doc__ for the documentation and parameter list."),
d("load_lfw_people(download_if_missing)", "Alias for fetch_lfw_people(download_if_missingFalse)  Check fetch_lfw_people.__doc__ for the documentation and parameter list."),
d("scale_face(face)", "Scale back to 0-1 range in case of normalization for plotting"),]),
c("mlcomp.py", "/datasets/mlcomp.py<br> Glue code to load http://mlcomp.org data as a scikit.learn dataset", [
d("_load_document_classification(dataset_path, metadata, set_)"),
d("load_mlcomp(name_or_id, set_, mlcomp_root)", "Load a datasets as downloaded from http://mlcomp.org  Parameters ----------  name_or_id : the integer id or the string name metadata of the MLComp              dataset to load  set_ : select the portion to load: 'train', 'test' or 'raw'  mlcomp_root : the filesystem path to the root folder where MLC..."),]),
c("mldata.py", "/datasets/mldata.py<br> Automatically download MLdata datasets.", [
d("fetch_mldata(dataname, target_name, data_name, transpose_data, data_home)", "Fetch an mldata.org data set  If the file does not exist yet, it is downloaded from mldata.org .  mldata.org does not have an enforced convention for storing data or naming the columns in a data set. The default behavior of this function works well with the most common cases:    1) data values are s..."),
d("mldata_filename(dataname)", "Convert a raw name for a data set in a mldata.org filename."),
d("setup_module(module)"),
d("teardown_module(module)"),]),
c("olivetti_faces.py", "/datasets/olivetti_faces.py<br> Modified Olivetti faces dataset.  The original database was available from (now defunct)      http://www.uk.research.att.com/facedatabase.html  The version retrieved here comes in MATLAB format from the personal web page of Sam Roweis:      http://www.cs.nyu.edu/~roweis/  There are ten different ima...", [
d("fetch_olivetti_faces(data_home, shuffle, random_state, download_if_missing)", "Loader for the Olivetti faces data-set from AT&T.  Read more in the :ref:`User Guide <olivetti_faces>`.  Parameters ---------- data_home : optional, default: None     Specify another download and cache folder for the datasets. By default     all scikit learn data is stored in '~/scikit_learn_data' s..."),]),
c("rcv1.py", "/datasets/rcv1.py<br> RCV1 dataset.", [
d("_find_permutation(a, b)", "find the permutation from a to b"),
d("_inverse_permutation(p)", "inverse permutation p"),
d("fetch_rcv1(data_home, subset, download_if_missing, random_state, shuffle)", "Load the RCV1 multilabel dataset, downloading it if necessary.  Version: RCV1-v2, vectors, full sets, topics multilabels.        Classes                              103 Samples total                     804414 Dimensionality                     47236 Features           real, between 0 and 1        ..."),]),
c("samples_generator.py", "/datasets/samples_generator.py<br> Generate samples of synthetic data sets.", [
d("_generate_hypercube(samples, dimensions, rng)", "Returns distinct binary samples of length dimensions     "),
d("_shuffle(data, random_state)"),
d("make_biclusters(shape, n_clusters, noise, minval, maxval, shuffle, random_state)", "Generate an array with constant block diagonal structure for biclustering.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- shape : iterable (n_rows, n_cols)     The shape of the result.  n_clusters : integer     The number of biclusters.  noise : float, optional (defa..."),
d("make_blobs(n_samples, n_features, centers, cluster_std, center_box, shuffle, random_state)", "Generate isotropic Gaussian blobs for clustering.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_samples : int, optional (default100)     The total number of points equally divided among clusters.  n_features : int, optional (default2)     The number of features fo..."),
d("make_checkerboard(shape, n_clusters, noise, minval, maxval, shuffle, random_state)", "Generate an array with block checkerboard structure for biclustering.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- shape : iterable (n_rows, n_cols)     The shape of the result.  n_clusters : integer or iterable (n_row_clusters, n_column_clusters)     The number of..."),
d("make_circles(n_samples, shuffle, noise, random_state, factor)", "Make a large circle containing a smaller circle in 2d.  A simple toy dataset to visualize clustering and classification algorithms.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_samples : int, optional (default100)     The total number of points generated.  shuffl..."),
d("make_classification(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)", "Generate a random n-class classification problem.  This initially creates clusters of points normally distributed (std1) about vertices of a `2 * class_sep`-sided hypercube, and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various t..."),
d("make_friedman1(n_samples, n_features, noise, random_state)", "Generate the 'Friedman \#1' regression problem  This dataset is described in Friedman [1] and Breiman [2].  Inputs `X` are independent features uniformly distributed on the interval [0, 1]. The output `y` is created according to the formula::      y(X)  10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, ..."),
d("make_friedman2(n_samples, noise, random_state)", "Generate the 'Friedman \#2' regression problem  This dataset is described in Friedman [1] and Breiman [2].  Inputs `X` are 4 independent features uniformly distributed on the intervals::      0 < X[:, 0] < 100,     40 * pi < X[:, 1] < 560 * pi,     0 < X[:, 2] < 1,     1 < X[:, 3] < 11.  The output ..."),
d("make_friedman3(n_samples, noise, random_state)", "Generate the 'Friedman \#3' regression problem  This dataset is described in Friedman [1] and Breiman [2].  Inputs `X` are 4 independent features uniformly distributed on the intervals::      0 < X[:, 0] < 100,     40 * pi < X[:, 1] < 560 * pi,     0 < X[:, 2] < 1,     1 < X[:, 3] < 11.  The output ..."),
d("make_gaussian_quantiles(mean, cov, n_samples, n_features, n_classes, shuffle, random_state)", "Generate isotropic Gaussian and label samples by quantile  This classification dataset is constructed by taking a multi-dimensional standard normal distribution and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples are in each class ..."),
d("make_hastie_10_2(n_samples, random_state)", "Generates data for binary classification used in Hastie et al. 2009, Example 10.2.  The ten features are standard independent Gaussian and the target ``y`` is defined by::    y[i]  1 if np.sum(X[i] ** 2) > 9.34 else -1  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n..."),
d("make_low_rank_matrix(n_samples, n_features, effective_rank, tail_strength, random_state)", "Generate a mostly low rank matrix with bell-shaped singular values  Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is::      (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)  The remaining singular val..."),
d("make_moons(n_samples, shuffle, noise, random_state)", "Make two interleaving half circles  A simple toy dataset to visualize clustering and classification algorithms.  Parameters ---------- n_samples : int, optional (default100)     The total number of points generated.  shuffle : bool, optional (defaultTrue)     Whether to shuffle the samples.  noise :..."),
d("make_multilabel_classification(n_samples, n_features, n_classes, n_labels, length, allow_unlabeled, sparse, return_indicator, return_distributions, random_state)", "Generate a random multilabel classification problem.  For each sample, the generative process is:     - pick the number of labels: n ~ Poisson(n_labels)     - n times, choose a class c: c ~ Multinomial(theta)     - pick the document length: k ~ Poisson(length)     - k times, choose a word: w ~ Multi..."),
d("make_regression(n_samples, n_features, n_informative, n_targets, bias, effective_rank, tail_strength, noise, shuffle, coef, random_state)", "Generate a random regression problem.  The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See :func:`make_low_rank_matrix` for more details.  The output is generated by applying a (potentially biased) random linear regression model with `n_informa..."),
d("make_s_curve(n_samples, noise, random_state)", "Generate an S curve dataset.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_samples : int, optional (default100)     The number of sample points on the S curve.  noise : float, optional (default0.0)     The standard deviation of the gaussian noise.  random_state : ..."),
d("make_sparse_coded_signal(n_samples, n_components, n_features, n_nonzero_coefs, random_state)", "Generate a signal as a sparse combination of dictionary elements.  Returns a matrix Y  DX, such as D is (n_features, n_components), X is (n_components, n_samples) and each column of X has exactly n_nonzero_coefs non-zero elements.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ..."),
d("make_sparse_spd_matrix(dim, alpha, norm_diag, smallest_coef, largest_coef, random_state)", "Generate a sparse symmetric definite positive matrix.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- dim: integer, optional (default1)     The size of the random matrix to generate.  alpha: float between 0 and 1, optional (default0.95)     The probability that a coef..."),
d("make_sparse_uncorrelated(n_samples, n_features, random_state)", "Generate a random regression problem with sparse uncorrelated design  This dataset is described in Celeux et al [1]. as::      X ~ N(0, 1)     y(X)  X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]  Only the first 4 features are informative. The remaining features are useless.  Read more in the :..."),
d("make_spd_matrix(n_dim, random_state)", "Generate a random symmetric, positive-definite matrix.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_dim : int     The matrix dimension.  random_state : int, RandomState instance or None, optional (defaultNone)     If int, random_state is the seed used by the rand..."),
d("make_swiss_roll(n_samples, noise, random_state)", "Generate a swiss roll dataset.  Read more in the :ref:`User Guide <sample_generators>`.  Parameters ---------- n_samples : int, optional (default100)     The number of sample points on the S curve.  noise : float, optional (default0.0)     The standard deviation of the gaussian noise.  random_state ..."),]),
c("setup.py", "/datasets/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("species_distributions.py", "/datasets/species_distributions.py<br>  Species distribution dataset   This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).  The two species are:   - `'Bradypus variegatus'    <http://www.iucnredlist.org/apps/redlist/details/3038/0>`_ ,    the Brown-throated Sloth.   - `'Micr...", [
d("_load_coverage(F, header_length, dtype)", "Load a coverage file from an open file object.  This will return a numpy array of the given dtype"),
d("_load_csv(F)", "Load csv file.  Parameters ---------- F : file object     CSV file open in byte mode.  Returns ------- rec : np.ndarray     record array representing the data"),
d("construct_grids(batch)", "Construct the map grid from the batch object  Parameters ---------- batch : Batch object     The object returned by :func:`fetch_species_distributions`  Returns ------- (xgrid, ygrid) : 1-D arrays     The grid corresponding to the values in batch.coverages"),
d("fetch_species_distributions(data_home, download_if_missing)", "Loader for species distribution dataset from Phillips et. al. (2006)  Read more in the :ref:`User Guide <datasets>`.  Parameters ---------- data_home : optional, default: None     Specify another download and cache folder for the datasets. By default     all scikit learn data is stored in '~/scikit_..."),]),
c("svmlight_format.py", "/datasets/svmlight_format.py<br> This module implements a loader and dumper for the svmlight format  This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.  The first element of each line can be used to store a target variable to predict.  This form...", [
d("_dump_svmlight(X, y, f, multilabel, one_based, comment, query_id)"),
d("_gen_open(f)"),
d("_open_and_load(f, dtype, multilabel, zero_based, query_id)"),
d("dump_svmlight_file(X, y, f, zero_based, comment, query_id, multilabel)", "Dump the dataset in svmlight / libsvm file format.  This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.  The first element of each line can be used to store a target variable to predict.  Parameters ---------- X :..."),
d("load_svmlight_file(f, n_features, dtype, multilabel, zero_based, query_id)", "Load datasets in the svmlight / libsvm format into sparse CSR matrix  This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.  The first element of each line can be used to store a target variable to predict.  This fo..."),
d("load_svmlight_files(files, n_features, dtype, multilabel, zero_based, query_id)", "Load dataset from multiple files in SVMlight format  This function is equivalent to mapping load_svmlight_file over a list of files, except that the results are concatenated into a single, flat list and the samples vectors are constrained to all have the same number of features.  In case the file co..."),]),
c("twenty_newsgroups.py", "/datasets/twenty_newsgroups.py<br> Caching loader for the 20 newsgroups text classification dataset   The description of the dataset is available on the official website at:      http://people.csail.mit.edu/jrennie/20Newsgroups/  Quoting the introduction:      The 20 Newsgroups data set is a collection of approximately 20,000     new...", [
d("download_20newsgroups(target_dir, cache_path)", "Download the 20 newsgroups data and stored it as a zipped pickle."),
d("fetch_20newsgroups(data_home, subset, categories, shuffle, random_state, remove, download_if_missing)", "Load the filenames and data from the 20 newsgroups dataset.  Read more in the :ref:`User Guide <20newsgroups>`.  Parameters ---------- subset: 'train' or 'test', 'all', optional     Select the dataset to load: 'train' for the training set, 'test'     for the test set, 'all' for both, with shuffled o..."),
d("fetch_20newsgroups_vectorized(subset, remove, data_home)", "Load the 20 newsgroups dataset and transform it into tf-idf vectors.  This is a convenience function; the tf-idf transformation is done using the default settings for `sklearn.feature_extraction.text.Vectorizer`. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20..."),
d("strip_newsgroup_footer(text)", "Given text in 'news' format, attempt to remove a signature block.  As a rough heuristic, we assume that signatures are set apart by either a blank line or a line made of hyphens, and that it is the last such line in the file (disregarding blank lines at the end)."),
d("strip_newsgroup_header(text)", "Given text in 'news' format, strip the headers, by removing everything before the first blank line."),
d("strip_newsgroup_quoting(text)", "Given text in 'news' format, strip lines beginning with the quote characters > or |, plus lines that often introduce a quoted section (for example, because they contain the string 'writes:'.)"),]),]),
c("decomposition", "/decomposition<br>", [
c("tests", "/decomposition/tests<br>", [
c("test_dict_learning.py", "/decomposition/tests/test_dict_learning.py<br>", [
d("test_dict_learning_lassocd_readonly_data()"),
d("test_dict_learning_nonzero_coefs()"),
d("test_dict_learning_online_estimator_shapes()"),
d("test_dict_learning_online_initialization()"),
d("test_dict_learning_online_overcomplete()"),
d("test_dict_learning_online_partial_fit()"),
d("test_dict_learning_online_shapes()"),
d("test_dict_learning_online_verbosity()"),
d("test_dict_learning_overcomplete()"),
d("test_dict_learning_reconstruction()"),
d("test_dict_learning_reconstruction_parallel()"),
d("test_dict_learning_shapes()"),
d("test_dict_learning_split()"),
d("test_dict_learning_unknown_fit_algorithm()"),
d("test_sparse_coder_estimator()"),
d("test_sparse_encode_error()"),
d("test_sparse_encode_error_default_sparsity()"),
d("test_sparse_encode_shapes()"),
d("test_unknown_method()"),]),
c("test_factor_analysis.py", "/decomposition/tests/test_factor_analysis.py<br>", [
d("test_factor_analysis()"),]),
c("test_fastica.py", "/decomposition/tests/test_fastica.py<br> Test the fastica algorithm.", [
d("center_and_norm(x, axis)", "Centers and norms x **in place**  Parameters ----------- x: ndarray     Array with an axis of observations (statistical units) measured on     random variables. axis: int, optional     Axis along which the mean and variance are calculated."),
d("test_fastica_nowhiten()"),
d("test_fastica_simple(add_noise)"),
d("test_fit_transform()"),
d("test_gs()"),
d("test_inverse_transform()"),
d("test_non_square_fastica(add_noise)"),]),
c("test_incremental_pca.py", "/decomposition/tests/test_incremental_pca.py<br> Tests for Incremental PCA.", [
d("test_explained_variances()"),
d("test_incremental_pca()"),
d("test_incremental_pca_against_pca_iris()"),
d("test_incremental_pca_against_pca_random_data()"),
d("test_incremental_pca_batch_signs()"),
d("test_incremental_pca_batch_values()"),
d("test_incremental_pca_check_projection()"),
d("test_incremental_pca_inverse()"),
d("test_incremental_pca_num_features_change()"),
d("test_incremental_pca_partial_fit()"),
d("test_incremental_pca_set_params()"),
d("test_incremental_pca_validation()"),
d("test_whitening()"),]),
c("test_kernel_pca.py", "/decomposition/tests/test_kernel_pca.py<br>", [
d("test_gridsearch_pipeline()"),
d("test_gridsearch_pipeline_precomputed()"),
d("test_invalid_parameters()"),
d("test_kernel_pca()"),
d("test_kernel_pca_invalid_kernel()"),
d("test_kernel_pca_linear_kernel()"),
d("test_kernel_pca_n_components()"),
d("test_kernel_pca_precomputed()"),
d("test_kernel_pca_sparse()"),
d("test_nested_circles()"),
d("test_remove_zero_eig()"),]),
c("test_nmf.py", "/decomposition/tests/test_nmf.py<br>", [
d("test_initialize_close()"),
d("test_initialize_nn_input()"),
d("test_initialize_nn_output()"),
d("test_initialize_variants()"),
d("test_n_components_greater_n_features()"),
d("test_nls_close()"),
d("test_nls_nn_output()"),
d("test_projgrad_nmf_fit_close()"),
d("test_projgrad_nmf_fit_nn_input()"),
d("test_projgrad_nmf_fit_nn_output()"),
d("test_projgrad_nmf_sparseness()"),
d("test_projgrad_nmf_transform()"),
d("test_sparse_input()"),
d("test_sparse_transform()"),]),
c("test_online_lda.py", "/decomposition/tests/test_online_lda.py<br>", [
d("_build_sparse_mtx()"),
d("test_dirichlet_expectation()", "Test Cython version of Dirichlet expectation calculation."),
d("test_invalid_params()"),
d("test_lda_default_prior_params()"),
d("test_lda_dense_input()"),
d("test_lda_empty_docs()", "Test LDA on empty document (all-zero rows)."),
d("test_lda_fit_batch()"),
d("test_lda_fit_online()"),
d("test_lda_fit_transform()"),
d("test_lda_multi_jobs()"),
d("test_lda_negative_input()"),
d("test_lda_no_component_error()"),
d("test_lda_partial_fit()"),
d("test_lda_partial_fit_dim_mismatch()"),
d("test_lda_partial_fit_multi_jobs()"),
d("test_lda_perplexity()"),
d("test_lda_preplexity_mismatch()"),
d("test_lda_score()"),
d("test_lda_score_perplexity()"),
d("test_lda_transform()"),
d("test_lda_transform_mismatch()"),
d("test_perplexity_input_format()"),]),
c("test_pca.py", "/decomposition/tests/test_pca.py<br>", [
d("test_explained_variance()"),
d("test_infer_dim_1()"),
d("test_infer_dim_2()"),
d("test_infer_dim_3()"),
d("test_infer_dim_by_explained_variance()"),
d("test_pca()"),
d("test_pca_check_projection()"),
d("test_pca_dim()"),
d("test_pca_inverse()"),
d("test_pca_score()"),
d("test_pca_score2()"),
d("test_pca_score3()"),
d("test_pca_validation()"),
d("test_randomized_pca_check_list()"),
d("test_randomized_pca_check_projection()"),
d("test_randomized_pca_inverse()"),
d("test_whitening()"),]),
c("test_sparse_pca.py", "/decomposition/tests/test_sparse_pca.py<br>", [
d("generate_toy_data(n_components, n_samples, image_size, random_state)"),
d("test_correct_shapes()"),
d("test_fit_transform()"),
d("test_fit_transform_parallel()"),
d("test_fit_transform_tall()"),
d("test_initialization()"),
d("test_mini_batch_correct_shapes()"),
d("test_mini_batch_fit_transform()"),
d("test_transform_nan()"),]),
c("test_truncated_svd.py", "/decomposition/tests/test_truncated_svd.py<br> Test truncated SVD transformer.", [
d("test_algorithms()"),
d("test_attributes()"),
d("test_explained_variance()"),
d("test_integers()"),
d("test_inverse_transform()"),
d("test_sparse_formats()"),
d("test_too_many_components()"),]),]),
c("base.py", "/decomposition/base.py<br> Principal Component Analysis Base Classes", [
c("_BasePCA()", "/decomposition/base.py<br>Base class for PCA methods.  Warning: This class should not be used directly. Use derived classes instead.", [
d("fit(X, y)"),
d("get_covariance(self)"),
d("get_precision(self)"),
d("inverse_transform(self, X, y)"),
d("transform(self, X, y)"),]),]),
c("dict_learning.py", "/decomposition/dict_learning.py<br> Dictionary learning", [
c("DictionaryLearning(BaseEstimator, SparseCodingMixin)", "/decomposition/dict_learning.py<br>Dictionary learning  Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.  Solves the optimization problem::      (U^*,V^*)  argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1                 (U,V)                 with || V_k ||_2  1 for all  0 < k < n_compo...", [
d("__init__(self, n_components, alpha, max_iter, tol, fit_algorithm, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, n_jobs, code_init, dict_init, verbose, split_sign, random_state)"),
d("fit(self, X, y)"),]),
c("MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin)", "/decomposition/dict_learning.py<br>Mini-batch dictionary learning  Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.  Solves the optimization problem::     (U^*,V^*)  argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1                 (U,V)                 with || V_k ||_2  1 for all  0 < k...", [
d("__init__(self, n_components, alpha, n_iter, fit_algorithm, n_jobs, batch_size, shuffle, dict_init, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, verbose, split_sign, random_state)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y, iter_offset)"),]),
c("SparseCoder(BaseEstimator, SparseCodingMixin)", "/decomposition/dict_learning.py<br>Sparse coding  Finds a sparse representation of data against a fixed, precomputed dictionary.  Each row of the result is the solution to a sparse coding problem. The goal is to find a sparse array `code` such that::      X ~ code * dictionary  Read more in the :ref:`User Guide <SparseCoder>`.  Param...", [
d("__init__(self, dictionary, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs)"),
d("fit(self, X, y)"),]),
c("SparseCodingMixin(TransformerMixin)", "/decomposition/dict_learning.py<br>Sparse coding mixin", [
d("_set_sparse_coding_params(self, n_components, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs)"),
d("transform(self, X, y)"),]),
d("_sparse_encode(X, dictionary, gram, cov, algorithm, regularization, copy_cov, init, max_iter)", "Generic sparse coding  Each column of the result is the solution to a Lasso problem.  Parameters ---------- X: array of shape (n_samples, n_features)     Data matrix.  dictionary: array of shape (n_components, n_features)     The dictionary matrix against which to solve the sparse coding of     the ..."),
d("_update_dict(dictionary, Y, code, verbose, return_r2, random_state)", "Update the dense dictionary factor in place.  Parameters ---------- dictionary: array of shape (n_features, n_components)     Value of the dictionary at the previous iteration.  Y: array of shape (n_features, n_samples)     Data matrix.  code: array of shape (n_components, n_samples)     Sparse codi..."),
d("dict_learning(X, n_components, alpha, max_iter, tol, method, n_jobs, dict_init, code_init, callback, verbose, random_state, return_n_iter)", "Solves a dictionary learning matrix factorization problem.  Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving::      (U^*, V^*)  argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1                  (U,V)                 with || V_k ||_2  1 for al..."),
d("dict_learning_online(X, n_components, alpha, n_iter, return_code, dict_init, callback, batch_size, verbose, shuffle, n_jobs, method, iter_offset, random_state, return_inner_stats, inner_stats, return_n_iter)", "Solves a dictionary learning matrix factorization problem online.  Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving::      (U^*, V^*)  argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1                  (U,V)                  with || V_k ||_2  ..."),
d("sparse_encode(X, dictionary, gram, cov, algorithm, n_nonzero_coefs, alpha, copy_cov, init, max_iter, n_jobs)", "Sparse coding  Each row of the result is the solution to a sparse coding problem. The goal is to find a sparse array `code` such that::      X ~ code * dictionary  Read more in the :ref:`User Guide <SparseCoder>`.  Parameters ---------- X: array of shape (n_samples, n_features)     Data matrix  dict..."),]),
c("factor_analysis.py", "/decomposition/factor_analysis.py<br> Factor Analysis.  A latent linear variable model.  FactorAnalysis is similar to probabilistic PCA implemented by PCA.score While PCA assumes Gaussian noise with the same variance for each feature, the FactorAnalysis model assumes different variances for each of them.  This implementation is based on...", [
c("FactorAnalysis(BaseEstimator, TransformerMixin)", "/decomposition/factor_analysis.py<br>Factor Analysis (FA)  A simple linear generative model with Gaussian latent variables.  The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian wi...", [
d("__init__(self, n_components, tol, copy, max_iter, noise_variance_init, svd_method, iterated_power, random_state)"),
d("fit(self, X, y)"),
d("get_covariance(self)"),
d("get_precision(self)"),
d("score(self, X, y)"),
d("score_samples(self, X)"),
d("transform(self, X)"),]),]),
c("fastica_.py", "/decomposition/fastica_.py<br> Python implementation of the fast ICA algorithms.  Reference: Tables 8.3 and 8.4 page 196 in the book: Independent Component Analysis, by  Hyvarinen et al.", [
c("FastICA(BaseEstimator, TransformerMixin)", "/decomposition/fastica_.py<br>FastICA: a fast algorithm for Independent Component Analysis.  Read more in the :ref:`User Guide <ICA>`.  Parameters ---------- n_components : int, optional     Number of components to use. If none is passed, all are used.  algorithm : {'parallel', 'deflation'}     Apply parallel or deflational algo...", [
d("__init__(self, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, random_state)"),
d("_fit(self, X, compute_sources)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("inverse_transform(self, X, copy)"),
d("transform(self, X, y, copy)"),]),
d("_cube(x, fun_args)"),
d("_exp(x, fun_args)"),
d("_gs_decorrelation(w, W, j)", "Orthonormalize w wrt the first j rows of W  Parameters ---------- w : ndarray of shape(n)     Array to be orthogonalized  W : ndarray of shape(p, n)     Null space definition  j : int < p     The no of (from the first) rows of Null space W wrt which w is     orthogonalized.  Notes ----- Assumes that..."),
d("_ica_def(X, tol, g, fun_args, max_iter, w_init)", "Deflationary FastICA using fun approx to neg-entropy function  Used internally by FastICA."),
d("_ica_par(X, tol, g, fun_args, max_iter, w_init)", "Parallel FastICA.  Used internally by FastICA --main loop"),
d("_logcosh(x, fun_args)"),
d("_sym_decorrelation(W)", "Symmetric decorrelation i.e. W <- (W * W.T) ^{-1/2} * W"),
d("fastica(X, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, random_state, return_X_mean, compute_sources, return_n_iter)", "Perform Fast Independent Component Analysis.  Read more in the :ref:`User Guide <ICA>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Training vector, where n_samples is the number of samples and     n_features is the number of features.  n_components : int, optional     N..."),]),
c("incremental_pca.py", "/decomposition/incremental_pca.py<br> Incremental Principal Components Analysis.", [
c("IncrementalPCA(_BasePCA)", "/decomposition/incremental_pca.py<br>Incremental principal components analysis (IPCA).  Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.  Depending on the size of the input data, this algorithm can be...", [
d("__init__(self, n_components, whiten, copy, batch_size)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),]),]),
c("kernel_pca.py", "/decomposition/kernel_pca.py<br> Kernel Principal Components Analysis", [
c("KernelPCA(BaseEstimator, TransformerMixin)", "/decomposition/kernel_pca.py<br>Kernel Principal component analysis (KPCA)  Non-linear dimensionality reduction through the use of kernels (see :ref:`metrics`).  Read more in the :ref:`User Guide <kernel_PCA>`.  Parameters ---------- n_components: int or None     Number of components. If None, all non-zero components are kept.  ke...", [
d("__init__(self, n_components, kernel, gamma, degree, coef0, kernel_params, alpha, fit_inverse_transform, eigen_solver, tol, max_iter, remove_zero_eig)"),
d("_fit_inverse_transform(self, X_transformed, X)"),
d("_fit_transform(self, K)"),
d("_get_kernel(self, X, Y)"),
d("_pairwise(self)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("inverse_transform(self, X)"),
d("transform(self, X)"),]),]),
c("nmf.py", "/decomposition/nmf.py<br> Non-negative matrix factorization", [
c("NMF(ProjectedGradientNMF)", "/decomposition/nmf.py<br>", [
]),
c("ProjectedGradientNMF(BaseEstimator, TransformerMixin)", "/decomposition/nmf.py<br>Non-Negative matrix factorization by Projected Gradient (NMF)  Read more in the :ref:`User Guide <NMF>`.  Parameters ---------- n_components : int or None     Number of components, if n_components is not set all components     are kept  init :  'nndsvd' |  'nndsvda' | 'nndsvdar' | 'random'     Metho...", [
d("__init__(self, n_components, init, sparseness, beta, eta, tol, max_iter, nls_max_iter, random_state)"),
d("_init(self, X)"),
d("_update_H(self, X, H, W, tolH)"),
d("_update_W(self, X, H, W, tolW)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),]),
d("_initialize_nmf(X, n_components, variant, eps, random_state)", "NNDSVD algorithm for NMF initialization.  Computes a good initial guess for the non-negative rank k matrix approximation for X: X  WH  Parameters ----------  X : array, [n_samples, n_features]     The data matrix to be decomposed.  n_components : array, [n_components, n_features]     The number of c..."),
d("_nls_subproblem(V, W, H, tol, max_iter, sigma, beta)", "Non-negative least square solver  Solves a non-negative least squares subproblem using the projected gradient descent algorithm. min || WH - V ||_2  Parameters ---------- V, W : array-like     Constant matrices.  H : array-like     Initial guess for the solution.  tol : float     Tolerance of the st..."),
d("_sparseness(x)", "Hoyer's measure of sparsity for a vector"),
d("norm(x)", "Dot product-based Euclidean norm implementation  See: http://fseoane.net/blog/2011/computing-the-vector-norm/"),
d("safe_vstack(Xs)"),
d("trace_dot(X, Y)", "Trace of np.dot(X, Y.T)."),]),
c("online_lda.py", "/decomposition/online_lda.py<br>  Online Latent Dirichlet Allocation with variational inference   This implementation is modified from Matthew D. Hoffman's onlineldavb code Link: http://www.cs.princeton.edu/~mdhoffma/code/onlineldavb.tar", [
c("LatentDirichletAllocation(BaseEstimator, TransformerMixin)", "/decomposition/online_lda.py<br>Latent Dirichlet Allocation with online variational Bayes algorithm  Parameters ---------- n_topics : int, optional (default10)     Number of topics.  doc_topic_prior : float, optional (defaultNone)     Prior of document topic distribution `theta`. If the value is None,     defaults to `1 / n_topics...", [
d("__init__(self, n_topics, doc_topic_prior, topic_word_prior, learning_method, learning_decay, learning_offset, max_iter, batch_size, evaluate_every, total_samples, perp_tol, mean_change_tol, max_doc_update_iter, n_jobs, verbose, random_state)"),
d("_approx_bound(self, X, doc_topic_distr, sub_sampling)"),
d("_check_non_neg_array(self, X, whom)"),
d("_check_params(self)"),
d("_e_step(self, X, cal_sstats, random_init)"),
d("_em_step(self, X, total_samples, batch_update)"),
d("_init_latent_vars(self, n_features)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("perplexity(self, X, doc_topic_distr, sub_sampling)"),
d("score(self, X, y)"),
d("transform(self, X)"),]),
d("_update_doc_distribution(X, exp_topic_word_distr, doc_topic_prior, max_iters, mean_change_tol, cal_sstats, random_state)", "E-step: update document-topic distribution.  Parameters ---------- X : array-like or sparse matrix, shape(n_samples, n_features)     Document word matrix.  exp_topic_word_distr : dense matrix, shape(n_topics, n_features)     Exponential value of expection of log topic word distribution.     In the l..."),]),
c("pca.py", "/decomposition/pca.py<br> Principal Component Analysis", [
c("PCA(BaseEstimator, TransformerMixin)", "/decomposition/pca.py<br>Principal component analysis (PCA)  Linear dimensionality reduction using Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.  This implementation uses the scipy.linalg implementation of the singular value ...", [
d("__init__(self, n_components, copy, whiten)"),
d("_fit(self, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("get_covariance(self)"),
d("get_precision(self)"),
d("inverse_transform(self, X)"),
d("score(self, X, y)"),
d("score_samples(self, X)"),
d("transform(self, X)"),]),
c("RandomizedPCA(BaseEstimator, TransformerMixin)", "/decomposition/pca.py<br>Principal component analysis (PCA) using randomized SVD  Linear dimensionality reduction using approximated Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.  Read more in the :ref:`User Guide <Randomized...", [
d("__init__(self, n_components, copy, iterated_power, whiten, random_state)"),
d("_fit(self, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("inverse_transform(self, X, y)"),
d("transform(self, X, y)"),]),
d("_assess_dimension_(spectrum, rank, n_samples, n_features)", "Compute the likelihood of a rank ``rank`` dataset  The dataset is assumed to be embedded in gaussian noise of shape(n, dimf) having spectrum ``spectrum``.  Parameters ---------- spectrum: array of shape (n)     Data spectrum. rank: int     Tested rank value. n_samples: int     Number of samples. n_f..."),
d("_infer_dimension_(spectrum, n_samples, n_features)", "Infers the dimension of a dataset of shape (n_samples, n_features)  The dataset is described by its spectrum `spectrum`."),]),
c("setup.py", "/decomposition/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("sparse_pca.py", "/decomposition/sparse_pca.py<br> Matrix factorization with Sparse PCA", [
c("MiniBatchSparsePCA(SparsePCA)", "/decomposition/sparse_pca.py<br>Mini-batch Sparse Principal Components Analysis  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.  Parameters ----...", [
d("__init__(self, n_components, alpha, ridge_alpha, n_iter, callback, batch_size, verbose, shuffle, n_jobs, method, random_state)"),
d("fit(self, X, y)"),]),
c("SparsePCA(BaseEstimator, TransformerMixin)", "/decomposition/sparse_pca.py<br>Sparse Principal Components Analysis (SparsePCA)  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.  Parameters ---...", [
d("__init__(self, n_components, alpha, ridge_alpha, max_iter, tol, method, n_jobs, U_init, V_init, verbose, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X, ridge_alpha)"),]),]),
c("truncated_svd.py", "/decomposition/truncated_svd.py<br> Truncated SVD for sparse matrices, aka latent semantic analysis (LSA).", [
c("TruncatedSVD(BaseEstimator, TransformerMixin)", "/decomposition/truncated_svd.py<br>Dimensionality reduction using truncated SVD (aka LSA).  This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). It is very similar to PCA, but operates on sample vectors directly, instead of on a covariance matrix. This means it can work w...", [
d("__init__(self, n_components, algorithm, n_iter, random_state, tol)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("inverse_transform(self, X)"),
d("transform(self, X)"),]),]),]),
c("ensemble", "/ensemble<br>", [
c("tests", "/ensemble/tests<br>", [
c("test_bagging.py", "/ensemble/tests/test_bagging.py<br> Testing for the bagging ensemble module (sklearn.ensemble.bagging).", [
c("DummyZeroEstimator(BaseEstimator)", "/ensemble/tests/test_bagging.py<br>", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
d("test_bagging_sample_weight_unsupported_but_passed()"),
d("test_bagging_with_pipeline()"),
d("test_base_estimator()"),
d("test_bootstrap_features()"),
d("test_bootstrap_samples()"),
d("test_classification()"),
d("test_error()"),
d("test_gridsearch()"),
d("test_oob_score_classification()"),
d("test_oob_score_regression()"),
d("test_oob_score_removed_on_warm_start()"),
d("test_parallel_classification()"),
d("test_parallel_regression()"),
d("test_probability()"),
d("test_regression()"),
d("test_single_estimator()"),
d("test_sparse_classification()"),
d("test_sparse_regression()"),
d("test_warm_start(random_state)"),
d("test_warm_start_equal_n_estimators()"),
d("test_warm_start_equivalence()"),
d("test_warm_start_smaller_n_estimators()"),
d("test_warm_start_with_oob_score_fails()"),]),
c("test_base.py", "/ensemble/tests/test_base.py<br> Testing for the base module (sklearn.ensemble.base).", [
d("test_base()"),
d("test_base_zero_n_estimators()"),]),
c("test_forest.py", "/ensemble/tests/test_forest.py<br> Testing for the forest module (sklearn.ensemble.forest).", [
d("check_1d_input(name, X, X_2d, y)"),
d("check_boston_criterion(name, criterion)"),
d("check_class_weight_balanced_and_bootstrap_multi_output(name)"),
d("check_class_weight_errors(name)"),
d("check_class_weights(name)"),
d("check_classes_shape(name)"),
d("check_classification_toy(name)", "Check classification on a toy dataset."),
d("check_gridsearch(name)"),
d("check_importances(name, X, y)"),
d("check_iris_criterion(name, criterion)"),
d("check_max_leaf_nodes_max_depth(name, X, y)"),
d("check_memory_layout(name, dtype)"),
d("check_min_samples_leaf(name, X, y)"),
d("check_min_weight_fraction_leaf(name, X, y)"),
d("check_multioutput(name)"),
d("check_oob_score(name, X, y, n_estimators)"),
d("check_oob_score_raise_error(name)"),
d("check_parallel(name, X, y)", "Check parallel computations in classification"),
d("check_pickle(name, X, y)"),
d("check_probability(name)"),
d("check_regressor_attributes(name)"),
d("check_sparse_input(name, X, X_sparse, y)"),
d("check_unfitted_feature_importances(name)"),
d("check_warm_start(name, random_state)"),
d("check_warm_start_clear(name)"),
d("check_warm_start_equal_n_estimators(name)"),
d("check_warm_start_oob(name)"),
d("check_warm_start_smaller_n_estimators(name)"),
d("test_1d_input()"),
d("test_boston()"),
d("test_class_weight_balanced_and_bootstrap_multi_output()"),
d("test_class_weight_errors()"),
d("test_class_weights()"),
d("test_classes_shape()"),
d("test_classification_toy()"),
d("test_distribution()"),
d("test_dtype_convert()"),
d("test_gridsearch()"),
d("test_importances()"),
d("test_iris()"),
d("test_max_leaf_nodes_max_depth()"),
d("test_memory_layout()"),
d("test_min_samples_leaf()"),
d("test_min_weight_fraction_leaf()"),
d("test_multioutput()"),
d("test_oob_score()"),
d("test_oob_score_raise_error()"),
d("test_parallel()"),
d("test_parallel_train()"),
d("test_pickle()"),
d("test_probability()"),
d("test_random_hasher()"),
d("test_random_hasher_sparse_data()"),
d("test_random_trees_dense_equal()"),
d("test_random_trees_dense_type()"),
d("test_regressor_attributes()"),
d("test_sparse_input()"),
d("test_unfitted_feature_importances()"),
d("test_warm_start()"),
d("test_warm_start_clear()"),
d("test_warm_start_equal_n_estimators()"),
d("test_warm_start_oob()"),
d("test_warm_start_smaller_n_estimators()"),]),
c("test_gradient_boosting.py", "/ensemble/tests/test_gradient_boosting.py<br> Testing for the gradient boosting module (sklearn.ensemble.gradient_boosting).", [
d("early_stopping_monitor(i, est, locals)", "Returns True on the 10th iteration. "),
d("test_boston()"),
d("test_check_inputs()"),
d("test_check_inputs_predict()"),
d("test_check_max_features()"),
d("test_classification_synthetic()"),
d("test_classification_toy()"),
d("test_complete_classification()"),
d("test_complete_regression()"),
d("test_degenerate_targets()"),
d("test_feature_importances()"),
d("test_float_class_labels()"),
d("test_iris()"),
d("test_loss_function()"),
d("test_max_feature_auto()"),
d("test_max_feature_regression()"),
d("test_max_leaf_nodes_max_depth()"),
d("test_mem_layout()"),
d("test_monitor_early_stopping()"),
d("test_more_verbose_output()"),
d("test_non_uniform_weights_toy_edge_case_clf()"),
d("test_non_uniform_weights_toy_edge_case_reg()"),
d("test_non_uniform_weights_toy_min_weight_leaf()"),
d("test_oob_improvement()"),
d("test_oob_improvement_raise()"),
d("test_oob_multilcass_iris()"),
d("test_parameter_checks()"),
d("test_probability_exponential()"),
d("test_probability_log()"),
d("test_quantile_loss()"),
d("test_regression_synthetic()"),
d("test_serialization()"),
d("test_shape_y()"),
d("test_staged_functions_defensive()"),
d("test_staged_predict()"),
d("test_staged_predict_proba()"),
d("test_symbol_labels()"),
d("test_verbose_output()"),
d("test_warm_start()"),
d("test_warm_start_clear()"),
d("test_warm_start_equal_n_estimators()"),
d("test_warm_start_max_depth()"),
d("test_warm_start_n_estimators()"),
d("test_warm_start_oob()"),
d("test_warm_start_oob_switch()"),
d("test_warm_start_smaller_n_estimators()"),
d("test_warm_start_wo_nestimators_change()"),
d("test_warm_start_zero_n_estimators()"),
d("test_zero_estimator_clf()"),
d("test_zero_estimator_reg()"),]),
c("test_gradient_boosting_loss_functions.py", "/ensemble/tests/test_gradient_boosting_loss_functions.py<br> Testing for the gradient boosting loss functions and initial estimators.", [
d("test_binomial_deviance()"),
d("test_log_odds_estimator()"),
d("test_sample_weight_deviance()"),
d("test_sample_weight_init_estimators()"),
d("test_sample_weight_smoke()"),
d("test_weighted_percentile()"),
d("test_weighted_percentile_equal()"),
d("test_weighted_percentile_zero_weight()"),]),
c("test_partial_dependence.py", "/ensemble/tests/test_partial_dependence.py<br> Testing for the partial dependence module.", [
d("test_partial_dependecy_input()"),
d("test_partial_dependence_classifier()"),
d("test_partial_dependence_multiclass()"),
d("test_partial_dependence_regressor()"),
d("test_plot_partial_dependence()"),
d("test_plot_partial_dependence_input()"),
d("test_plot_partial_dependence_multiclass()"),]),
c("test_voting_classifier.py", "/ensemble/tests/test_voting_classifier.py<br> Testing for the boost module (sklearn.ensemble.boost).", [
d("test_gridsearch()", "Check GridSearch support."),
d("test_majority_label_iris()", "Check classification by majority label on dataset iris."),
d("test_multilabel()", "Check if error is raised for multilabel classification."),
d("test_predict_on_toy_problem()", "Manually check predicted class labels for toy dataset."),
d("test_predict_proba_on_toy_problem()", "Calculate predicted probabilities on toy dataset."),
d("test_tie_situation()", "Check voting classifier selects smaller class label in tie situation."),
d("test_weights_iris()", "Check classification by average probabilities on dataset iris."),]),
c("test_weight_boosting.py", "/ensemble/tests/test_weight_boosting.py<br> Testing for the boost module (sklearn.ensemble.boost).", [
d("test_base_estimator()"),
d("test_boston()"),
d("test_classification_toy()"),
d("test_error()"),
d("test_gridsearch()"),
d("test_importances()"),
d("test_iris()"),
d("test_pickle()"),
d("test_regression_toy()"),
d("test_samme_proba()"),
d("test_sample_weight_missing()"),
d("test_sparse_classification()"),
d("test_sparse_regression()"),
d("test_staged_predict()"),]),]),
c("bagging.py", "/ensemble/bagging.py<br> Bagging meta-estimator.", [
c("BaggingClassifier(BaseBagging, ClassifierMixin)", "/ensemble/bagging.py<br>A Bagging classifier.  A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be us...", [
d("__init__(self, base_estimator, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, oob_score, warm_start, n_jobs, random_state, verbose)"),
d("_set_oob_score(self, X, y)"),
d("_validate_estimator(self)"),
d("_validate_y(self, y)"),
d("decision_function(self, X)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),
c("BaggingRegressor(BaseBagging, RegressorMixin)", "/ensemble/bagging.py<br>A Bagging regressor.  A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used ...", [
d("__init__(self, base_estimator, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, oob_score, warm_start, n_jobs, random_state, verbose)"),
d("_set_oob_score(self, X, y)"),
d("_validate_estimator(self)"),
d("predict(self, X)"),]),
c("BaseBagging()", "/ensemble/bagging.py<br>Base class for Bagging meta-estimator.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, oob_score, warm_start, n_jobs, random_state, verbose)"),
d("_set_oob_score(self, X, y)"),
d("_validate_y(self, y)"),
d("fit(self, X, y, sample_weight)"),]),
d("_parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight, seeds, verbose)", "Private function used to build a batch of estimators within a job."),
d("_parallel_decision_function(estimators, estimators_features, X)", "Private function used to compute decisions within a job."),
d("_parallel_predict_log_proba(estimators, estimators_features, X, n_classes)", "Private function used to compute log probabilities within a job."),
d("_parallel_predict_proba(estimators, estimators_features, X, n_classes)", "Private function used to compute (proba-)predictions within a job."),
d("_parallel_predict_regression(estimators, estimators_features, X)", "Private function used to compute predictions within a job."),]),
c("base.py", "/ensemble/base.py<br> Base class for ensemble-based estimators.", [
c("BaseEnsemble(BaseEstimator, MetaEstimatorMixin)", "/ensemble/base.py<br>Base class for all ensemble classes.  Warning: This class should not be used directly. Use derived classes instead.  Parameters ---------- base_estimator : object, optional (defaultNone)     The base estimator from which the ensemble is built.  n_estimators : integer     The number of estimators in ...", [
d("__getitem__(self, index)"),
d("__init__(self, base_estimator, n_estimators, estimator_params)"),
d("__iter__(self)"),
d("__len__(self)"),
d("_make_estimator(self, append)"),
d("_validate_estimator(self, default)"),]),
d("_partition_estimators(n_estimators, n_jobs)", "Private function used to partition estimators between jobs."),]),
c("forest.py", "/ensemble/forest.py<br> Forest of trees-based ensemble methods  Those methods include random forests and extremely randomized trees.  The module structure is the following:  - The ``BaseForest`` base class implements a common ``fit`` method for all   the estimators in the module. The ``fit`` method of the base ``Forest``  ...", [
c("BaseForest()", "/ensemble/forest.py<br>Base class for forests of trees.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),
d("_set_oob_score(self, X, y)"),
d("_validate_X_predict(self, X)"),
d("_validate_y_class_weight(self, y)"),
d("apply(self, X)"),
d("feature_importances_(self)"),
d("fit(self, X, y, sample_weight)"),]),
c("ExtraTreesClassifier(ForestClassifier)", "/ensemble/forest.py<br>An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  ...", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),]),
c("ExtraTreesRegressor(ForestRegressor)", "/ensemble/forest.py<br>An extra-trees regressor.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  P...", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)"),]),
c("ForestClassifier()", "/ensemble/forest.py<br>Base class for forest of trees-based classifiers.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),
d("_set_oob_score(self, X, y)"),
d("_validate_y_class_weight(self, y)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),
c("ForestRegressor()", "/ensemble/forest.py<br>Base class for forest of trees-based regressors.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)"),
d("_set_oob_score(self, X, y)"),
d("predict(self, X)"),]),
c("RandomForestClassifier(ForestClassifier)", "/ensemble/forest.py<br>A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample si...", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),]),
c("RandomForestRegressor(ForestRegressor)", "/ensemble/forest.py<br>A random forest regressor.  A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample si...", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)"),]),
c("RandomTreesEmbedding(BaseForest)", "/ensemble/forest.py<br>An ensemble of totally random trees.  An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there...", [
d("__init__(self, n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, sparse_output, n_jobs, random_state, verbose, warm_start)"),
d("_set_oob_score(self, X, y)"),
d("fit(self, X, y, sample_weight)"),
d("fit_transform(self, X, y, sample_weight)"),
d("transform(self, X)"),]),
d("_generate_sample_indices(random_state, n_samples)", "Private function used to _parallel_build_trees function."),
d("_generate_unsampled_indices(random_state, n_samples)", "Private function used to forest._set_oob_score fuction."),
d("_parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)", "Private function used to fit a single tree in parallel."),
d("_parallel_helper(obj, methodname)", "Private helper to workaround Python 2 pickle limitations"),]),
c("gradient_boosting.py", "/ensemble/gradient_boosting.py<br> Gradient Boosted Regression Trees  This module contains methods for fitting gradient boosted regression trees for both classification and regression.  The module structure is the following:  - The ``BaseGradientBoosting`` base class implements a common ``fit`` method   for all the estimators in the ...", [
c("BaseGradientBoosting()", "/ensemble/gradient_boosting.py<br>Abstract base class for Gradient Boosting. ", [
d("__init__(self, loss, learning_rate, n_estimators, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, init, subsample, max_features, random_state, alpha, verbose, max_leaf_nodes, warm_start)"),
d("_check_params(self)"),
d("_clear_state(self)"),
d("_decision_function(self, X)"),
d("_fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask, criterion, splitter, random_state)"),
d("_fit_stages(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor)"),
d("_init_decision_function(self, X)"),
d("_init_state(self)"),
d("_is_initialized(self)"),
d("_make_estimator(self, append)"),
d("_resize_state(self)"),
d("_staged_decision_function(self, X)"),
d("_validate_y(self, y)"),
d("decision_function(self, X)"),
d("feature_importances_(self)"),
d("fit(self, X, y, sample_weight, monitor)"),
d("staged_decision_function(self, X)"),]),
c("BinomialDeviance(ClassificationLossFunction)", "/ensemble/gradient_boosting.py<br>Binomial deviance loss function for binary classification.  Binary classification is a special case; here, we only need to fit one tree instead of ``n_classes`` trees.", [
d("__call__(self, y, pred, sample_weight)"),
d("__init__(self, n_classes)"),
d("_score_to_decision(self, score)"),
d("_score_to_proba(self, score)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred)"),]),
c("ClassificationLossFunction()", "/ensemble/gradient_boosting.py<br>Base class for classification loss functions. ", [
d("_score_to_decision(self, score)"),
d("_score_to_proba(self, score)"),]),
c("ExponentialLoss(ClassificationLossFunction)", "/ensemble/gradient_boosting.py<br>Exponential loss function for binary classification.  Same loss as AdaBoost.  References ---------- Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007", [
d("__call__(self, y, pred, sample_weight)"),
d("__init__(self, n_classes)"),
d("_score_to_decision(self, score)"),
d("_score_to_proba(self, score)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred)"),]),
c("GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin)", "/ensemble/gradient_boosting.py<br>Gradient Boosting for classification.  GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the binomial or multinomial deviance loss f...", [
d("__init__(self, loss, learning_rate, n_estimators, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, init, random_state, max_features, verbose, max_leaf_nodes, warm_start)"),
d("_validate_y(self, y)"),
d("decision_function(self, X)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),
d("staged_decision_function(self, X)"),
d("staged_predict(self, X)"),
d("staged_predict_proba(self, X)"),]),
c("GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin)", "/ensemble/gradient_boosting.py<br>Gradient Boosting for regression.  GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.  Read more in the :ref:`User Guide <g...", [
d("__init__(self, loss, learning_rate, n_estimators, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, init, random_state, max_features, alpha, verbose, max_leaf_nodes, warm_start)"),
d("predict(self, X)"),
d("staged_predict(self, X)"),]),
c("HuberLossFunction(RegressionLossFunction)", "/ensemble/gradient_boosting.py<br>Huber loss function for robust regression.  M-Regression proposed in Friedman 2001.  References ---------- J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.", [
d("__call__(self, y, pred, sample_weight)"),
d("__init__(self, n_classes, alpha)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred, sample_weight)"),]),
c("LeastAbsoluteError(RegressionLossFunction)", "/ensemble/gradient_boosting.py<br>Loss function for least absolute deviation (LAD) regression. ", [
d("__call__(self, y, pred, sample_weight)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred)"),]),
c("LeastSquaresError(RegressionLossFunction)", "/ensemble/gradient_boosting.py<br>Loss function for least squares (LS) estimation. Terminal regions need not to be updated for least squares. ", [
d("__call__(self, y, pred, sample_weight)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred)"),
d("update_terminal_regions(self, tree, X, y, residual, y_pred, sample_weight, sample_mask, learning_rate, k)"),]),
c("LogOddsEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py<br>An estimator predicting the log odds ratio.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("LossFunction()", "/ensemble/gradient_boosting.py<br>Abstract base class for various loss functions.  Attributes ---------- K : int     The number of regression trees to be induced;     1 for regression and binary classification;     ``n_classes`` for multi-class classification.", [
d("__call__(self, y, pred, sample_weight)"),
d("__init__(self, n_classes)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, y_pred)"),
d("update_terminal_regions(self, tree, X, y, residual, y_pred, sample_weight, sample_mask, learning_rate, k)"),]),
c("MeanEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py<br>An estimator predicting the mean of the training targets.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("MultinomialDeviance(ClassificationLossFunction)", "/ensemble/gradient_boosting.py<br>Multinomial deviance loss function for multi-class classification.  For multi-class classification we need to fit ``n_classes`` trees at each stage.", [
d("__call__(self, y, pred, sample_weight)"),
d("__init__(self, n_classes)"),
d("_score_to_decision(self, score)"),
d("_score_to_proba(self, score)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred, k)"),]),
c("PriorProbabilityEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py<br>An estimator predicting the probability of each class in the training data.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("QuantileEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py<br>An estimator predicting the alpha-quantile of the training targets.", [
d("__init__(self, alpha)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("QuantileLossFunction(RegressionLossFunction)", "/ensemble/gradient_boosting.py<br>Loss function for quantile regression.  Quantile regression allows to estimate the percentiles of the conditional distribution of the target.", [
d("__call__(self, y, pred, sample_weight)"),
d("__init__(self, n_classes, alpha)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("init_estimator(self)"),
d("negative_gradient(self, y, pred)"),]),
c("RegressionLossFunction()", "/ensemble/gradient_boosting.py<br>Base class for regression loss functions. ", [
d("__init__(self, n_classes)"),]),
c("ScaledLogOddsEstimator(LogOddsEstimator)", "/ensemble/gradient_boosting.py<br>Log odds ratio scaled by 0.5 -- for exponential loss. ", [
]),
c("VerboseReporter(object)", "/ensemble/gradient_boosting.py<br>Reports verbose output to stdout.  If ``verbose1`` output is printed once in a while (when iteration mod verbose_mod is zero).; if larger than 1 then output is printed for each update.", [
d("__init__(self, verbose)"),
d("init(self, est, begin_at_stage)"),
d("update(self, j, est)"),]),
c("ZeroEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py<br>An estimator that simply predicts zero. ", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),]),
c("partial_dependence.py", "/ensemble/partial_dependence.py<br> Partial dependence plots for tree ensembles. ", [
d("_grid_from_X(X, percentiles, grid_resolution)", "Generate a grid of points based on the ``percentiles of ``X``.  The grid is generated by placing ``grid_resolution`` equally spaced points between the ``percentiles`` of each column of ``X``.  Parameters ---------- X : ndarray     The data percentiles : tuple of floats     The percentiles which are ..."),
d("partial_dependence(gbrt, target_variables, grid, X, percentiles, grid_resolution)", "Partial dependence of ``target_variables``.  Partial dependence plots show the dependence between the joint values of the ``target_variables`` and the function represented by the ``gbrt``.  Read more in the :ref:`User Guide <partial_dependence>`.  Parameters ---------- gbrt : BaseGradientBoosting   ..."),
d("plot_partial_dependence(gbrt, X, features, feature_names, label, n_cols, grid_resolution, percentiles, n_jobs, verbose, ax, line_kw, contour_kw)", "Partial dependence plots for ``features``.  The ``len(features)`` plots are arranged in a grid with ``n_cols`` columns. Two-way partial dependence plots are plotted as contour plots.  Read more in the :ref:`User Guide <partial_dependence>`.  Parameters ---------- gbrt : BaseGradientBoosting     A fi..."),]),
c("setup.py", "/ensemble/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("voting_classifier.py", "/ensemble/voting_classifier.py<br> Soft Voting/Majority Rule classifier.  This module contains a Soft Voting/Majority Rule classifier for classification estimators.", [
c("VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin)", "/ensemble/voting_classifier.py<br>Soft Voting/Majority Rule classifier for unfitted estimators.  Read more in the :ref:`User Guide <voting_classifier>`.  Parameters ---------- estimators : list of (string, estimator) tuples     Invoking the `fit` method on the `VotingClassifier` will fit clones     of those original estimators that ...", [
d("__init__(self, estimators, voting, weights)"),
d("_collect_probas(self, X)"),
d("_predict(self, X)"),
d("_predict_proba(self, X)"),
d("fit(self, X, y)"),
d("get_params(self, deep)"),
d("predict(self, X)"),
d("predict_proba(self)"),
d("transform(self, X)"),]),]),
c("weight_boosting.py", "/ensemble/weight_boosting.py<br> Weight Boosting  This module contains weight boosting estimators for both classification and regression.  The module structure is the following:  - The ``BaseWeightBoosting`` base class implements a common ``fit`` method   for all the estimators in the module. Regression and classification   only di...", [
c("AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin)", "/ensemble/weight_boosting.py<br>An AdaBoost classifier.  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classi...", [
d("__init__(self, base_estimator, n_estimators, learning_rate, algorithm, random_state)"),
d("_boost(self, iboost, X, y, sample_weight)"),
d("_boost_discrete(self, iboost, X, y, sample_weight)"),
d("_boost_real(self, iboost, X, y, sample_weight)"),
d("_validate_estimator(self)"),
d("decision_function(self, X)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),
d("staged_decision_function(self, X)"),
d("staged_predict(self, X)"),
d("staged_predict_proba(self, X)"),]),
c("AdaBoostRegressor(BaseWeightBoosting, RegressorMixin)", "/ensemble/weight_boosting.py<br>An AdaBoost regressor.  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As s...", [
d("__init__(self, base_estimator, n_estimators, learning_rate, loss, random_state)"),
d("_boost(self, iboost, X, y, sample_weight)"),
d("_get_median_predict(self, X, limit)"),
d("_validate_estimator(self)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),
d("staged_predict(self, X)"),]),
c("BaseWeightBoosting()", "/ensemble/weight_boosting.py<br>Base class for AdaBoost estimators.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, learning_rate, random_state)"),
d("_boost(self, iboost, X, y, sample_weight)"),
d("_check_sample_weight(self)"),
d("_validate_X_predict(self, X)"),
d("feature_importances_(self)"),
d("fit(self, X, y, sample_weight)"),
d("staged_score(self, X, y, sample_weight)"),]),
d("_samme_proba(estimator, n_classes, X)", "Calculate algorithm 4, step 2, equation c) of Zhu et al [1].  References ---------- .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, 'Multi-class AdaBoost', 2009."),]),]),
c("externals", "/externals<br>", [
c("joblib", "/externals/joblib<br>", [
c("disk.py", "/externals/joblib/disk.py<br> Disk management utilities.", [
d("disk_used(path)", "Return the disk usage in a directory."),
d("memstr_to_kbytes(text)", "Convert a memory text to it's value in kilobytes.     "),
d("mkdirp(d)", "Ensure directory d exists (like mkdir -p on Unix) No guarantee that the directory is writable."),
d("rm_subdirs(path, onerror)", "Remove all subdirectories in this path.  The directory indicated by `path` is left in place, and its subdirectories are erased.  If onerror is set, it is called to handle the error with arguments (func, path, exc_info) where func is os.listdir, os.remove, or os.rmdir; path is the argument to that fu..."),]),
c("format_stack.py", "/externals/joblib/format_stack.py<br> Represent an exception with a lot of information.  Provides 2 useful functions:  format_exc: format an exception into a complete traceback, with full             debugging instruction.  format_outer_frames: format the current position in the stack call.  Adapted from IPython's VerboseTB.", [
d("_fixed_getframes(etb, context, tb_offset)"),
d("_format_traceback_lines(lnum, index, lines, lvals)"),
d("eq_repr(value, repr)"),
d("fix_frame_records_filenames(records)", "Try to fix the filenames in each record from inspect.getinnerframes().  Particularly, modules loaded from within zip files have useless filenames attached to their code object, and inspect.getinnerframes() just uses it."),
d("format_exc(etype, evalue, etb, context, tb_offset)", "Return a nice text document describing the traceback.  Parameters ----------- etype, evalue, etb: as returned by sys.exc_info context: number of lines of the source file to plot tb_offset: the number of stack frame not to use (0  use all)"),
d("format_outer_frames(context, stack_start, stack_end, ignore_ipython)"),
d("format_records(records)"),
d("safe_repr(value)", "Hopefully pretty robust repr equivalent."),
d("uniq_stable(elems)", "uniq_stable(elems) -> list  Return from an iterable, a list of all the unique elements in the input, but maintaining the order in which they first appear.  A naive solution to this problem which just makes a dictionary with the elements as keys fails to respect the stability condition, since diction..."),]),
c("func_inspect.py", "/externals/joblib/func_inspect.py<br> My own variation on function-specific inspect-like features.", [
d("_clean_win_chars(string)", "Windows cannot encode some characters in filename."),
d("filter_args(func, ignore_lst, args, kwargs)", "Filters the given args and kwargs using a list of arguments to ignore, and a function specification.  Parameters ---------- func: callable     Function giving the argument specification ignore_lst: list of strings     List of arguments to ignore (either a name of an argument     in the function spec..."),
d("format_call(func, args, kwargs, object_name)", "Returns a nicely formatted statement displaying the function call with the given arguments."),
d("format_signature(func)"),
d("get_func_code(func)", "Attempts to retrieve a reliable function code hash.  The reason we don't use inspect.getsource is that it caches the source, whereas we want this to be modified on the fly when the function is modified.  Returns ------- func_code: string     The function code source_file: string     The path to the ..."),
d("get_func_name(func, resolv_alias, win_characters)", "Return the function import path (as a list of module names), and a name for the function.  Parameters ---------- func: callable     The func to inspect resolv_alias: boolean, optional     If true, possible local aliases are indicated. win_characters: boolean, optional     If true, substitute special..."),]),
c("hashing.py", "/externals/joblib/hashing.py<br> Fast cryptographic hash of Python objects, with a special case for fast hashing of numpy arrays.", [
c("Hasher(Pickler)", "/externals/joblib/hashing.py<br>A subclass of pickler, to do cryptographic hashing, rather than pickling.", [
d("__init__(self, hash_name)"),
d("_batch_setitems(self, items)"),
d("hash(self, obj, return_digest)"),
d("save(self, obj)"),
d("save_global(self, obj, name, pack)"),
d("save_set(self, set_items)"),]),
c("NumpyHasher(Hasher)", "/externals/joblib/hashing.py<br>Special case the hasher for when numpy is loaded.     ", [
d("__init__(self, hash_name, coerce_mmap)"),
d("save(self, obj)"),]),
c("_ConsistentSet(object)", "/externals/joblib/hashing.py<br>Class used to ensure the hash of Sets is preserved whatever the order of its items.", [
d("__init__(self, set_sequence)"),]),
c("_MyHash(object)", "/externals/joblib/hashing.py<br>Class used to hash objects that won't normally pickle ", [
d("__init__(self)"),]),
d("hash(obj, hash_name, coerce_mmap)", "Quick calculation of a hash to identify uniquely Python objects containing numpy arrays.   Parameters ----------- hash_name: 'md5' or 'sha1'     Hashing algorithm used. sha1 is supposedly safer, but md5 is     faster. coerce_mmap: boolean     Make no difference between np.memmap and np.ndarray"),]),
c("logger.py", "/externals/joblib/logger.py<br> Helpers for logging.  This module needs much love to become useful.", [
c("Logger(object)", "/externals/joblib/logger.py<br>Base class for logging messages.     ", [
d("__init__(self, depth)"),
d("debug(self, msg)"),
d("format(self, obj, indent)"),
d("warn(self, msg)"),]),
c("PrintTime(object)", "/externals/joblib/logger.py<br>Print and log messages while keeping track of time.     ", [
d("__call__(self, msg, total)"),
d("__init__(self, logfile, logdir)"),]),
d("_squeeze_time(t)", "Remove .1s to the time under Windows: this is the time it take to stat files. This is needed to make results similar to timings under Unix, for tests"),
d("format_time(t)"),
d("pformat(obj, indent, depth)"),
d("short_format_time(t)"),]),
c("memory.py", "/externals/joblib/memory.py<br> A context object for caching a function's return value each time it is called with the same input arguments.", [
c("JobLibCollisionWarning(UserWarning)", "/externals/joblib/memory.py<br>Warn that there might be a collision between names of functions.     ", [
]),
c("MemorizedFunc(Logger)", "/externals/joblib/memory.py<br>Callable object decorating a function for caching its return value each time it is called.  All values are cached on the filesystem, in a deep directory structure. Methods are provided to inspect the cache or clean it.  Attributes ---------- func: callable     The original, undecorated, function.  c...", [
d("__call__(self)"),
d("__init__(self, func, cachedir, ignore, mmap_mode, compress, verbose, timestamp)"),
d("__reduce__(self)"),
d("__repr__(self)"),
d("_cached_call(self, args, kwargs)"),
d("_check_previous_func_code(self, stacklevel)"),
d("_get_argument_hash(self)"),
d("_get_func_dir(self, mkdir)"),
d("_get_output_dir(self)"),
d("_hash_func(self)"),
d("_persist_input(self, output_dir, duration, args, kwargs, this_duration_limit)"),
d("_persist_output(self, output, dir)"),
d("_write_func_code(self, filename, func_code, first_line)"),
d("call(self)"),
d("call_and_shelve(self)"),
d("clear(self, warn)"),
d("format_call(self)"),
d("format_signature(self)"),
d("load_output(self, output_dir)"),]),
c("MemorizedResult(Logger)", "/externals/joblib/memory.py<br>Object representing a cached value.  Attributes ---------- cachedir: string     path to root of joblib cache  func: function or string     function whose output is cached. The string case is intended only for     instanciation based on the output of repr() on another instance.     (namely eval(repr(...", [
d("__init__(self, cachedir, func, argument_hash, mmap_mode, verbose, timestamp, metadata)"),
d("__reduce__(self)"),
d("__repr__(self)"),
d("clear(self)"),
d("get(self)"),]),
c("Memory(Logger)", "/externals/joblib/memory.py<br>A context object for caching a function's return value each time it is called with the same input arguments.  All values are cached on the filesystem, in a deep directory structure.  see :ref:`memory_reference`", [
d("__init__(self, cachedir, mmap_mode, compress, verbose)"),
d("__reduce__(self)"),
d("__repr__(self)"),
d("cache(self, func, ignore, verbose, mmap_mode)"),
d("clear(self, warn)"),
d("eval(self, func)"),]),
c("NotMemorizedFunc(object)", "/externals/joblib/memory.py<br>No-op object decorating a function.  This class replaces MemorizedFunc when there is no cache. It provides an identical API but does not write anything on disk.  Attributes ---------- func: callable     Original undecorated function.", [
d("__call__(self)"),
d("__init__(self, func)"),
d("__reduce__(self)"),
d("__repr__(self)"),
d("call_and_shelve(self)"),
d("clear(self, warn)"),]),
c("NotMemorizedResult(object)", "/externals/joblib/memory.py<br>Class representing an arbitrary value.  This class is a replacement for MemorizedResult when there is no cache.", [
d("__getstate__(self)"),
d("__init__(self, value)"),
d("__repr__(self)"),
d("__setstate__(self, state)"),
d("clear(self)"),
d("get(self)"),]),
d("_cache_key_to_dir(cachedir, func, argument_hash)", "Compute directory associated with a given cache key.  func can be a function or a string as returned by _get_func_fullname()."),
d("_get_func_fullname(func)", "Compute the part of part associated with a function.  See code of_cache_key_to_dir() for details"),
d("_load_output(output_dir, func_name, timestamp, metadata, mmap_mode, verbose)", "Load output of a computation."),
d("extract_first_line(func_code)", "Extract the first line information from the function code text if available."),]),
c("my_exceptions.py", "/externals/joblib/my_exceptions.py<br> Exceptions", [
c("JoblibException(Exception)", "/externals/joblib/my_exceptions.py<br>A simple exception with an error message that you can get to.", [
d("__init__(self)"),
d("__reduce__(self)"),
d("__repr__(self)"),]),
c("TransportableException(JoblibException)", "/externals/joblib/my_exceptions.py<br>An exception containing all the info to wrap an original exception and recreate it.", [
d("__init__(self, message, etype)"),
d("__reduce__(self)"),]),
d("_mk_common_exceptions()"),
d("_mk_exception(exception, name)"),]),
c("numpy_pickle.py", "/externals/joblib/numpy_pickle.py<br> Utilities for fast persistence of big data, with optional compression.", [
c("NDArrayWrapper(object)", "/externals/joblib/numpy_pickle.py<br>An object to be persisted instead of numpy arrays.  The only thing this object does, is to carry the filename in which the array has been persisted, and the array subclass.", [
d("__init__(self, filename, subclass, allow_mmap)"),
d("read(self, unpickler)"),]),
c("NumpyPickler(Pickler)", "/externals/joblib/numpy_pickle.py<br>A pickler to persist of big data efficiently.  The main features of this object are:   * persistence of numpy arrays in separate .npy files, for which    I/O is fast.   * optional compression using Zlib, with a special care on avoid    temporaries.", [
d("__init__(self, filename, compress, cache_size)"),
d("_write_array(self, array, filename)"),
d("close(self)"),
d("save(self, obj)"),
d("save_bytes(self, obj)"),]),
c("NumpyUnpickler(Unpickler)", "/externals/joblib/numpy_pickle.py<br>A subclass of the Unpickler to unpickle our numpy pickles.     ", [
d("__init__(self, filename, file_handle, mmap_mode)"),
d("_open_pickle(self, file_handle)"),
d("load_build(self)"),]),
c("ZNDArrayWrapper(NDArrayWrapper)", "/externals/joblib/numpy_pickle.py<br>An object to be persisted instead of numpy arrays.  This object store the Zfile filename in which the data array has been persisted, and the meta information to retrieve it.  The reason that we store the raw buffer data of the array and the meta information, rather than array representation routine ...", [
d("__init__(self, filename, init_args, state)"),
d("read(self, unpickler)"),]),
c("ZipNumpyUnpickler(NumpyUnpickler)", "/externals/joblib/numpy_pickle.py<br>A subclass of our Unpickler to unpickle on the fly from compressed storage.", [
d("__init__(self, filename, file_handle)"),
d("_open_pickle(self, file_handle)"),]),
d("_read_magic(file_handle)", "Utility to check the magic signature of a file identifying it as a Zfile"),
d("dump(value, filename, compress, cache_size)", "Fast persistence of an arbitrary Python object into a files, with dedicated storage for numpy arrays.  Parameters ----------- value: any Python object     The object to store to disk filename: string     The name of the file in which it is to be stored compress: integer for 0 to 9, optional     Opti..."),
d("hex_str(an_int)", "Converts an int to an hexadecimal string     "),
d("load(filename, mmap_mode)", "Reconstruct a Python object from a file persisted with joblib.dump.  Parameters ----------- filename: string     The name of the file from which to load the object mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional     If not None, the arrays are memory-mapped from the disk. This     mode has no effe..."),
d("read_zfile(file_handle)", "Read the z-file and return the content as a string  Z-files are raw data compressed with zlib used internally by joblib for persistence. Backward compatibility is not guaranteed. Do not use for external purposes."),
d("write_zfile(file_handle, data, compress)", "Write the data in the given file as a Z-file.  Z-files are raw data compressed with zlib used internally by joblib for persistence. Backward compatibility is not guarantied. Do not use for external purposes."),]),
c("parallel.py", "/externals/joblib/parallel.py<br> Helpers for embarrassingly parallel code.", [
c("BatchCompletionCallBack(object)", "/externals/joblib/parallel.py<br>Callback used by joblib.Parallel's multiprocessing backend.  This callable is executed by the parent process whenever a worker process has returned the results of a batch of tasks.  It is used for progress reporting, to update estimate of the batch processing duration and to schedule the next batch ...", [
d("__call__(self, out)"),
d("__init__(self, dispatch_timestamp, batch_size, parallel)"),]),
c("BatchedCalls(object)", "/externals/joblib/parallel.py<br>Wrap a sequence of (func, args, kwargs) tuples as a single callable", [
d("__call__(self)"),
d("__init__(self, iterator_slice)"),
d("__len__(self)"),]),
c("ImmediateComputeBatch(object)", "/externals/joblib/parallel.py<br>Sequential computation of a batch of tasks.  This replicates the async computation API but actually does not delay the computations when joblib.Parallel runs in sequential mode.", [
d("__init__(self, batch)"),
d("get(self)"),]),
c("Parallel(Logger)", "/externals/joblib/parallel.py<br>Helper class for readable parallel mapping.  Parameters ----------- n_jobs: int, default: 1     The maximum number of concurrently running jobs, such as the number     of Python worker processes when backend'multiprocessing'     or the size of the thread-pool when backend'threading'.     If -1 all C...", [
d("__call__(self, iterable)"),
d("__enter__(self)"),
d("__exit__(self, exc_type, exc_value, traceback)"),
d("__init__(self, n_jobs, backend, verbose, pre_dispatch, batch_size, temp_folder, max_nbytes, mmap_mode)"),
d("__repr__(self)"),
d("_dispatch(self, batch)"),
d("_effective_n_jobs(self)"),
d("_initialize_pool(self)"),
d("_print(self, msg, msg_args)"),
d("_terminate_pool(self)"),
d("dispatch_next(self)"),
d("dispatch_one_batch(self, iterator)"),
d("print_progress(self)"),
d("retrieve(self)"),]),
c("SafeFunction(object)", "/externals/joblib/parallel.py<br>Wraps a function to make it exception with full traceback in their representation. Useful for parallel computing with multiprocessing, for which exceptions cannot be captured.", [
d("__call__(self)"),
d("__init__(self, func)"),]),
c("WorkerInterrupt(Exception)", "/externals/joblib/parallel.py<br>An exception that is not KeyboardInterrupt to allow subprocesses to be interrupted.", [
]),
d("_verbosity_filter(index, verbose)", "Returns False for indices increasingly apart, the distance depending on the value of verbose.  We use a lag increasing as the square of index"),
d("cpu_count()", "Return the number of CPUs.     "),
d("delayed(function, check_pickle)", "Decorator used to capture the arguments of a function.  Pass `check_pickleFalse` when:  - performing a possibly repeated check is too costly and has been done   already once outside of the call to delayed.  - when used in conjunction `Parallel(backend'threading')`."),]),
c("pool.py", "/externals/joblib/pool.py<br> Custom implementation of multiprocessing.Pool with custom pickler  This module provides efficient ways of working with data stored in shared memory with numpy.memmap arrays without inducing any memory copy between the parent and child processes.  This module should not be imported if multiprocessing...", [
c("ArrayMemmapReducer(object)", "/externals/joblib/pool.py<br>Reducer callable to dump large arrays to memmap files.  Parameters ---------- max_nbytes: int     Threshold to trigger memmaping of large arrays to files created     a folder. temp_folder: str     Path of a folder where files for backing memmaped arrays are created. mmap_mode: 'r', 'r+' or 'c'     M...", [
d("__call__(self, a)"),
d("__init__(self, max_nbytes, temp_folder, mmap_mode, verbose, context_id, prewarm)"),]),
c("CustomizablePickler(Pickler)", "/externals/joblib/pool.py<br>Pickler that accepts custom reducers.  HIGHEST_PROTOCOL is selected by default as this pickler is used to pickle ephemeral datastructures for interprocess communication hence no backward compatibility is required.  `reducers` is expected expected to be a dictionary with key/values being `(type, call...", [
d("__init__(self, writer, reducers, protocol)"),
d("register(self, type, reduce_func)"),]),
c("CustomizablePicklingQueue(object)", "/externals/joblib/pool.py<br>Locked Pipe implementation that uses a customizable pickler.  This class is an alternative to the multiprocessing implementation of SimpleQueue in order to make it possible to pass custom pickling reducers, for instance to avoid memory copy when passing memmory mapped datastructures.  `reducers` is ...", [
d("__getstate__(self)"),
d("__init__(self, context, reducers)"),
d("__setstate__(self, state)"),
d("_make_methods(self)"),
d("empty(self)"),]),
c("MemmapingPool(PicklingPool)", "/externals/joblib/pool.py<br>Process pool that shares large arrays to avoid memory copy.  This drop-in replacement for `multiprocessing.pool.Pool` makes it possible to work efficiently with shared memory in a numpy context.  Existing instances of numpy.memmap are preserved: the child suprocesses will have access to the same sha...", [
d("__init__(self, processes, temp_folder, max_nbytes, mmap_mode, forward_reducers, backward_reducers, verbose, context_id, prewarm)"),
d("terminate(self)"),]),
c("PicklingPool(Pool)", "/externals/joblib/pool.py<br>Pool implementation with customizable pickling reducers.  This is useful to control how data is shipped between processes and makes it possible to use shared memory without useless copies induces by the default pickling methods of the original objects passed as arguments to dispatch.  `forward_reduc...", [
d("__init__(self, processes, forward_reducers, backward_reducers)"),
d("_setup_queues(self)"),]),
d("_get_backing_memmap(a)", "Recursively look up the original np.memmap instance base if any"),
d("_reduce_memmap_backed(a, m)", "Pickling reduction for memmap backed arrays  a is expected to be an instance of np.ndarray (or np.memmap) m is expected to be an instance of np.memmap on the top of the ``base`` attribute ancestry of a. ``m.base`` should be the real python mmap object."),
d("_strided_from_memmap(filename, dtype, mode, offset, order, shape, strides, total_buffer_len)", "Reconstruct an array view on a memmory mapped file"),
d("delete_folder(folder_path)", "Utility function to cleanup a temporary folder if still existing"),
d("has_shareable_memory(a)", "Return True if a is backed by some mmap buffer directly or not"),
d("reduce_memmap(a)", "Pickle the descriptors of a memmap instance to reopen on same file"),]),
c("testing.py", "/externals/joblib/testing.py<br> Helper for testing.", [
d("warnings_to_stdout()", "Redirect all warnings to stdout.     "),]),]),
c("setup.py", "/externals/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("six.py", "/externals/six.py<br> Utilities for writing code that runs on Python 2 and 3", [
c("Module_six_moves_urllib()", "/externals/six.py<br>Create a six.moves.urllib namespace that resembles the Python 3 namespace", [
]),
c("Module_six_moves_urllib_error()", "/externals/six.py<br>Lazy loading of moved objects in six.moves.urllib_error", [
]),
c("Module_six_moves_urllib_parse()", "/externals/six.py<br>Lazy loading of moved objects in six.moves.urllib_parse", [
]),
c("Module_six_moves_urllib_request()", "/externals/six.py<br>Lazy loading of moved objects in six.moves.urllib_request", [
]),
c("Module_six_moves_urllib_response()", "/externals/six.py<br>Lazy loading of moved objects in six.moves.urllib_response", [
]),
c("Module_six_moves_urllib_robotparser()", "/externals/six.py<br>Lazy loading of moved objects in six.moves.urllib_robotparser", [
]),
c("MovedAttribute(_LazyDescr)", "/externals/six.py<br>", [
d("__init__(self, name, old_mod, new_mod, old_attr, new_attr)"),
d("_resolve(self)"),]),
c("MovedModule(_LazyDescr)", "/externals/six.py<br>", [
d("__init__(self, name, old, new)"),
d("_resolve(self)"),]),
c("_LazyDescr(object)", "/externals/six.py<br>", [
d("__get__(self, obj, tp)"),
d("__init__(self, name)"),]),
c("_MovedItems()", "/externals/six.py<br>Lazy loading of moved objects", [
]),
d("_add_doc(func, doc)", "Add documentation to a function."),
d("_import_module(name)", "Import module, returning the module after the last dot."),
d("add_metaclass(metaclass)", "Class decorator for creating a class with a metaclass."),
d("add_move(move)", "Add an item to six.moves."),
d("iteritems(d)", "Return an iterator over the (key, value) pairs of a dictionary."),
d("iterkeys(d)", "Return an iterator over the keys of a dictionary."),
d("iterlists(d)", "Return an iterator over the (key, [values]) pairs of a dictionary."),
d("itervalues(d)", "Return an iterator over the values of a dictionary."),
d("remove_move(name)", "Remove item from six.moves."),
d("with_metaclass(meta)", "Create a base class with a metaclass."),]),]),
c("feature_extraction", "/feature_extraction<br>", [
c("tests", "/feature_extraction/tests<br>", [
c("test_dict_vectorizer.py", "/feature_extraction/tests/test_dict_vectorizer.py<br>", [
d("test_deterministic_vocabulary()"),
d("test_dictvectorizer()"),
d("test_feature_selection()"),
d("test_one_of_k()"),
d("test_unseen_or_no_features()"),]),
c("test_feature_hasher.py", "/feature_extraction/tests/test_feature_hasher.py<br>", [
d("test_feature_hasher_dicts()"),
d("test_feature_hasher_pairs()"),
d("test_feature_hasher_strings()"),
d("test_hash_empty_input()"),
d("test_hasher_invalid_input()"),
d("test_hasher_set_params()"),
d("test_hasher_zeros()"),]),
c("test_image.py", "/feature_extraction/tests/test_image.py<br>", [
d("_downsampled_lena()"),
d("_make_images(lena)"),
d("_orange_lena(lena)"),
d("test_connect_regions()"),
d("test_connect_regions_with_grid()"),
d("test_extract_patches_all()"),
d("test_extract_patches_all_color()"),
d("test_extract_patches_all_rect()"),
d("test_extract_patches_max_patches()"),
d("test_extract_patches_square()"),
d("test_extract_patches_strided()"),
d("test_grid_to_graph()"),
d("test_img_to_graph()"),
d("test_patch_extractor_all_patches()"),
d("test_patch_extractor_color()"),
d("test_patch_extractor_fit()"),
d("test_patch_extractor_max_patches()"),
d("test_patch_extractor_max_patches_default()"),
d("test_reconstruct_patches_perfect()"),
d("test_reconstruct_patches_perfect_color()"),
d("test_width_patch()"),]),
c("test_text.py", "/feature_extraction/tests/test_text.py<br>", [
d("lazy_analyze(s)"),
d("split_tokenize(s)"),
d("strip_eacute(s)"),
d("test_char_ngram_analyzer()"),
d("test_char_wb_ngram_analyzer()"),
d("test_count_binary_occurrences()"),
d("test_count_vectorizer_max_features()"),
d("test_count_vectorizer_pipeline_grid_selection()"),
d("test_countvectorizer_custom_vocabulary()"),
d("test_countvectorizer_custom_vocabulary_gap_index()"),
d("test_countvectorizer_custom_vocabulary_pipeline()"),
d("test_countvectorizer_custom_vocabulary_repeated_indeces()"),
d("test_countvectorizer_empty_vocabulary()"),
d("test_countvectorizer_stop_words()"),
d("test_feature_names()"),
d("test_fit_countvectorizer_twice()"),
d("test_hashed_binary_occurrences()"),
d("test_hashing_vectorizer()"),
d("test_hashingvectorizer_nan_in_docs()"),
d("test_non_unique_vocab()"),
d("test_pickling_transformer()"),
d("test_pickling_vectorizer()"),
d("test_stop_words_removal()"),
d("test_strip_accents()"),
d("test_sublinear_tf()"),
d("test_tf_idf_smoothing()"),
d("test_tfidf_no_smoothing()"),
d("test_tfidf_vectorizer_setters()"),
d("test_tfidf_vectorizer_with_fixed_vocabulary()"),
d("test_tfidfvectorizer_binary()"),
d("test_tfidfvectorizer_export_idf()"),
d("test_to_ascii()"),
d("test_unicode_decode_error()"),
d("test_vectorizer()"),
d("test_vectorizer_inverse_transform()"),
d("test_vectorizer_max_df()"),
d("test_vectorizer_max_features()"),
d("test_vectorizer_min_df()"),
d("test_vectorizer_pipeline_cross_validation()"),
d("test_vectorizer_pipeline_grid_selection()"),
d("test_vectorizer_unicode()"),
d("test_vectorizer_vocab_clone()"),
d("test_word_analyzer_unigrams()"),
d("test_word_analyzer_unigrams_and_bigrams()"),
d("uppercase(s)"),]),]),
c("dict_vectorizer.py", "/feature_extraction/dict_vectorizer.py<br>", [
c("DictVectorizer(BaseEstimator, TransformerMixin)", "/feature_extraction/dict_vectorizer.py<br>Transforms lists of feature-value mappings to vectors.  This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.  When feature values are strings, this transformer will do a binary...", [
d("__init__(self, dtype, separator, sparse, sort)"),
d("_transform(self, X, fitting)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("get_feature_names(self)"),
d("inverse_transform(self, X, dict_type)"),
d("restrict(self, support, indices)"),
d("transform(self, X, y)"),]),
d("_tosequence(X)", "Turn X into a sequence or ndarray, avoiding a copy if possible."),]),
c("hashing.py", "/feature_extraction/hashing.py<br>", [
c("FeatureHasher(BaseEstimator, TransformerMixin)", "/feature_extraction/hashing.py<br>Implements feature hashing, aka the hashing trick.  This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.  Feature ...", [
d("__init__(self, n_features, input_type, dtype, non_negative)"),
d("_validate_params(n_features, input_type)"),
d("fit(self, X, y)"),
d("transform(self, raw_X, y)"),]),
d("_iteritems(d)", "Like d.iteritems, but accepts any collections.Mapping."),]),
c("image.py", "/feature_extraction/image.py<br> The sklearn.feature_extraction.image submodule gathers utilities to extract features from images.", [
c("PatchExtractor(BaseEstimator)", "/feature_extraction/image.py<br>Extracts patches from a collection of images  Read more in the :ref:`User Guide <image_feature_extraction>`.  Parameters ---------- patch_size : tuple of ints (patch_height, patch_width)     the dimensions of one patch  max_patches : integer or float, optional default is None     The maximum number ...", [
d("__init__(self, patch_size, max_patches, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X)"),]),
d("_compute_gradient_3d(edges, img)"),
d("_compute_n_patches(i_h, i_w, p_h, p_w, max_patches)", "Compute the number of patches that will be extracted in an image.  Read more in the :ref:`User Guide <image_feature_extraction>`.  Parameters ---------- i_h : int     The image height i_w : int     The image with p_h : int     The height of a patch p_w : int     The width of a patch max_patches : in..."),
d("_make_edges_3d(n_x, n_y, n_z)", "Returns a list of edges for a 3D image.  Parameters  n_x: integer     The size of the grid in the x direction. n_y: integer     The size of the grid in the y direction. n_z: integer, optional     The size of the grid in the z direction, defaults to 1"),
d("_mask_edges_weights(mask, edges, weights)", "Apply a mask to edges (weighted or not)"),
d("_to_graph(n_x, n_y, n_z, mask, img, return_as, dtype)", "Auxiliary function for img_to_graph and grid_to_graph     "),
d("extract_patches(arr, patch_shape, extraction_step)", "Extracts patches of any n-dimensional array in place using strides.  Given an n-dimensional array it will return a 2n-dimensional array with the first n dimensions indexing patch position and the last n indexing the patch content. This operation is immediate (O(1)). A reshape performed on the first ..."),
d("extract_patches_2d(image, patch_size, max_patches, random_state)", "Reshape a 2D image into a collection of patches  The resulting patches are allocated in a dedicated array.  Read more in the :ref:`User Guide <image_feature_extraction>`.  Parameters ---------- image : array, shape  (image_height, image_width) or     (image_height, image_width, n_channels)     The o..."),
d("grid_to_graph(n_x, n_y, n_z, mask, return_as, dtype)", "Graph of the pixel-to-pixel connections  Edges exist if 2 voxels are connected.  Parameters ---------- n_x : int     Dimension in x axis n_y : int     Dimension in y axis n_z : int, optional, default 1     Dimension in z axis mask : ndarray of booleans, optional     An optional mask of the image, to..."),
d("img_to_graph(img, mask, return_as, dtype)", "Graph of the pixel-to-pixel gradient connections  Edges are weighted with the gradient values.  Read more in the :ref:`User Guide <image_feature_extraction>`.  Parameters ---------- img : ndarray, 2D or 3D     2D or 3D image mask : ndarray of booleans, optional     An optional mask of the image, to ..."),
d("reconstruct_from_patches_2d(patches, image_size)", "Reconstruct the image from all of its patches.  Patches are assumed to overlap and the image is constructed by filling in the patches from left to right, top to bottom, averaging the overlapping regions.  Read more in the :ref:`User Guide <image_feature_extraction>`.  Parameters ---------- patches :..."),]),
c("setup.py", "/feature_extraction/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("text.py", "/feature_extraction/text.py<br> The sklearn.feature_extraction.text submodule gathers utilities to build feature vectors from text documents.", [
c("CountVectorizer(BaseEstimator, VectorizerMixin)", "/feature_extraction/text.py<br>Convert a collection of text documents to a matrix of token counts  This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix.  If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number ...", [
d("__init__(self, input, encoding, decode_error, strip_accents, lowercase, preprocessor, tokenizer, stop_words, token_pattern, ngram_range, analyzer, max_df, min_df, max_features, vocabulary, binary, dtype)"),
d("_count_vocab(self, raw_documents, fixed_vocab)"),
d("_limit_features(self, X, vocabulary, high, low, limit)"),
d("_sort_features(self, X, vocabulary)"),
d("fit(self, raw_documents, y)"),
d("fit_transform(self, raw_documents, y)"),
d("get_feature_names(self)"),
d("inverse_transform(self, X)"),
d("transform(self, raw_documents)"),]),
c("HashingVectorizer(BaseEstimator, VectorizerMixin)", "/feature_extraction/text.py<br>Convert a collection of text documents to a matrix of token occurrences  It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm'l1' or projected on the euclidean unit sphe...", [
d("__init__(self, input, encoding, decode_error, strip_accents, lowercase, preprocessor, tokenizer, stop_words, token_pattern, ngram_range, analyzer, n_features, binary, norm, non_negative, dtype)"),
d("_get_hasher(self)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("TfidfTransformer(BaseEstimator, TransformerMixin)", "/feature_extraction/text.py<br>Transform a count matrix to a normalized tf or tf-idf representation  Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.  The goal of usi...", [
d("__init__(self, norm, use_idf, smooth_idf, sublinear_tf)"),
d("fit(self, X, y)"),
d("idf_(self)"),
d("transform(self, X, copy)"),]),
c("TfidfVectorizer(CountVectorizer)", "/feature_extraction/text.py<br>Convert a collection of raw documents to a matrix of TF-IDF features.  Equivalent to CountVectorizer followed by TfidfTransformer.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- input : string {'filename', 'file', 'content'}     If 'filename', the sequence pass...", [
d("__init__(self, input, encoding, decode_error, strip_accents, lowercase, preprocessor, tokenizer, analyzer, stop_words, token_pattern, ngram_range, max_df, min_df, max_features, vocabulary, binary, dtype, norm, use_idf, smooth_idf, sublinear_tf)"),
d("fit(self, raw_documents, y)"),
d("fit_transform(self, raw_documents, y)"),
d("idf_(self)"),
d("norm(self)"),
d("norm(self, value)"),
d("smooth_idf(self)"),
d("smooth_idf(self, value)"),
d("sublinear_tf(self)"),
d("sublinear_tf(self, value)"),
d("transform(self, raw_documents, copy)"),
d("use_idf(self)"),
d("use_idf(self, value)"),]),
c("VectorizerMixin(object)", "/feature_extraction/text.py<br>Provides common code for text vectorizers (tokenization logic).", [
d("_char_ngrams(self, text_document)"),
d("_char_wb_ngrams(self, text_document)"),
d("_check_vocabulary(self)"),
d("_validate_vocabulary(self)"),
d("_word_ngrams(self, tokens, stop_words)"),
d("build_analyzer(self)"),
d("build_preprocessor(self)"),
d("build_tokenizer(self)"),
d("decode(self, doc)"),
d("fixed_vocabulary(self)"),
d("get_stop_words(self)"),]),
d("_check_stop_list(stop)"),
d("_document_frequency(X)", "Count the number of non-zero values for each feature in sparse X."),
d("_make_int_array()", "Construct an array.array of a type suitable for scipy.sparse indices."),
d("strip_accents_ascii(s)", "Transform accentuated unicode symbols into ascii or nothing  Warning: this solution is only suited for languages that have a direct transliteration to ASCII symbols.  See also -------- strip_accents_unicode     Remove accentuated char for any unicode symbol."),
d("strip_accents_unicode(s)", "Transform accentuated unicode symbols into their simple counterpart  Warning: the python-level loop and join operations make this implementation 20 times slower than the strip_accents_ascii basic normalization.  See also -------- strip_accents_ascii     Remove accentuated char for any unicode symbol..."),
d("strip_tags(s)", "Basic regexp based HTML / XML tag stripper function  For serious HTML/XML preprocessing you should rather use an external library such as lxml or BeautifulSoup."),]),]),
c("feature_selection", "/feature_selection<br>", [
c("tests", "/feature_selection/tests<br>", [
c("test_base.py", "/feature_selection/tests/test_base.py<br>", [
c("StepSelector(SelectorMixin, BaseEstimator)", "/feature_selection/tests/test_base.py<br>Retain every `step` features (beginning with 0)", [
d("__init__(self, step)"),
d("_get_support_mask(self)"),
d("fit(self, X, y)"),]),
d("test_get_support()"),
d("test_inverse_transform_dense()"),
d("test_inverse_transform_sparse()"),
d("test_transform_dense()"),
d("test_transform_sparse()"),]),
c("test_chi2.py", "/feature_selection/tests/test_chi2.py<br> Tests for chi2, currently the only feature selection function designed specifically to work with sparse matrices.", [
d("mkchi2(k)", "Make k-best chi2 selector"),
d("test_chi2()"),
d("test_chi2_coo()"),
d("test_chi2_negative()"),
d("test_chisquare()"),]),
c("test_feature_select.py", "/feature_selection/tests/test_feature_select.py<br> Todo: cross-check the F-value with stats model", [
d("assert_best_scores_kept(score_filter)"),
d("test_f_classif()"),
d("test_f_classif_constant_feature()"),
d("test_f_classif_multi_class()"),
d("test_f_oneway_ints()"),
d("test_f_oneway_vs_scipy_stats()"),
d("test_f_regression()"),
d("test_f_regression_center()"),
d("test_f_regression_input_dtype()"),
d("test_invalid_k()"),
d("test_invalid_percentile()"),
d("test_nans()"),
d("test_no_feature_selected()"),
d("test_score_func_error()"),
d("test_select_fdr_regression()"),
d("test_select_fwe_regression()"),
d("test_select_heuristics_classif()"),
d("test_select_heuristics_regression()"),
d("test_select_kbest_all()"),
d("test_select_kbest_classif()"),
d("test_select_kbest_regression()"),
d("test_select_kbest_zero()"),
d("test_select_percentile_classif()"),
d("test_select_percentile_classif_sparse()"),
d("test_select_percentile_regression()"),
d("test_select_percentile_regression_full()"),
d("test_selectkbest_tiebreaking()"),
d("test_selectpercentile_tiebreaking()"),
d("test_tied_pvalues()"),
d("test_tied_scores()"),]),
c("test_from_model.py", "/feature_selection/tests/test_from_model.py<br>", [
d("test_invalid_input()"),
d("test_transform_linear_model()"),]),
c("test_rfe.py", "/feature_selection/tests/test_rfe.py<br> Testing Recursive feature elimination", [
c("MockClassifier(object)", "/feature_selection/tests/test_rfe.py<br>Dummy classifier to test recursive feature ellimination", [
d("__init__(self, foo_param)"),
d("fit(self, X, Y)"),
d("get_params(self, deep)"),
d("predict(self, T)"),
d("score(self, X, Y)"),
d("set_params(self)"),]),
d("test_number_of_subsets_of_features()"),
d("test_rfe()"),
d("test_rfe_deprecation_estimator_params()"),
d("test_rfe_estimator_tags()"),
d("test_rfe_features_importance()"),
d("test_rfe_min_step()"),
d("test_rfe_mockclassifier()"),
d("test_rfe_set_params()"),
d("test_rfecv()"),
d("test_rfecv_mockclassifier()"),]),
c("test_variance_threshold.py", "/feature_selection/tests/test_variance_threshold.py<br>", [
d("test_variance_threshold()"),
d("test_zero_variance()"),]),]),
c("base.py", "/feature_selection/base.py<br> Generic feature selection mixin", [
c("SelectorMixin()", "/feature_selection/base.py<br>Tranformer mixin that performs feature selection given a support mask  This mixin provides a feature selector implementation with `transform` and `inverse_transform` functionality given an implementation of `_get_support_mask`.", [
d("_get_support_mask(self)"),
d("get_support(self, indices)"),
d("inverse_transform(self, X)"),
d("transform(self, X)"),]),]),
c("from_model.py", "/feature_selection/from_model.py<br>", [
c("_LearntSelectorMixin(TransformerMixin)", "/feature_selection/from_model.py<br>Transformer mixin selecting features based on importance weights.  This implementation can be mixin on any estimator that exposes a ``feature_importances_`` or ``coef_`` attribute to evaluate the relative importance of individual features for feature selection.", [
d("transform(self, X, threshold)"),]),]),
c("rfe.py", "/feature_selection/rfe.py<br> Recursive feature elimination for feature ranking", [
c("RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin)", "/feature_selection/rfe.py<br>Feature ranking with recursive feature elimination.  Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, ...", [
d("__init__(self, estimator, n_features_to_select, step, estimator_params, verbose)"),
d("_estimator_type(self)"),
d("_fit(self, X, y, step_score)"),
d("_get_support_mask(self)"),
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),
d("score(self, X, y)"),]),
c("RFECV(RFE, MetaEstimatorMixin)", "/feature_selection/rfe.py<br>Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.  Read more in the :ref:`User Guide <rfe>`.  Parameters ---------- estimator : object     A supervised learning estimator with a `fit` method that updates a     `coef_` attribute that hold...", [
d("__init__(self, estimator, step, cv, scoring, estimator_params, verbose)"),
d("fit(self, X, y)"),]),]),
c("univariate_selection.py", "/feature_selection/univariate_selection.py<br> Univariate features selection.", [
c("GenericUnivariateSelect(_BaseFilter)", "/feature_selection/univariate_selection.py<br>Univariate feature selector with configurable strategy.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  mode : {'percentile', 'k_best', 'fpr...", [
d("__init__(self, score_func, mode, param)"),
d("_check_params(self, X, y)"),
d("_get_support_mask(self)"),
d("_make_selector(self)"),]),
c("SelectFdr(_BaseFilter)", "/feature_selection/univariate_selection.py<br>Filter: Select the p-values for an estimated false discovery rate  This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound on the expected false discovery rate.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Functi...", [
d("__init__(self, score_func, alpha)"),
d("_get_support_mask(self)"),]),
c("SelectFpr(_BaseFilter)", "/feature_selection/univariate_selection.py<br>Filter: Select the pvalues below alpha based on a FPR test.  FPR test stands for False Positive Rate test. It controls the total amount of false detections.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays...", [
d("__init__(self, score_func, alpha)"),
d("_get_support_mask(self)"),]),
c("SelectFwe(_BaseFilter)", "/feature_selection/univariate_selection.py<br>Filter: Select the p-values corresponding to Family-wise error rate  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  alpha : float, optional ...", [
d("__init__(self, score_func, alpha)"),
d("_get_support_mask(self)"),]),
c("SelectKBest(_BaseFilter)", "/feature_selection/univariate_selection.py<br>Select features according to the k highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  k : int or 'all', optional, default10    ...", [
d("__init__(self, score_func, k)"),
d("_check_params(self, X, y)"),
d("_get_support_mask(self)"),]),
c("SelectPercentile(_BaseFilter)", "/feature_selection/univariate_selection.py<br>Select features according to a percentile of the highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  percentile : int, optional,...", [
d("__init__(self, score_func, percentile)"),
d("_check_params(self, X, y)"),
d("_get_support_mask(self)"),]),
c("_BaseFilter(BaseEstimator, SelectorMixin)", "/feature_selection/univariate_selection.py<br>Initialize the univariate feature selection.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).", [
d("__init__(self, score_func)"),
d("_check_params(self, X, y)"),
d("fit(self, X, y)"),]),
d("_chisquare(f_obs, f_exp)", "Fast replacement for scipy.stats.chisquare.  Version from https://github.com/scipy/scipy/pull/2525 with additional optimizations."),
d("_clean_nans(scores)", "Fixes Issue #1240: NaNs can't be properly compared, so change them to the smallest value of scores's dtype. -inf seems to be unreliable."),
d("chi2(X, y)", "Compute chi-squared stats between each non-negative feature and class.  This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in docu..."),
d("f_classif(X, y)", "Compute the ANOVA F-value for the provided sample.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- X : {array-like, sparse matrix} shape  [n_samples, n_features]     The set of regressors that will tested sequentially.  y : array of shape(n_samples)     The..."),
d("f_oneway()", "Performs a 1-way ANOVA.  The one-way ANOVA tests the null hypothesis that 2 or more groups have the same population mean. The test is applied to samples from two or more groups, possibly with differing sizes.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- ..."),
d("f_regression(X, y, center)", "Univariate linear regression tests.  Quick linear model for testing the effect of a single regressor, sequentially for many regressors.  This is done in 3 steps:  1. The regressor of interest and the data are orthogonalized    wrt constant regressors. 2. The cross correlation between data and regres..."),]),
c("variance_threshold.py", "/feature_selection/variance_threshold.py<br>", [
c("VarianceThreshold(BaseEstimator, SelectorMixin)", "/feature_selection/variance_threshold.py<br>Feature selector that removes all low-variance features.  This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.  Read more in the :ref:`User Guide <variance_threshold>`.  Parameters ---------- threshold : float, ...", [
d("__init__(self, threshold)"),
d("_get_support_mask(self)"),
d("fit(self, X, y)"),]),]),]),
c("gaussian_process", "/gaussian_process<br>", [
c("tests", "/gaussian_process/tests<br>", [
c("test_gaussian_process.py", "/gaussian_process/tests/test_gaussian_process.py<br> Testing for Gaussian Process module (sklearn.gaussian_process)", [
d("test_1d(regr, corr, random_start, beta0)"),
d("test_2d(regr, corr, random_start, beta0)"),
d("test_2d_2d(regr, corr, random_start, beta0)"),
d("test_more_builtin_correlation_models(random_start)"),
d("test_mse_solving()"),
d("test_no_normalize()"),
d("test_ordinary_kriging()"),
d("test_random_starts()"),
d("test_wrong_number_of_outputs()"),]),]),
c("correlation_models.py", "/gaussian_process/correlation_models.py<br> The built-in correlation models submodule for the gaussian_process module.", [
d("absolute_exponential(theta, d)", "Absolute exponential autocorrelation model. (Ornstein-Uhlenbeck stochastic process)::                                        n     theta, d --> r(theta, d)  exp(  sum  - theta_i * |d_i| )                                     i  1  Parameters ---------- theta : array_like     An array with shape 1 (is..."),
d("cubic(theta, d)", "Cubic correlation model::      theta, d --> r(theta, d)        n      prod max(0, 1 - 3(theta_j*d_ij)^2 + 2(theta_j*d_ij)^3) ,  i  1,...,m     j  1  Parameters ---------- theta : array_like     An array with shape 1 (isotropic) or n (anisotropic) giving the     autocorrelation parameter(s).  d : arr..."),
d("generalized_exponential(theta, d)", "Generalized exponential correlation model. (Useful when one does not know the smoothness of the function to be predicted.)::                                        n     theta, d --> r(theta, d)  exp(  sum  - theta_i * |d_i|^p )                                     i  1  Parameters ---------- theta :..."),
d("linear(theta, d)", "Linear correlation model::      theta, d --> r(theta, d)            n         prod max(0, 1 - theta_j*d_ij) ,  i  1,...,m         j  1  Parameters ---------- theta : array_like     An array with shape 1 (isotropic) or n (anisotropic) giving the     autocorrelation parameter(s).  d : array_like     A..."),
d("pure_nugget(theta, d)", "Spatial independence correlation model (pure nugget). (Useful when one wants to solve an ordinary least squares problem!)::                                         n     theta, d --> r(theta, d)  1 if   sum |d_i|  0                                      i  1                                0 otherwise..."),
d("squared_exponential(theta, d)", "Squared exponential correlation model (Radial Basis Function). (Infinitely differentiable stochastic process, very smooth)::                                        n     theta, d --> r(theta, d)  exp(  sum  - theta_i * (d_i)^2 )                                     i  1  Parameters ---------- theta :..."),]),
c("gaussian_process.py", "/gaussian_process/gaussian_process.py<br>", [
c("GaussianProcess(BaseEstimator, RegressorMixin)", "/gaussian_process/gaussian_process.py<br>The Gaussian Process model class.  Read more in the :ref:`User Guide <gaussian_process>`.  Parameters ---------- regr : string or callable, optional     A regression function returning an array of outputs of the linear     regression functional basis. The number of observations n_samples     should ...", [
d("__init__(self, regr, corr, beta0, storage_mode, verbose, theta0, thetaL, thetaU, optimizer, random_start, normalize, nugget, random_state)"),
d("_arg_max_reduced_likelihood_function(self)"),
d("_check_params(self, n_samples)"),
d("fit(self, X, y)"),
d("predict(self, X, eval_MSE, batch_size)"),
d("reduced_likelihood_function(self, theta)"),]),
d("l1_cross_distances(X)", "Computes the nonzero componentwise L1 cross-distances between the vectors in X.  Parameters ----------  X: array_like     An array with shape (n_samples, n_features)  Returns -------  D: array with shape (n_samples * (n_samples - 1) / 2, n_features)     The array of componentwise L1 cross-distances...."),]),
c("regression_models.py", "/gaussian_process/regression_models.py<br> The built-in regression models submodule for the gaussian_process module.", [
d("constant(x)", "Zero order polynomial (constant, p  1) regression model.  x --> f(x)  1  Parameters ---------- x : array_like     An array with shape (n_eval, n_features) giving the locations x at     which the regression model should be evaluated.  Returns ------- f : array_like     An array with shape (n_eval, p)..."),
d("linear(x)", "First order polynomial (linear, p  n+1) regression model.  x --> f(x)  [ 1, x_1, ..., x_n ].T  Parameters ---------- x : array_like     An array with shape (n_eval, n_features) giving the locations x at     which the regression model should be evaluated.  Returns ------- f : array_like     An array ..."),
d("quadratic(x)", "Second order polynomial (quadratic, p  n*(n-1)/2+n+1) regression model.  x --> f(x)  [ 1, { x_i, i  1,...,n }, { x_i * x_j,  (i,j)  1,...,n } ].T                                                       i > j  Parameters ---------- x : array_like     An array with shape (n_eval, n_features) giving the ..."),]),]),
c("linear_model", "/linear_model<br>", [
c("tests", "/linear_model/tests<br>", [
c("test_base.py", "/linear_model/tests/test_base.py<br>", [
d("test_center_data()"),
d("test_center_data_multioutput()"),
d("test_center_data_weighted()"),
d("test_csr_sparse_center_data()"),
d("test_fit_intercept()"),
d("test_linear_regression()"),
d("test_linear_regression_multiple_outcome(random_state)", "Test multiple-outcome linear regressions"),
d("test_linear_regression_sparse(random_state)", "Test that linear regression also works with sparse data"),
d("test_linear_regression_sparse_multiple_outcome(random_state)", "Test multiple-outcome linear regressions with sparse data"),
d("test_sparse_center_data()"),]),
c("test_bayes.py", "/linear_model/tests/test_bayes.py<br>", [
d("test_bayesian_on_diabetes()"),
d("test_toy_ard_object()"),
d("test_toy_bayesian_ridge_object()"),]),
c("test_coordinate_descent.py", "/linear_model/tests/test_coordinate_descent.py<br>", [
d("build_dataset(n_samples, n_features, n_informative_features, n_targets)", "build an ill-posed linear regression problem with many noisy features and comparatively few samples"),
d("check_warnings()"),
d("test_1d_multioutput_enet_and_multitask_enet_cv()"),
d("test_1d_multioutput_lasso_and_multitask_lasso_cv()"),
d("test_check_input_false()"),
d("test_deprection_precompute_enet()"),
d("test_enet_cv_positive_constraint()"),
d("test_enet_multitarget()"),
d("test_enet_path()"),
d("test_enet_path_positive()"),
d("test_enet_positive_constraint()"),
d("test_enet_toy()"),
d("test_lasso_alpha_warning()"),
d("test_lasso_cv()"),
d("test_lasso_cv_positive_constraint()"),
d("test_lasso_path_return_models_vs_new_return_gives_same_coefficients()"),
d("test_lasso_positive_constraint()"),
d("test_lasso_readonly_data()"),
d("test_lasso_toy()"),
d("test_lasso_zero()"),
d("test_multi_task_lasso_and_enet()"),
d("test_multi_task_lasso_readonly_data()"),
d("test_multioutput_enetcv_error()"),
d("test_multitask_enet_and_lasso_cv()"),
d("test_overrided_gram_matrix()"),
d("test_path_parameters()"),
d("test_precompute_invalid_argument()"),
d("test_random_descent()"),
d("test_sparse_dense_descent_paths()"),
d("test_sparse_input_dtype_enet_and_lassocv()"),
d("test_uniform_targets()"),
d("test_warm_start()"),
d("test_warm_start_convergence()"),
d("test_warm_start_convergence_with_regularizer_decrement()"),]),
c("test_least_angle.py", "/linear_model/tests/test_least_angle.py<br>", [
d("test_all_precomputed()"),
d("test_collinearity()"),
d("test_lars_add_features()"),
d("test_lars_cv()"),
d("test_lars_lstsq()"),
d("test_lars_n_nonzero_coefs(verbose)"),
d("test_lars_path_readonly_data()"),
d("test_lasso_gives_lstsq_solution()"),
d("test_lasso_lars_ic()"),
d("test_lasso_lars_path_length()"),
d("test_lasso_lars_vs_lasso_cd(verbose)"),
d("test_lasso_lars_vs_lasso_cd_early_stopping(verbose)"),
d("test_lasso_lars_vs_lasso_cd_ill_conditioned()"),
d("test_lasso_lars_vs_lasso_cd_ill_conditioned2()"),
d("test_multitarget()"),
d("test_no_path()"),
d("test_no_path_all_precomputed()"),
d("test_no_path_precomputed()"),
d("test_no_warning_for_zero_mse()"),
d("test_rank_deficient_design()"),
d("test_simple()"),
d("test_simple_precomputed()"),
d("test_singular_matrix()"),]),
c("test_logistic.py", "/linear_model/tests/test_logistic.py<br>", [
d("check_predictions(clf, X, y)", "Check that the model is able to fit the classification data"),
d("test_check_solver_option()"),
d("test_consistency_path()"),
d("test_error()"),
d("test_inconsistent_input()"),
d("test_intercept_logistic_helper()"),
d("test_liblinear_decision_function_zero()"),
d("test_liblinear_dual_random_state()"),
d("test_liblinear_logregcv_sparse()"),
d("test_logistic_cv()"),
d("test_logistic_cv_sparse()"),
d("test_logistic_grad_hess()"),
d("test_logistic_loss_and_grad()"),
d("test_logistic_regression_convergence_warnings()"),
d("test_logistic_regression_multinomial()"),
d("test_logistic_regression_solvers()"),
d("test_logistic_regression_solvers_multiclass()"),
d("test_logistic_regressioncv_class_weights()"),
d("test_logreg_cv_penalty()"),
d("test_logreg_intercept_scaling()"),
d("test_logreg_intercept_scaling_zero()"),
d("test_multinomial_binary()"),
d("test_multinomial_grad_hess()"),
d("test_multinomial_validation()"),
d("test_nan()"),
d("test_ovr_multinomial_iris()"),
d("test_predict_2_classes()"),
d("test_predict_3_classes()"),
d("test_predict_iris()"),
d("test_sparsify()"),
d("test_write_parameters()"),]),
c("test_omp.py", "/linear_model/tests/test_omp.py<br>", [
d("test_bad_input()"),
d("test_correct_shapes()"),
d("test_correct_shapes_gram()"),
d("test_estimator()"),
d("test_identical_regressors()"),
d("test_n_nonzero_coefs()"),
d("test_no_atoms()"),
d("test_omp_cv()"),
d("test_omp_path()"),
d("test_omp_reaches_least_squares()"),
d("test_omp_return_path_prop_with_gram()"),
d("test_perfect_signal_recovery()"),
d("test_swapped_regressors()"),
d("test_tol()"),
d("test_unreachable_accuracy()"),
d("test_with_without_gram()"),
d("test_with_without_gram_tol()"),]),
c("test_passive_aggressive.py", "/linear_model/tests/test_passive_aggressive.py<br>", [
c("MyPassiveAggressive(ClassifierMixin)", "/linear_model/tests/test_passive_aggressive.py<br>", [
d("__init__(self, C, epsilon, loss, fit_intercept, n_iter, random_state)"),
d("fit(self, X, y)"),
d("project(self, X)"),]),
d("test_classifier_accuracy()"),
d("test_classifier_correctness()"),
d("test_classifier_partial_fit()"),
d("test_classifier_refit()"),
d("test_classifier_undefined_methods()"),
d("test_regressor_correctness()"),
d("test_regressor_mse()"),
d("test_regressor_partial_fit()"),
d("test_regressor_undefined_methods()"),]),
c("test_perceptron.py", "/linear_model/tests/test_perceptron.py<br>", [
c("MyPerceptron(object)", "/linear_model/tests/test_perceptron.py<br>", [
d("__init__(self, n_iter)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("project(self, X)"),]),
d("test_perceptron_accuracy()"),
d("test_perceptron_correctness()"),
d("test_undefined_methods()"),]),
c("test_randomized_l1.py", "/linear_model/tests/test_randomized_l1.py<br>", [
d("test_lasso_stability_path()"),
d("test_randomized_lasso()"),
d("test_randomized_logistic()"),
d("test_randomized_logistic_sparse()"),]),
c("test_ransac.py", "/linear_model/tests/test_ransac.py<br>", [
d("test_ransac_default_residual_threshold()"),
d("test_ransac_dynamic_max_trials()"),
d("test_ransac_inliers_outliers()"),
d("test_ransac_is_data_valid()"),
d("test_ransac_is_model_valid()"),
d("test_ransac_max_trials()"),
d("test_ransac_min_n_samples()"),
d("test_ransac_multi_dimensional_targets()"),
d("test_ransac_none_estimator()"),
d("test_ransac_predict()"),
d("test_ransac_resid_thresh_no_inliers()"),
d("test_ransac_residual_metric()"),
d("test_ransac_score()"),
d("test_ransac_sparse_coo()"),
d("test_ransac_sparse_csc()"),
d("test_ransac_sparse_csr()"),
d("test_ransac_stop_n_inliers()"),
d("test_ransac_stop_score()"),]),
c("test_ridge.py", "/linear_model/tests/test_ridge.py<br>", [
d("_test_multi_ridge_diabetes(filter_)"),
d("_test_ridge_classifiers(filter_)"),
d("_test_ridge_cv(filter_)"),
d("_test_ridge_diabetes(filter_)"),
d("_test_ridge_loo(filter_)"),
d("_test_tolerance(filter_)"),
d("test_class_weight_vs_sample_weight()", "Check class_weights resemble sample_weights behavior."),
d("test_class_weights()"),
d("test_class_weights_cv()"),
d("test_dense_sparse()"),
d("test_primal_dual_relationship()"),
d("test_raises_value_error_if_sample_weights_greater_than_1d()"),
d("test_raises_value_error_if_solver_not_supported()"),
d("test_ridge()"),
d("test_ridge_cv_sparse_svd()"),
d("test_ridge_individual_penalties()"),
d("test_ridge_intercept()"),
d("test_ridge_sample_weights()"),
d("test_ridge_shapes()"),
d("test_ridge_singular()"),
d("test_ridge_sparse_svd()"),
d("test_ridge_vs_lstsq()"),
d("test_ridgecv_sample_weight()"),
d("test_ridgecv_store_cv_values()"),
d("test_sparse_cg_max_iter()"),
d("test_sparse_design_with_sample_weights()"),
d("test_toy_ridge_object()"),]),
c("test_sgd.py", "/linear_model/tests/test_sgd.py<br>", [
c("CommonTest(object)", "/linear_model/tests/test_sgd.py<br>", [
d("_test_warm_start(self, X, Y, lr)"),
d("asgd(self, X, y, eta, alpha, weight_init, intercept_init)"),
d("factory(self)"),
d("test_clone(self)"),
d("test_input_format(self)"),
d("test_late_onset_averaging_not_reached(self)"),
d("test_late_onset_averaging_reached(self)"),
d("test_plain_has_no_average_attr(self)"),
d("test_warm_start_constant(self)"),
d("test_warm_start_invscaling(self)"),
d("test_warm_start_optimal(self)"),]),
c("DenseSGDClassifierTestCase(CommonTest)", "/linear_model/tests/test_sgd.py<br>Test suite for the dense representation variant of SGD", [
d("_test_partial_fit_equal_fit(self, lr)"),
d("test_argument_coef(self)"),
d("test_average_binary_computed_correctly(self)"),
d("test_balanced_weight(self)"),
d("test_class_weights(self)"),
d("test_equal_class_weight(self)"),
d("test_fit_then_partial_fit(self)"),
d("test_multiple_fit(self)"),
d("test_partial_fit_binary(self)"),
d("test_partial_fit_equal_fit_constant(self)"),
d("test_partial_fit_equal_fit_invscaling(self)"),
d("test_partial_fit_equal_fit_optimal(self)"),
d("test_partial_fit_exception(self)"),
d("test_partial_fit_multiclass(self)"),
d("test_partial_fit_weight_class_balanced(self)"),
d("test_provide_coef(self)"),
d("test_regression_losses(self)"),
d("test_sample_weights(self)"),
d("test_set_coef_multiclass(self)"),
d("test_set_intercept(self)"),
d("test_set_intercept_binary(self)"),
d("test_set_intercept_to_intercept(self)"),
d("test_sgd(self)"),
d("test_sgd_at_least_two_labels(self)"),
d("test_sgd_bad_alpha(self)"),
d("test_sgd_bad_eta0(self)"),
d("test_sgd_bad_l1_ratio(self)"),
d("test_sgd_bad_learning_rate_schedule(self)"),
d("test_sgd_bad_loss(self)"),
d("test_sgd_bad_penalty(self)"),
d("test_sgd_l1(self)"),
d("test_sgd_multiclass(self)"),
d("test_sgd_multiclass_average(self)"),
d("test_sgd_multiclass_njobs(self)"),
d("test_sgd_multiclass_with_init_coef(self)"),
d("test_sgd_n_iter_param(self)"),
d("test_sgd_proba(self)"),
d("test_sgd_shuffle_param(self)"),
d("test_warm_start_multiclass(self)"),
d("test_weights_multiplied(self)"),
d("test_wrong_class_weight_format(self)"),
d("test_wrong_class_weight_label(self)"),
d("test_wrong_sample_weights(self)"),]),
c("DenseSGDRegressorTestCase(CommonTest)", "/linear_model/tests/test_sgd.py<br>Test suite for the dense representation variant of SGD", [
d("_test_partial_fit_equal_fit(self, lr)"),
d("test_average_sparse(self)"),
d("test_elasticnet_convergence(self)"),
d("test_loss_function_epsilon(self)"),
d("test_partial_fit(self)"),
d("test_partial_fit_equal_fit_constant(self)"),
d("test_partial_fit_equal_fit_invscaling(self)"),
d("test_partial_fit_equal_fit_optimal(self)"),
d("test_sgd(self)"),
d("test_sgd_averaged_computed_correctly(self)"),
d("test_sgd_averaged_partial_fit(self)"),
d("test_sgd_bad_loss(self)"),
d("test_sgd_bad_penalty(self)"),
d("test_sgd_epsilon_insensitive(self)"),
d("test_sgd_huber_fit(self)"),
d("test_sgd_least_squares_fit(self)"),]),
c("SparseSGDClassifier(SGDClassifier)", "/linear_model/tests/test_sgd.py<br>", [
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("predict_proba(self, X)"),]),
c("SparseSGDClassifierTestCase(DenseSGDClassifierTestCase)", "/linear_model/tests/test_sgd.py<br>Run exactly the same tests using the sparse representation variant", [
]),
c("SparseSGDRegressor(SGDRegressor)", "/linear_model/tests/test_sgd.py<br>", [
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),]),
c("SparseSGDRegressorTestCase(DenseSGDRegressorTestCase)", "/linear_model/tests/test_sgd.py<br>", [
]),
d("test_l1_ratio()"),
d("test_large_regularization()"),
d("test_numerical_stability_large_gradient()"),
d("test_underflow_or_overlow()"),]),
c("test_sparse_coordinate_descent.py", "/linear_model/tests/test_sparse_coordinate_descent.py<br>", [
d("_test_sparse_enet_not_as_toy_dataset(alpha, fit_intercept, positive)"),
d("make_sparse_data(n_samples, n_features, n_informative, seed, positive, n_targets)"),
d("test_enet_multitarget()"),
d("test_enet_toy_explicit_sparse_input()"),
d("test_enet_toy_list_input()"),
d("test_lasso_zero()"),
d("test_normalize_option()"),
d("test_path_parameters()"),
d("test_same_output_sparse_dense_lasso_and_enet_cv()"),
d("test_sparse_coef()"),
d("test_sparse_enet_not_as_toy_dataset()"),
d("test_sparse_lasso_not_as_toy_dataset()"),]),
c("test_theil_sen.py", "/linear_model/tests/test_theil_sen.py<br> Testing for Theil-Sen module (sklearn.linear_model.theil_sen)", [
d("gen_toy_problem_1d(intercept)"),
d("gen_toy_problem_2d()"),
d("gen_toy_problem_4d()"),
d("no_stdout_stderr()"),
d("test_calc_breakdown_point()"),
d("test_checksubparams_n_subsamples_if_less_samples_than_features()"),
d("test_checksubparams_negative_subpopulation()"),
d("test_checksubparams_too_few_subsamples()"),
d("test_checksubparams_too_many_subsamples()"),
d("test_less_samples_than_features()"),
d("test_modweiszfeld_step_1d()"),
d("test_modweiszfeld_step_2d()"),
d("test_spatial_median_1d()"),
d("test_spatial_median_2d()"),
d("test_subpopulation()"),
d("test_subsamples()"),
d("test_theil_sen_1d()"),
d("test_theil_sen_1d_no_intercept()"),
d("test_theil_sen_2d()"),
d("test_theil_sen_parallel()"),
d("test_verbosity()"),]),]),
c("base.py", "/linear_model/base.py<br> Generalized Linear models.", [
c("LinearClassifierMixin(ClassifierMixin)", "/linear_model/base.py<br>Mixin for linear classifiers.  Handles prediction for sparse and dense X.", [
d("_predict_proba_lr(self, X)"),
d("decision_function(self, X)"),
d("predict(self, X)"),]),
c("LinearModel()", "/linear_model/base.py<br>Base class for Linear Models", [
d("_decision_function(self, X)"),
d("_set_intercept(self, X_mean, y_mean, X_std)"),
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("LinearRegression(LinearModel, RegressorMixin)", "/linear_model/base.py<br>Ordinary least squares Linear Regression.  Parameters ---------- fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional,...", [
d("__init__(self, fit_intercept, normalize, copy_X, n_jobs)"),
d("fit(self, X, y)"),]),
c("SparseCoefMixin(object)", "/linear_model/base.py<br>Mixin for converting coef_ to and from CSR format.  L1-regularizing estimators should inherit this.", [
d("densify(self)"),
d("sparsify(self)"),]),
d("_pre_fit(X, y, Xy, precompute, normalize, fit_intercept, copy, Xy_precompute_order)", "Aux function used at beginning of fit in linear models"),
d("center_data(X, y, fit_intercept, normalize, copy, sample_weight)", "Centers data to have mean zero along axis 0. This is here because nearly all linear models will want their data to be centered.  If sample_weight is not None, then the weighted mean of X and y is zero, and not the mean itself"),
d("sparse_center_data(X, y, fit_intercept, normalize)", "Compute information needed to center data to have mean zero along axis 0. Be aware that X will not be centered since it would break the sparsity, but will be normalized if asked so."),]),
c("bayes.py", "/linear_model/bayes.py<br> Various bayesian regression", [
c("ARDRegression(LinearModel, RegressorMixin)", "/linear_model/bayes.py<br>Bayesian ARD regression.  Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the n...", [
d("__init__(self, n_iter, tol, alpha_1, alpha_2, lambda_1, lambda_2, compute_score, threshold_lambda, fit_intercept, normalize, copy_X, verbose)"),
d("fit(self, X, y)"),]),
c("BayesianRidge(LinearModel, RegressorMixin)", "/linear_model/bayes.py<br>Bayesian ridge regression  Fit a Bayesian ridge model and optimize the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`.  Parameters ---------- n_iter : int, optional     Maximum number of iterat...", [
d("__init__(self, n_iter, tol, alpha_1, alpha_2, lambda_1, lambda_2, compute_score, fit_intercept, normalize, copy_X, verbose)"),
d("fit(self, X, y)"),]),]),
c("coordinate_descent.py", "/linear_model/coordinate_descent.py<br>", [
c("ElasticNet(LinearModel, RegressorMixin)", "/linear_model/coordinate_descent.py<br>Linear regression with combined L1 and L2 priors as regularizer.  Minimizes the objective function::          1 / (2 * n_samples) * ||y - Xw||^2_2 +         + alpha * l1_ratio * ||w||_1         + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty sep...", [
d("__init__(self, alpha, l1_ratio, fit_intercept, normalize, precompute, max_iter, copy_X, tol, warm_start, positive, random_state, selection)"),
d("_decision_function(self, X)"),
d("decision_function(self, X)"),
d("fit(self, X, y, check_input)"),
d("sparse_coef_(self)"),]),
c("ElasticNetCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py<br>Elastic Net model with iterative fitting along a regularization path  The best model is selected by cross-validation.  Read more in the :ref:`User Guide <elastic_net>`.  Parameters ---------- l1_ratio : float, optional     float between 0 and 1 passed to ElasticNet (scaling between     l1 and l2 pen...", [
d("__init__(self, l1_ratio, eps, n_alphas, alphas, fit_intercept, normalize, precompute, max_iter, tol, cv, copy_X, verbose, n_jobs, positive, random_state, selection)"),]),
c("Lasso(ElasticNet)", "/linear_model/coordinate_descent.py<br>Linear Model trained with L1 prior as regularizer (aka the Lasso)  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Technically the Lasso model is optimizing the same objective function as the Elastic Net with ``l1_ratio1.0`` (no L2 penalty).  ...", [
d("__init__(self, alpha, fit_intercept, normalize, precompute, copy_X, max_iter, tol, warm_start, positive, random_state, selection)"),]),
c("LassoCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py<br>Lasso linear model with iterative fitting along a regularization path  The best model is selected by cross-validation.  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <lasso>`.  Parameters ---------- eps : fl...", [
d("__init__(self, eps, n_alphas, alphas, fit_intercept, normalize, precompute, max_iter, tol, copy_X, cv, verbose, n_jobs, positive, random_state, selection)"),]),
c("LinearModelCV()", "/linear_model/coordinate_descent.py<br>Base class for iterative model fitting along a regularization path", [
d("__init__(self, eps, n_alphas, alphas, fit_intercept, normalize, precompute, max_iter, tol, copy_X, cv, verbose, n_jobs, positive, random_state, selection)"),
d("fit(self, X, y)"),]),
c("MultiTaskElasticNet(Lasso)", "/linear_model/coordinate_descent.py<br>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21  \sum_i \sqrt{\sum...", [
d("__init__(self, alpha, l1_ratio, fit_intercept, normalize, copy_X, max_iter, tol, warm_start, random_state, selection)"),
d("fit(self, X, y)"),]),
c("MultiTaskElasticNetCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py<br>Multi-task L1/L2 ElasticNet with built-in cross-validation.  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21  \sum_i \sqrt{\sum_j w_{ij}^2} ...", [
d("__init__(self, l1_ratio, eps, n_alphas, alphas, fit_intercept, normalize, max_iter, tol, cv, copy_X, verbose, n_jobs, random_state, selection)"),]),
c("MultiTaskLasso(MultiTaskElasticNet)", "/linear_model/coordinate_descent.py<br>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21  Where::      ||W||_21  \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of earch row.  Read more in the :ref:`User Guide...", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, warm_start, random_state, selection)"),]),
c("MultiTaskLassoCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py<br>Multi-task L1/L2 Lasso with built-in cross-validation.  The optimization objective for MultiTaskLasso is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21  Where::      ||W||_21  \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <mul...", [
d("__init__(self, eps, n_alphas, alphas, fit_intercept, normalize, max_iter, tol, copy_X, cv, verbose, n_jobs, random_state, selection)"),]),
d("_alpha_grid(X, y, Xy, l1_ratio, fit_intercept, eps, n_alphas, normalize, copy_X)", "Compute the grid of alpha values for elastic net parameter search  Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data. Pass directly as Fortran-contiguous data to avoid     unnecessary memory duplication  y : ndarray, shape (n_samples,)     Target ..."),
d("_path_residuals(X, y, train, test, path, path_params, alphas, l1_ratio, X_order, dtype)", "Returns the MSE for the models computed by 'path'  Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  y : array-like, shape (n_samples,) or (n_samples, n_targets)     Target values  train : list of indices     The indices of the train set  test :..."),
d("enet_path(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive)", "Compute elastic net path with coordinate descent  The elastic net optimization function varies for mono and multi-outputs.  For mono-output tasks it is::      1 / (2 * n_samples) * ||y - Xw||^2_2 +     + alpha * l1_ratio * ||w||_1     + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  For multi-output task..."),
d("lasso_path(X, y, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive)", "Compute Lasso path with coordinate descent  The Lasso optimization function varies for mono and multi-outputs.  For mono-output tasks it is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  For multi-output tasks it is::      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21..."),]),
c("least_angle.py", "/linear_model/least_angle.py<br> Least Angle Regression algorithm. See the documentation on the Generalized Linear Model for a complete discussion.", [
c("Lars(LinearModel, RegressorMixin)", "/linear_model/least_angle.py<br>Least Angle Regression model a.k.a. LAR  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- n_nonzero_coefs : int, optional     Target number of non-zero coefficients. Use ``np.inf`` for no limit.  fit_intercept : boolean     Whether to calculate the intercept for th...", [
d("__init__(self, fit_intercept, verbose, normalize, precompute, n_nonzero_coefs, eps, copy_X, fit_path)"),
d("_get_gram(self)"),
d("fit(self, X, y, Xy)"),]),
c("LarsCV(Lars)", "/linear_model/least_angle.py<br>Cross-validated Least Angle Regression model  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected t...", [
d("__init__(self, fit_intercept, verbose, max_iter, normalize, precompute, cv, max_n_alphas, n_jobs, eps, copy_X)"),
d("alpha(self)"),
d("fit(self, X, y)"),]),
c("LassoLars(Lars)", "/linear_model/least_angle.py<br>Lasso model fit with Least Angle Regression a.k.a. Lars  It is a Linear Model trained with an L1 prior as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ------...", [
d("__init__(self, alpha, fit_intercept, verbose, normalize, precompute, max_iter, eps, copy_X, fit_path)"),]),
c("LassoLarsCV(LarsCV)", "/linear_model/least_angle.py<br>Cross-validated Lasso, using the LARS algorithm  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : boolean     whether to calculate the intercept for...", [
]),
c("LassoLarsIC(LassoLars)", "/linear_model/least_angle.py<br>Lasso model fit with Lars using BIC or AIC for model selection  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the...", [
d("__init__(self, criterion, fit_intercept, verbose, normalize, precompute, max_iter, eps, copy_X)"),
d("fit(self, X, y, copy_X)"),]),
d("_check_copy_and_writeable(array, copy)"),
d("_lars_path_residues(X_train, y_train, X_test, y_test, Gram, copy, method, verbose, fit_intercept, normalize, max_iter, eps)", "Compute the residues on left-out data for a full LARS path  Parameters ----------- X_train : array, shape (n_samples, n_features)     The data to fit the LARS on  y_train : array, shape (n_samples)     The target variable to fit LARS on  X_test : array, shape (n_samples, n_features)     The data to ..."),
d("lars_path(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter)", "Compute Least Angle Regression or Lasso path using LARS algorithm [1]  The optimization objective for the case method'lasso' is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  in the case of method'lars', the objective function is only known in the form of an implicit equation (see disc..."),]),
c("logistic.py", "/linear_model/logistic.py<br> Logistic Regression", [
c("LogisticRegression(BaseEstimator, LinearClassifierMixin, _LearntSelectorMixin, SparseCoefMixin)", "/linear_model/logistic.py<br>Logistic Regression (aka logit, MaxEnt) classifier.  In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr' and uses the cross-entropy loss, if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option ...", [
d("__init__(self, penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state, solver, max_iter, multi_class, verbose)"),
d("fit(self, X, y)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),
c("LogisticRegressionCV(LogisticRegression, BaseEstimator, LinearClassifierMixin, _LearntSelectorMixin)", "/linear_model/logistic.py<br>Logistic Regression CV (aka logit, MaxEnt) classifier.  This class implements logistic regression using liblinear, newton-cg or LBFGS optimizer. The newton-cg and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a...", [
d("__init__(self, Cs, fit_intercept, cv, dual, penalty, scoring, solver, tol, max_iter, class_weight, n_jobs, verbose, refit, intercept_scaling, multi_class)"),
d("fit(self, X, y)"),]),
d("_check_solver_option(solver, multi_class, penalty, dual)"),
d("_intercept_dot(w, X, y)", "Computes y * np.dot(X, w).  It takes into consideration if the intercept should be fit or not.  Parameters ---------- w : ndarray, shape (n_features,) or (n_features + 1,)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  y : ndarray, shape (..."),
d("_log_reg_scoring_path(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, copy, intercept_scaling, multi_class)", "Computes scores across logistic_regression_path  Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  y : array-like, shape (n_samples,) or (n_samples, n_targets)     Target labels.  train : list of indices     The indices of the train set.  test :..."),
d("_logistic_grad_hess(w, X, y, alpha, sample_weight)", "Computes the gradient and the Hessian, in the case of a logistic loss.  Parameters ---------- w : ndarray, shape (n_features,) or (n_features + 1,)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  y : ndarray, shape (n_samples,)     Array of..."),
d("_logistic_loss(w, X, y, alpha, sample_weight)", "Computes the logistic loss.  Parameters ---------- w : ndarray, shape (n_features,) or (n_features + 1,)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  y : ndarray, shape (n_samples,)     Array of labels.  alpha : float     Regularization ..."),
d("_logistic_loss_and_grad(w, X, y, alpha, sample_weight)", "Computes the logistic loss and gradient.  Parameters ---------- w : ndarray, shape (n_features,) or (n_features + 1,)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  y : ndarray, shape (n_samples,)     Array of labels.  alpha : float     Re..."),
d("_multinomial_grad_hess(w, X, Y, alpha, sample_weight)", "Computes the gradient and the Hessian, in the case of a multinomial loss.  Parameters ---------- w : ndarray, shape (n_classes * n_features,) or (n_classes * (n_features + 1),)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  Y : ndarray, sh..."),
d("_multinomial_loss(w, X, Y, alpha, sample_weight)", "Computes multinomial loss and class probabilities.  Parameters ---------- w : ndarray, shape (n_classes * n_features,) or (n_classes * (n_features + 1),)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  Y : ndarray, shape (n_samples, n_class..."),
d("_multinomial_loss_grad(w, X, Y, alpha, sample_weight)", "Computes the multinomial loss, gradient and class probabilities.  Parameters ---------- w : ndarray, shape (n_classes * n_features,) or (n_classes * (n_features + 1),)     Coefficient vector.  X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training data.  Y : ndarray, shape (n_sa..."),
d("logistic_regression_path(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state)", "Compute a Logistic Regression model for a list of regularization parameters.  This is an implementation that uses the result of the previous model to speed up computations along the set of solutions, making it faster than sequentially calling LogisticRegression for the different parameters.  Read mo..."),]),
c("omp.py", "/linear_model/omp.py<br> Orthogonal matching pursuit algorithms", [
c("OrthogonalMatchingPursuit(LinearModel, RegressorMixin)", "/linear_model/omp.py<br>Orthogonal Matching Pursuit model (OMP)  Parameters ---------- n_nonzero_coefs : int, optional     Desired number of non-zero entries in the solution. If None (by     default) this value is set to 10% of n_features.  tol : float, optional     Maximum norm of the residual. If not None, overrides n_no...", [
d("__init__(self, n_nonzero_coefs, tol, fit_intercept, normalize, precompute)"),
d("fit(self, X, y)"),]),
c("OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin)", "/linear_model/omp.py<br>Cross-validated Orthogonal Matching Pursuit model (OMP)  Parameters ---------- copy : bool, optional     Whether the design matrix X must be copied by the algorithm. A false     value is only helpful if X is already Fortran-ordered, otherwise a     copy is made anyway.  fit_intercept : boolean, opti...", [
d("__init__(self, copy, fit_intercept, normalize, max_iter, cv, n_jobs, verbose)"),
d("fit(self, X, y)"),]),
d("_cholesky_omp(X, y, n_nonzero_coefs, tol, copy_X, return_path)", "Orthogonal Matching Pursuit step using the Cholesky decomposition.  Parameters ---------- X : array, shape (n_samples, n_features)     Input dictionary. Columns are assumed to have unit norm.  y : array, shape (n_samples,)     Input targets  n_nonzero_coefs : int     Targeted number of non-zero elem..."),
d("_gram_omp(Gram, Xy, n_nonzero_coefs, tol_0, tol, copy_Gram, copy_Xy, return_path)", "Orthogonal Matching Pursuit step on a precomputed Gram matrix.  This function uses the the Cholesky decomposition method.  Parameters ---------- Gram : array, shape (n_features, n_features)     Gram matrix of the input data matrix  Xy : array, shape (n_features,)     Input targets  n_nonzero_coefs :..."),
d("_omp_path_residues(X_train, y_train, X_test, y_test, copy, fit_intercept, normalize, max_iter)", "Compute the residues on left-out data for a full LARS path  Parameters ----------- X_train : array, shape (n_samples, n_features)     The data to fit the LARS on  y_train : array, shape (n_samples)     The target variable to fit LARS on  X_test : array, shape (n_samples, n_features)     The data to ..."),
d("orthogonal_mp(X, y, n_nonzero_coefs, tol, precompute, copy_X, return_path, return_n_iter)", "Orthogonal Matching Pursuit (OMP)  Solves n_targets Orthogonal Matching Pursuit problems. An instance of the problem has the form:  When parametrized by the number of non-zero coefficients using `n_nonzero_coefs`: argmin ||y - X\gamma||^2 subject to ||\gamma||_0 < n_{nonzero coefs}  When parametrize..."),
d("orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs, tol, norms_squared, copy_Gram, copy_Xy, return_path, return_n_iter)", "Gram Orthogonal Matching Pursuit (OMP)  Solves n_targets Orthogonal Matching Pursuit problems using only the Gram matrix X.T * X and the product X.T * y.  Read more in the :ref:`User Guide <omp>`.  Parameters ---------- Gram : array, shape (n_features, n_features)     Gram matrix of the input data: ..."),]),
c("passive_aggressive.py", "/linear_model/passive_aggressive.py<br>", [
c("PassiveAggressiveClassifier(BaseSGDClassifier)", "/linear_model/passive_aggressive.py<br>Passive Aggressive Classifier  Read more in the :ref:`User Guide <passive_aggressive>`.  Parameters ----------  C : float     Maximum step size (regularization). Defaults to 1.0.  fit_intercept : bool, defaultFalse     Whether the intercept should be estimated or not. If False, the     data is assum...", [
d("__init__(self, C, fit_intercept, n_iter, shuffle, verbose, loss, n_jobs, random_state, warm_start)"),
d("fit(self, X, y, coef_init, intercept_init)"),
d("partial_fit(self, X, y, classes)"),]),
c("PassiveAggressiveRegressor(BaseSGDRegressor)", "/linear_model/passive_aggressive.py<br>Passive Aggressive Regressor  Read more in the :ref:`User Guide <passive_aggressive>`.  Parameters ----------  C : float     Maximum step size (regularization). Defaults to 1.0.  epsilon : float     If the difference between the current prediction and the correct label     is below this threshold, t...", [
d("__init__(self, C, fit_intercept, n_iter, shuffle, verbose, loss, epsilon, random_state, class_weight, warm_start)"),
d("fit(self, X, y, coef_init, intercept_init)"),
d("partial_fit(self, X, y)"),]),]),
c("perceptron.py", "/linear_model/perceptron.py<br>", [
c("Perceptron(BaseSGDClassifier, _LearntSelectorMixin)", "/linear_model/perceptron.py<br>Perceptron  Read more in the :ref:`User Guide <perceptron>`.  Parameters ----------  penalty : None, 'l2' or 'l1' or 'elasticnet'     The penalty (aka regularization term) to be used. Defaults to None.  alpha : float     Constant that multiplies the regularization term if regularization is     used....", [
d("__init__(self, penalty, alpha, fit_intercept, n_iter, shuffle, verbose, eta0, n_jobs, random_state, class_weight, warm_start)"),]),]),
c("randomized_l1.py", "/linear_model/randomized_l1.py<br> Randomized Lasso/Logistic: feature selection based on Lasso and sparse Logistic Regression", [
c("BaseRandomizedLinearModel()", "/linear_model/randomized_l1.py<br>Base class to implement randomized linear models for feature selection  This implements the strategy by Meinshausen and Buhlman: stability selection with randomized sampling, and random re-weighting of the penalty.", [
d("__init__(self)"),
d("_make_estimator_and_params(self, X, y)"),
d("fit(self, X, y)"),
d("get_support(self, indices)"),
d("inverse_transform(self, X)"),
d("transform(self, X)"),]),
c("RandomizedLasso(BaseRandomizedLinearModel)", "/linear_model/randomized_l1.py<br>Randomized Lasso.  Randomized Lasso works by resampling the train data and computing a Lasso on each resampling. In short, the features selected more often are good features. It is also known as stability selection.  Read more in the :ref:`User Guide <randomized_l1>`.  Parameters ---------- alpha : ...", [
d("__init__(self, alpha, scaling, sample_fraction, n_resampling, selection_threshold, fit_intercept, verbose, normalize, precompute, max_iter, eps, random_state, n_jobs, pre_dispatch, memory)"),
d("_make_estimator_and_params(self, X, y)"),]),
c("RandomizedLogisticRegression(BaseRandomizedLinearModel)", "/linear_model/randomized_l1.py<br>Randomized Logistic Regression  Randomized Regression works by resampling the train data and computing a LogisticRegression on each resampling. In short, the features selected more often are good features. It is also known as stability selection.  Read more in the :ref:`User Guide <randomized_l1>`. ...", [
d("__init__(self, C, scaling, sample_fraction, n_resampling, selection_threshold, tol, fit_intercept, verbose, normalize, random_state, n_jobs, pre_dispatch, memory)"),
d("_center_data(self, X, y, fit_intercept, normalize)"),
d("_make_estimator_and_params(self, X, y)"),]),
d("_lasso_stability_path(X, y, mask, weights, eps)", "Inner loop of lasso_stability_path"),
d("_randomized_lasso(X, y, weights, mask, alpha, verbose, precompute, eps, max_iter)"),
d("_randomized_logistic(X, y, weights, mask, C, verbose, fit_intercept, tol)"),
d("_resample_model(estimator_func, X, y, scaling, n_resampling, n_jobs, verbose, pre_dispatch, random_state, sample_fraction)"),
d("lasso_stability_path(X, y, scaling, random_state, n_resampling, n_grid, sample_fraction, eps, n_jobs, verbose)", "Stabiliy path based on randomized Lasso estimates  Read more in the :ref:`User Guide <randomized_l1>`.  Parameters ---------- X : array-like, shape  [n_samples, n_features]     training data.  y : array-like, shape  [n_samples]     target values.  scaling : float, optional, default0.5     The alpha ..."),]),
c("ransac.py", "/linear_model/ransac.py<br>", [
c("RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin)", "/linear_model/ransac.py<br>RANSAC (RANdom SAmple Consensus) algorithm.  RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. More information can be found in the general documentation of linear models.  A detailed description of the algorithm can be foun...", [
d("__init__(self, base_estimator, min_samples, residual_threshold, is_data_valid, is_model_valid, max_trials, stop_n_inliers, stop_score, stop_probability, residual_metric, random_state)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("score(self, X, y)"),]),
d("_dynamic_max_trials(n_inliers, n_samples, min_samples, probability)", "Determine number trials such that at least one outlier-free subset is sampled for the given inlier/outlier ratio.  Parameters ---------- n_inliers : int     Number of inliers in the data.  n_samples : int     Total number of samples in the data.  min_samples : int     Minimum number of samples chose..."),]),
c("ridge.py", "/linear_model/ridge.py<br> Ridge regression", [
c("Ridge(_BaseRidge, RegressorMixin)", "/linear_model/ridge.py<br>Linear least squares with l2 regularization.  This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate...", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, solver)"),
d("fit(self, X, y, sample_weight)"),]),
c("RidgeCV(_BaseRidgeCV, RegressorMixin)", "/linear_model/ridge.py<br>Ridge regression with built-in cross-validation.  By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alphas : numpy array of shape [n_alphas]     Array of al...", [
]),
c("RidgeClassifier(LinearClassifierMixin, _BaseRidge)", "/linear_model/ridge.py<br>Classifier using Ridge regression.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alpha : float     Small positive values of alpha improve the conditioning of the problem     and reduce the variance of the estimates.  Alpha corresponds to     ``(2*C)^-1`` in other lin...", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, class_weight, solver)"),
d("classes_(self)"),
d("fit(self, X, y, sample_weight)"),]),
c("RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV)", "/linear_model/ridge.py<br>Ridge classifier with built-in cross-validation.  By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n_features > n_samples case is handled efficiently.  Read more in the :ref:`User Guide <ridge_regression>`.  Parame...", [
d("__init__(self, alphas, fit_intercept, normalize, scoring, cv, class_weight)"),
d("classes_(self)"),
d("fit(self, X, y, sample_weight)"),]),
c("_BaseRidge()", "/linear_model/ridge.py<br>", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, solver)"),
d("fit(self, X, y, sample_weight)"),]),
c("_BaseRidgeCV(LinearModel)", "/linear_model/ridge.py<br>", [
d("__init__(self, alphas, fit_intercept, normalize, scoring, cv, gcv_mode, store_cv_values)"),
d("fit(self, X, y, sample_weight)"),]),
c("_RidgeGCV(LinearModel)", "/linear_model/ridge.py<br>Ridge regression with built-in Generalized Cross-Validation  It allows efficient Leave-One-Out cross-validation.  This class is not intended to be used directly. Use RidgeCV instead.  Notes -----  We want to solve (K + alpha*Id)c  y, where K  X X^T is the kernel matrix.  Let G  (K + alpha*Id)^-1.  D...", [
d("__init__(self, alphas, fit_intercept, normalize, scoring, copy_X, gcv_mode, store_cv_values)"),
d("_decomp_diag(self, v_prime, Q)"),
d("_diag_dot(self, D, B)"),
d("_errors(self, alpha, y, v, Q, QT_y)"),
d("_errors_svd(self, alpha, y, v, U, UT_y)"),
d("_pre_compute(self, X, y)"),
d("_pre_compute_svd(self, X, y)"),
d("_values(self, alpha, y, v, Q, QT_y)"),
d("_values_svd(self, alpha, y, v, U, UT_y)"),
d("fit(self, X, y, sample_weight)"),]),
d("_rescale_data(X, y, sample_weight)", "Rescale data so as to support sample_weight"),
d("_solve_cholesky(X, y, alpha)"),
d("_solve_cholesky_kernel(K, y, alpha, sample_weight, copy)"),
d("_solve_lsqr(X, y, alpha, max_iter, tol)"),
d("_solve_sparse_cg(X, y, alpha, max_iter, tol, verbose)"),
d("_solve_svd(X, y, alpha)"),
d("ridge_regression(X, y, alpha, sample_weight, solver, max_iter, tol, verbose)", "Solve the ridge equation by the method of normal equations.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- X : {array-like, sparse matrix, LinearOperator},     shape  [n_samples, n_features]     Training data  y : array-like, shape  [n_samples] or [n_samples, n_target..."),]),
c("setup.py", "/linear_model/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("stochastic_gradient.py", "/linear_model/stochastic_gradient.py<br> Classification and regression using Stochastic Gradient Descent (SGD).", [
c("BaseSGD()", "/linear_model/stochastic_gradient.py<br>Base class for SGD classification and regression.", [
d("__init__(self, loss, penalty, alpha, C, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, warm_start, average)"),
d("_allocate_parameter_mem(self, n_classes, n_features, coef_init, intercept_init)"),
d("_get_learning_rate_type(self, learning_rate)"),
d("_get_loss_function(self, loss)"),
d("_get_penalty_type(self, penalty)"),
d("_validate_params(self)"),
d("_validate_sample_weight(self, sample_weight, n_samples)"),
d("fit(self, X, y)"),
d("set_params(self)"),]),
c("BaseSGDClassifier()", "/linear_model/stochastic_gradient.py<br>", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, class_weight, warm_start, average)"),
d("_fit(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)"),
d("_fit_binary(self, X, y, alpha, C, sample_weight, learning_rate, n_iter)"),
d("_fit_multiclass(self, X, y, alpha, C, learning_rate, sample_weight, n_iter)"),
d("_partial_fit(self, X, y, alpha, C, loss, learning_rate, n_iter, classes, sample_weight, coef_init, intercept_init)"),
d("fit(self, X, y, coef_init, intercept_init, sample_weight)"),
d("partial_fit(self, X, y, classes, sample_weight)"),]),
c("BaseSGDRegressor(BaseSGD, RegressorMixin)", "/linear_model/stochastic_gradient.py<br>", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, warm_start, average)"),
d("_decision_function(self, X)"),
d("_fit(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)"),
d("_fit_regressor(self, X, y, alpha, C, loss, learning_rate, sample_weight, n_iter)"),
d("_partial_fit(self, X, y, alpha, C, loss, learning_rate, n_iter, sample_weight, coef_init, intercept_init)"),
d("decision_function(self, X)"),
d("fit(self, X, y, coef_init, intercept_init, sample_weight)"),
d("partial_fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin)", "/linear_model/stochastic_gradient.py<br>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.  This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength s...", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, class_weight, warm_start, average)"),
d("_check_proba(self)"),
d("_predict_log_proba(self, X)"),
d("_predict_proba(self, X)"),
d("predict_log_proba(self)"),
d("predict_proba(self)"),]),
c("SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin)", "/linear_model/stochastic_gradient.py<br>Linear model fitted by minimizing a regularized empirical loss with SGD  SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).  The regularizer is a penal...", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, warm_start, average)"),]),
d("_make_dataset(X, y_i, sample_weight)", "Create ``Dataset`` abstraction for sparse and dense inputs.  This also returns the ``intercept_decay`` which is different for sparse datasets."),
d("_prepare_fit_binary(est, y, i)", "Initialization for fit_binary.  Returns y, coef, intercept."),
d("fit_binary(est, i, X, y, alpha, C, learning_rate, n_iter, pos_weight, neg_weight, sample_weight)", "Fit a single binary classifier.  The i'th class is considered the 'positive' class."),]),
c("theil_sen.py", "/linear_model/theil_sen.py<br> A Theil-Sen Estimator for Multiple Linear Regression Model", [
c("TheilSenRegressor(LinearModel, RegressorMixin)", "/linear_model/theil_sen.py<br>Theil-Sen Estimator: robust multivariate regression model.  The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and effic...", [
d("__init__(self, fit_intercept, copy_X, max_subpopulation, n_subsamples, max_iter, tol, random_state, n_jobs, verbose)"),
d("_check_subparams(self, n_samples, n_features)"),
d("fit(self, X, y)"),]),
d("_breakdown_point(n_samples, n_subsamples)", "Approximation of the breakdown point.  Parameters ---------- n_samples : int     Number of samples.  n_subsamples : int     Number of subsamples to consider.  Returns ------- breakdown_point : float     Approximation of breakdown point."),
d("_lstsq(X, y, indices, fit_intercept)", "Least Squares Estimator for TheilSenRegressor class.  This function calculates the least squares method on a subset of rows of X and y defined by the indices array. Optionally, an intercept column is added if intercept is set to true.  Parameters ---------- X : array, shape  [n_samples, n_features] ..."),
d("_modified_weiszfeld_step(X, x_old)", "Modified Weiszfeld step.  This function defines one iteration step in order to approximate the spatial median (L1 median). It is a form of an iteratively re-weighted least squares method.  Parameters ---------- X : array, shape  [n_samples, n_features]     Training vector, where n_samples is the num..."),
d("_spatial_median(X, max_iter, tol)", "Spatial median (L1 median).  The spatial median is member of a class of so-called M-estimators which are defined by an optimization problem. Given a number of p points in an n-dimensional space, the point x minimizing the sum of all distances to the p other points is called spatial median.  Paramete..."),]),]),
c("manifold", "/manifold<br>", [
c("tests", "/manifold/tests<br>", [
c("test_isomap.py", "/manifold/tests/test_isomap.py<br>", [
d("test_isomap_reconstruction_error()"),
d("test_isomap_simple_grid()"),
d("test_pipeline()"),
d("test_transform()"),]),
c("test_locally_linear.py", "/manifold/tests/test_locally_linear.py<br>", [
d("test_barycenter_kneighbors_graph()"),
d("test_lle_manifold()"),
d("test_lle_simple_grid()"),
d("test_pipeline()"),
d("test_singular_matrix()"),]),
c("test_mds.py", "/manifold/tests/test_mds.py<br>", [
d("test_MDS()"),
d("test_smacof()"),
d("test_smacof_error()"),]),
c("test_spectral_embedding.py", "/manifold/tests/test_spectral_embedding.py<br>", [
d("_check_with_col_sign_flipping(A, B, tol)", "Check array A and B are equal with possible sign flipping on each columns"),
d("test_connectivity(seed)"),
d("test_pipeline_spectral_clustering(seed)"),
d("test_spectral_embedding_amg_solver(seed)"),
d("test_spectral_embedding_callable_affinity(seed)"),
d("test_spectral_embedding_deterministic()"),
d("test_spectral_embedding_precomputed_affinity(seed)"),
d("test_spectral_embedding_two_components(seed)"),
d("test_spectral_embedding_unknown_affinity(seed)"),
d("test_spectral_embedding_unknown_eigensolver(seed)"),]),
c("test_t_sne.py", "/manifold/tests/test_t_sne.py<br>", [
d("test_binary_search()"),
d("test_chebyshev_metric()"),
d("test_distance_not_available()"),
d("test_early_exaggeration_too_small()"),
d("test_fit_csr_matrix()"),
d("test_gradient()"),
d("test_gradient_descent_stops()"),
d("test_init_not_available()"),
d("test_non_square_precomputed_distances()"),
d("test_pca_initialization_not_compatible_with_precomputed_kernel()"),
d("test_preserve_trustworthiness_approximately()"),
d("test_preserve_trustworthiness_approximately_with_precomputed_distances()"),
d("test_reduction_to_one_component()"),
d("test_too_few_iterations()"),
d("test_trustworthiness()"),
d("test_verbose()"),]),]),
c("isomap.py", "/manifold/isomap.py<br> Isomap for manifold learning", [
c("Isomap(BaseEstimator, TransformerMixin)", "/manifold/isomap.py<br>Isomap Embedding  Non-linear dimensionality reduction through Isometric Mapping  Read more in the :ref:`User Guide <isomap>`.  Parameters ---------- n_neighbors : integer     number of neighbors to consider for each point.  n_components : integer     number of coordinates for the manifold  eigen_sol...", [
d("__init__(self, n_neighbors, n_components, eigen_solver, tol, max_iter, path_method, neighbors_algorithm)"),
d("_fit_transform(self, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("reconstruction_error(self)"),
d("transform(self, X)"),]),]),
c("locally_linear.py", "/manifold/locally_linear.py<br> Locally Linear Embedding", [
c("LocallyLinearEmbedding(BaseEstimator, TransformerMixin)", "/manifold/locally_linear.py<br>Locally Linear Embedding  Read more in the :ref:`User Guide <locally_linear_embedding>`.  Parameters ---------- n_neighbors : integer     number of neighbors to consider for each point.  n_components : integer     number of coordinates for the manifold  reg : float     regularization constant, multi...", [
d("__init__(self, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, neighbors_algorithm, random_state)"),
d("_fit_transform(self, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),]),
d("barycenter_kneighbors_graph(X, n_neighbors, reg)", "Computes the barycenter weighted graph of k-Neighbors for points in X  Parameters ---------- X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}     Sample data, shape  (n_samples, n_features), in the form of a     numpy array, sparse array, precomputed tree, or NearestNeighbors     ..."),
d("barycenter_weights(X, Z, reg)", "Compute barycenter weights of X from Y along the first axis  We estimate the weights to assign to each point in Y[i] to recover the point X[i]. The barycenter weights sum to 1.  Parameters ---------- X : array-like, shape (n_samples, n_dim)  Z : array-like, shape (n_samples, n_neighbors, n_dim)  reg..."),
d("locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state)", "Perform a Locally Linear Embedding analysis on the data.  Read more in the :ref:`User Guide <locally_linear_embedding>`.  Parameters ---------- X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}     Sample data, shape  (n_samples, n_features), in the form of a     numpy array, spars..."),
d("null_space(M, k, k_skip, eigen_solver, tol, max_iter, random_state)", "Find the null space of a matrix M.  Parameters ---------- M : {array, matrix, sparse matrix, LinearOperator}     Input covariance matrix: should be symmetric positive semi-definite  k : integer     Number of eigenvalues/vectors to return  k_skip : integer, optional     Number of low eigenvalues to s..."),]),
c("mds.py", "/manifold/mds.py<br> Multi-dimensional Scaling (MDS)", [
c("MDS(BaseEstimator)", "/manifold/mds.py<br>Multidimensional scaling  Read more in the :ref:`User Guide <multidimensional_scaling>`.  Parameters ---------- metric : boolean, optional, default: True     compute metric or nonmetric SMACOF (Scaling by Majorizing a     Complicated Function) algorithm  n_components : int, optional, default: 2     ...", [
d("__init__(self, n_components, metric, n_init, max_iter, verbose, eps, n_jobs, random_state, dissimilarity)"),
d("_pairwise(self)"),
d("fit(self, X, y, init)"),
d("fit_transform(self, X, y, init)"),]),
d("_smacof_single(similarities, metric, n_components, init, max_iter, verbose, eps, random_state)", "Computes multidimensional scaling using SMACOF algorithm  Parameters ---------- similarities: symmetric ndarray, shape [n * n]     similarities between the points  metric: boolean, optional, default: True     compute metric or nonmetric SMACOF algorithm  n_components: int, optional, default: 2     n..."),
d("smacof(similarities, metric, n_components, init, n_init, n_jobs, max_iter, verbose, eps, random_state, return_n_iter)", "Computes multidimensional scaling using SMACOF (Scaling by Majorizing a Complicated Function) algorithm  The SMACOF algorithm is a multidimensional scaling algorithm: it minimizes a objective function, the *stress*, using a majorization technique. The Stress Majorization, also known as the Guttman T..."),]),
c("setup.py", "/manifold/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("spectral_embedding_.py", "/manifold/spectral_embedding_.py<br> Spectral Embedding", [
c("SpectralEmbedding(BaseEstimator)", "/manifold/spectral_embedding_.py<br>Spectral embedding for non-linear dimensionality reduction.  Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.  Read more in the...", [
d("__init__(self, n_components, affinity, gamma, random_state, eigen_solver, n_neighbors)"),
d("_get_affinity_matrix(self, X, Y)"),
d("_pairwise(self)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),]),
d("_graph_connected_component(graph, node_id)", "Find the largest graph connected components that contains one given node  Parameters ---------- graph : array-like, shape: (n_samples, n_samples)     adjacency matrix of the graph, non-zero weight means an edge     between the nodes  node_id : int     The index of the query node of the graph  Return..."),
d("_graph_is_connected(graph)", "Return whether the graph is connected (True) or Not (False)  Parameters ---------- graph : array-like or sparse matrix, shape: (n_samples, n_samples)     adjacency matrix of the graph, non-zero weight means an edge     between the nodes  Returns ------- is_connected : bool     True means the graph i..."),
d("_set_diag(laplacian, value)", "Set the diagonal of the laplacian matrix and convert it to a sparse format well suited for eigenvalue decomposition  Parameters ---------- laplacian : array or sparse matrix     The graph laplacian value : float     The value of the diagonal  Returns ------- laplacian : array or sparse matrix     An..."),
d("spectral_embedding(adjacency, n_components, eigen_solver, random_state, eigen_tol, norm_laplacian, drop_first)", "Project the sample on the first eigenvectors of the graph Laplacian.  The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split t..."),]),
c("t_sne.py", "/manifold/t_sne.py<br>", [
c("TSNE(BaseEstimator)", "/manifold/t_sne.py<br>t-distributed Stochastic Neighbor Embedding.  t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the hig...", [
d("__init__(self, n_components, perplexity, early_exaggeration, learning_rate, n_iter, metric, init, verbose, random_state)"),
d("_tsne(self, P, alpha, n_samples, random_state, X_embedded)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),]),
d("_gradient_descent(objective, p0, it, n_iter, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, min_error_diff, verbose, args)", "Batch gradient descent with momentum and individual gains.  Parameters ---------- objective : function or callable     Should return a tuple of cost and gradient for a given parameter     vector.  p0 : array-like, shape (n_params,)     Initial parameter vector.  it : int     Current number of iterat..."),
d("_joint_probabilities(distances, desired_perplexity, verbose)", "Compute joint probabilities p_ij from distances.  Parameters ---------- distances : array, shape (n_samples * (n_samples-1) / 2,)     Distances of samples are stored as condensed matrices, i.e.     we omit the diagonal and duplicate entries and store everything     in a one-dimensional array.  desir..."),
d("_kl_divergence(params, P, alpha, n_samples, n_components)", "t-SNE objective function: KL divergence of p_ijs and q_ijs.  Parameters ---------- params : array, shape (n_params,)     Unraveled embedding.  P : array, shape (n_samples * (n_samples-1) / 2,)     Condensed joint probability matrix.  alpha : float     Degrees of freedom of the Student's-t distributi..."),
d("trustworthiness(X, X_embedded, n_neighbors, precomputed)", "Expresses to what extent the local structure is retained.  The trustworthiness is within [0, 1]. It is defined as  .. math::      T(k)  1 - rac{2}{nk (2n - 3k - 1)} \sum^n_{i1}         \sum_{j \in U^{(k)}_i (r(i, j) - k)}  where :math:`r(i, j)` is the rank of the embedded datapoint j according to t..."),]),]),
c("metrics", "/metrics<br>", [
c("cluster", "/metrics/cluster<br>", [
c("tests", "/metrics/cluster/tests<br>", [
c("test_bicluster.py", "/metrics/cluster/tests/test_bicluster.py<br> Testing for bicluster metrics module", [
d("test_consensus_score()"),
d("test_consensus_score_issue2445()", "Different number of biclusters in A and B"),
d("test_jaccard()"),]),
c("test_supervised.py", "/metrics/cluster/tests/test_supervised.py<br>", [
d("test_adjusted_mutual_info_score()"),
d("test_adjustment_for_chance()"),
d("test_complete_but_not_homogeneous_labeling()"),
d("test_contingency_matrix()"),
d("test_entropy()"),
d("test_error_messages_on_wrong_input()"),
d("test_exactly_zero_info_score()"),
d("test_homogeneous_but_not_complete_labeling()"),
d("test_non_consicutive_labels()"),
d("test_not_complete_and_not_homogeneous_labeling()"),
d("test_perfect_matches()"),
d("test_v_measure_and_mutual_information(seed)"),
d("uniform_labelings_scores(score_func, n_samples, k_range, n_runs, seed)"),]),
c("test_unsupervised.py", "/metrics/cluster/tests/test_unsupervised.py<br>", [
d("test_correct_labelsize()"),
d("test_no_nan()"),
d("test_silhouette()"),]),]),
c("bicluster.py", "/metrics/cluster/bicluster.py<br>", [
d("_check_rows_and_columns(a, b)", "Unpacks the row and column arrays and checks their shape."),
d("_jaccard(a_rows, a_cols, b_rows, b_cols)", "Jaccard coefficient on the elements of the two biclusters."),
d("_pairwise_similarity(a, b, similarity)", "Computes pairwise similarity matrix.  result[i, j] is the Jaccard coefficient of a's bicluster i and b's bicluster j."),
d("consensus_score(a, b, similarity)", "The similarity of two sets of biclusters.  Similarity between individual biclusters is computed. Then the best matching between sets is found using the Hungarian algorithm. The final score is the sum of similarities divided by the size of the larger set.  Read more in the :ref:`User Guide <bicluster..."),]),
c("setup.py", "/metrics/cluster/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("supervised.py", "/metrics/cluster/supervised.py<br> Utilities to evaluate the clustering performance of models  Functions named as *_score return a scalar value to maximize: the higher the better.", [
d("adjusted_mutual_info_score(labels_true, labels_pred)", "Adjusted Mutual Information between two clusterings  Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether the..."),
d("adjusted_rand_score(labels_true, labels_pred)", "Rand index adjusted for chance  The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.  The raw RI score is then 'adjusted for chance' into ..."),
d("check_clusterings(labels_true, labels_pred)", "Check that the two clusterings matching 1D integer arrays"),
d("comb2(n)"),
d("completeness_score(labels_true, labels_pred)", "Completeness metric of a cluster labeling given a ground truth  A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.  This metric is independent of the absolute values of the labels: a permutation of the class or cluste..."),
d("contingency_matrix(labels_true, labels_pred, eps)", "Build a contengency matrix describing the relationship between labels.  Parameters ---------- labels_true : int array, shape  [n_samples]     Ground truth class labels to be used as a reference  labels_pred : array, shape  [n_samples]     Cluster labels to evaluate  eps: None or float     If a float..."),
d("entropy(labels)", "Calculates the entropy for a labeling."),
d("homogeneity_completeness_v_measure(labels_true, labels_pred)", "Compute the homogeneity and completeness and V-Measure scores at once  Those metrics are based on normalized conditional entropy measures of the clustering labeling to evaluate given the knowledge of a Ground Truth class labels of the same samples.  A clustering result satisfies homogeneity if all o..."),
d("homogeneity_score(labels_true, labels_pred)", "Homogeneity metric of a cluster labeling given a ground truth  A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label ..."),
d("mutual_info_score(labels_true, labels_pred, contingency)", "Mutual Information between two clusterings  The Mutual Information is a measure of the similarity between two labels of the same data. Where :math:`P(i)` is the probability of a random sample occurring in cluster :math:`U_i` and :math:`P'(j)` is the probability of a random sample occurring in cluste..."),
d("normalized_mutual_info_score(labels_true, labels_pred)", "Normalized Mutual Information between two clusterings  Normalized Mutual Information (NMI) is an normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation). In this function, mutual information is normalized by ``sqrt(H(label..."),
d("v_measure_score(labels_true, labels_pred)", "V-measure cluster labeling given a ground truth.  This score is identical to :func:`normalized_mutual_info_score`.  The V-measure is the harmonic mean between homogeneity and completeness::      v  2 * (homogeneity * completeness) / (homogeneity + completeness)  This metric is independent of the abs..."),]),
c("unsupervised.py", "/metrics/cluster/unsupervised.py<br> Unsupervised evaluation metrics. ", [
d("_intra_cluster_distance(distances_row, labels, i)", "Calculate the mean intra-cluster distance for sample i.  Parameters ---------- distances_row : array, shape  [n_samples]     Pairwise distance matrix between sample i and each sample.  labels : array, shape  [n_samples]     label values for each sample  i : int     Sample index being calculated. It ..."),
d("_nearest_cluster_distance(distances_row, labels, i)", "Calculate the mean nearest-cluster distance for sample i.  Parameters ---------- distances_row : array, shape  [n_samples]     Pairwise distance matrix between sample i and each sample.  labels : array, shape  [n_samples]     label values for each sample  i : int     Sample index being calculated. I..."),
d("silhouette_samples(X, labels, metric)", "Compute the Silhouette Coefficient for each sample.  The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each..."),
d("silhouette_score(X, labels, metric, sample_size, random_state)", "Compute the mean Silhouette Coefficient of all samples.  The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``.  To clarify, ``b`` is th..."),]),]),
c("tests", "/metrics/tests<br>", [
c("test_classification.py", "/metrics/tests/test_classification.py<br>", [
d("make_prediction(dataset, binary)", "Make some classification predictions on a toy dataset using a SVC  If binary is True restrict to a binary classification problem instead of a multiclass classification problem"),
d("test__check_targets()"),
d("test_average_precision_score_duplicate_values()"),
d("test_average_precision_score_score_non_binary_class()"),
d("test_average_precision_score_tied_values()"),
d("test_brier_score_loss()"),
d("test_classification_report_multiclass()"),
d("test_classification_report_multiclass_with_digits()"),
d("test_classification_report_multiclass_with_string_label()"),
d("test_classification_report_multiclass_with_unicode_label()"),
d("test_cohen_kappa()"),
d("test_confusion_matrix_binary()"),
d("test_confusion_matrix_multiclass()"),
d("test_confusion_matrix_multiclass_subset_labels()"),
d("test_fscore_warnings()"),
d("test_hinge_loss_binary()"),
d("test_hinge_loss_multiclass()"),
d("test_hinge_loss_multiclass_invariance_lists()"),
d("test_hinge_loss_multiclass_missing_labels_with_labels_none()"),
d("test_hinge_loss_multiclass_with_missing_labels()"),
d("test_log_loss()"),
d("test_matthews_corrcoef_nan()"),
d("test_multilabel_accuracy_score_subset_accuracy()"),
d("test_multilabel_classification_report()"),
d("test_multilabel_hamming_loss()"),
d("test_multilabel_jaccard_similarity_score()"),
d("test_multilabel_zero_one_loss_subset()"),
d("test_precision_recall_f1_no_labels()"),
d("test_precision_recall_f1_score_binary()"),
d("test_precision_recall_f1_score_multiclass()"),
d("test_precision_recall_f1_score_multiclass_pos_label_none()"),
d("test_precision_recall_f1_score_multilabel_1()"),
d("test_precision_recall_f1_score_multilabel_2()"),
d("test_precision_recall_f1_score_with_an_empty_prediction()"),
d("test_precision_recall_f_binary_single_class()"),
d("test_precision_recall_f_extra_labels()", "Test handling of explicit additional (not in input) labels to PRF     "),
d("test_precision_recall_f_ignored_labels()", "Test a subset of labels may be requested for PRF"),
d("test_precision_recall_fscore_support_errors()"),
d("test_precision_refcall_f1_score_multilabel_unordered_labels()"),
d("test_precision_warnings()"),
d("test_prf_average_compat()"),
d("test_prf_warnings()"),
d("test_recall_warnings()"),
d("test_zero_precision_recall()"),]),
c("test_common.py", "/metrics/tests/test_common.py<br>", [
d("_check_averaging(metric, y_true, y_pred, y_true_binarize, y_pred_binarize, is_multilabel)"),
d("check_averaging(name, y_true, y_true_binarize, y_pred, y_pred_binarize, y_score)"),
d("check_sample_weight_invariance(name, metric, y1, y2)"),
d("check_single_sample(name)"),
d("check_single_sample_multioutput(name)"),
d("test_averaging_multiclass(n_samples, n_classes)"),
d("test_averaging_multilabel(n_classes, n_samples)"),
d("test_averaging_multilabel_all_ones()"),
d("test_averaging_multilabel_all_zeroes()"),
d("test_format_invariance_with_1d_vectors()"),
d("test_invariance_string_vs_numbers_labels()"),
d("test_multilabel_representation_invariance()"),
d("test_multioutput_number_of_output_differ()"),
d("test_multioutput_regression_invariance_to_dimension_shuffling()"),
d("test_no_averaging_labels()"),
d("test_normalize_option_binary_classification(n_samples)"),
d("test_normalize_option_multiclasss_classification()"),
d("test_normalize_option_multilabel_classification()"),
d("test_raise_value_error_multilabel_sequences()"),
d("test_sample_order_invariance()"),
d("test_sample_order_invariance_multilabel_and_multioutput()"),
d("test_sample_weight_invariance(n_samples)"),
d("test_single_sample()"),
d("test_symmetry()"),]),
c("test_pairwise.py", "/metrics/tests/test_pairwise.py<br>", [
d("callable_rbf_kernel(x, y)"),
d("check_pairwise_parallel(func, metric, kwds)"),
d("test_check_XB_returned()"),
d("test_check_dense_matrices()"),
d("test_check_different_dimensions()"),
d("test_check_invalid_dimensions()"),
d("test_check_preserve_type()"),
d("test_check_sparse_arrays()"),
d("test_check_tuple_input()"),
d("test_chi_square_kernel()"),
d("test_cosine_similarity()"),
d("test_cosine_similarity_sparse_output()"),
d("test_euclidean_distances()"),
d("test_kernel_sparse()"),
d("test_kernel_symmetry()"),
d("test_linear_kernel()"),
d("test_paired_distances()"),
d("test_paired_euclidean_distances()"),
d("test_paired_manhattan_distances()"),
d("test_pairwise_callable_nonstrict_metric()"),
d("test_pairwise_distances()"),
d("test_pairwise_distances_argmin_min()"),
d("test_pairwise_kernels()"),
d("test_pairwise_kernels_filter_param()"),
d("test_pairwise_parallel()"),
d("test_rbf_kernel()"),
d("tuplify(X)"),]),
c("test_ranking.py", "/metrics/tests/test_ranking.py<br>", [
d("_auc(y_true, y_score)", "Alternative implementation to check for correctness of `roc_auc_score`."),
d("_average_precision(y_true, y_score)", "Alternative implementation to check for correctness of `average_precision_score`."),
d("_my_lrap(y_true, y_score)", "Simple implementation of label ranking average precision"),
d("_test_precision_recall_curve(y_true, probas_pred)"),
d("check_alternative_lrap_implementation(lrap_score, n_classes, n_samples, random_state)"),
d("check_lrap_error_raised(lrap_score)"),
d("check_lrap_only_ties(lrap_score)"),
d("check_lrap_toy(lrap_score)"),
d("check_lrap_without_tie_and_increasing_score(lrap_score)"),
d("check_zero_or_all_relevant_labels(lrap_score)"),
d("make_prediction(dataset, binary)", "Make some classification predictions on a toy dataset using a SVC  If binary is True restrict to a binary classification problem instead of a multiclass classification problem"),
d("test_auc()"),
d("test_auc_duplicate_values()"),
d("test_auc_errors()"),
d("test_auc_score_non_binary_class()"),
d("test_coverage_error()"),
d("test_coverage_tie_handling()"),
d("test_label_ranking_avp()"),
d("test_label_ranking_loss()"),
d("test_precision_recall_curve()"),
d("test_precision_recall_curve_errors()"),
d("test_precision_recall_curve_pos_label()"),
d("test_precision_recall_curve_toydata()"),
d("test_ranking_appropriate_input_shape()"),
d("test_ranking_loss_ties_handling()"),
d("test_roc_curve()"),
d("test_roc_curve_confidence()"),
d("test_roc_curve_end_points()"),
d("test_roc_curve_hard()"),
d("test_roc_curve_multi()"),
d("test_roc_curve_one_label()"),
d("test_roc_curve_toydata()"),
d("test_roc_nonrepeating_thresholds()"),
d("test_roc_returns_consistency()"),
d("test_score_scale_invariance()"),]),
c("test_regression.py", "/metrics/tests/test_regression.py<br>", [
d("test__check_reg_targets()"),
d("test_multioutput_regression()"),
d("test_regression_custom_weights()"),
d("test_regression_metrics(n_samples)"),
d("test_regression_metrics_at_limits()"),
d("test_regression_multioutput_array()"),]),
c("test_score_objects.py", "/metrics/tests/test_score_objects.py<br>", [
c("DummyScorer(object)", "/metrics/tests/test_score_objects.py<br>Dummy scorer that always returns 1.", [
d("__call__(self, est, X, y)"),]),
c("EstimatorWithFit(BaseEstimator)", "/metrics/tests/test_score_objects.py<br>Dummy estimator to test check_scoring", [
d("fit(self, X, y)"),]),
c("EstimatorWithFitAndPredict(object)", "/metrics/tests/test_score_objects.py<br>Dummy estimator to test check_scoring", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("EstimatorWithFitAndScore(object)", "/metrics/tests/test_score_objects.py<br>Dummy estimator to test check_scoring", [
d("fit(self, X, y)"),
d("score(self, X, y)"),]),
c("EstimatorWithoutFit(object)", "/metrics/tests/test_score_objects.py<br>Dummy estimator to test check_scoring", [
]),
d("test_check_scoring()"),
d("test_check_scoring_gridsearchcv()"),
d("test_classification_scores()"),
d("test_make_scorer()"),
d("test_raises_on_score_list()"),
d("test_regression_scorers()"),
d("test_scorer_sample_weight()"),
d("test_thresholded_scorers()"),
d("test_thresholded_scorers_multilabel_indicator_data()"),
d("test_unsupervised_scorers()"),]),]),
c("base.py", "/metrics/base.py<br> Common code for all metrics", [
c("UndefinedMetricWarning(UserWarning)", "/metrics/base.py<br>", [
]),
d("_average_binary_score(binary_metric, y_true, y_score, average, sample_weight)", "Average a binary metric for multilabel classification  Parameters ---------- y_true : array, shape  [n_samples] or [n_samples, n_classes]     True binary labels in binary label indicators.  y_score : array, shape  [n_samples] or [n_samples, n_classes]     Target scores, can either be probability est..."),]),
c("classification.py", "/metrics/classification.py<br> Metrics to assess performance on classification task given classe prediction  Functions named as ``*_score`` return a scalar value to maximize: the higher the better  Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize: the lower the better", [
d("_check_binary_probabilistic_predictions(y_true, y_prob)", "Check that y_true is binary and y_prob contains valid probabilities"),
d("_check_targets(y_true, y_pred)", "Check that y_true and y_pred belong to the same classification task  This converts multiclass or binary types to a common shape, and raises a ValueError for a mix of multilabel and multiclass targets, a mix of multilabel formats, for the presence of continuous-valued or multioutput targets, or for t..."),
d("_prf_divide(numerator, denominator, metric, modifier, average, warn_for)", "Performs division and handles divide-by-zero.  On zero-division, sets the corresponding result elements to zero and raises a warning.  The metric, modifier and average arguments are used only for determining an appropriate warning."),
d("_weighted_sum(sample_score, sample_weight, normalize)"),
d("accuracy_score(y_true, y_pred, normalize, sample_weight)", "Accuracy classification score.  In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must *exactly* match the corresponding set of labels in y_true.  Read more in the :ref:`User Guide <accuracy_score>`.  Parameters ---------- y_true : 1d arra..."),
d("brier_score_loss(y_true, y_prob, sample_weight, pos_label)", "Compute the Brier score.  The smaller the Brier score, the better, hence the naming with 'loss'.  Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outco..."),
d("classification_report(y_true, y_pred, labels, target_names, sample_weight, digits)", "Build a text report showing the main classification metrics  Read more in the :ref:`User Guide <classification_report>`.  Parameters ---------- y_true : 1d array-like, or label indicator array / sparse matrix     Ground truth (correct) target values.  y_pred : 1d array-like, or label indicator array..."),
d("cohen_kappa_score(y1, y2, labels)", "Cohen's kappa: a statistic that measures inter-annotator agreement.  This function computes Cohen's kappa [1], a score that expresses the level of agreement between two annotators on a classification problem. It is defined as  .. math::     \kappa  (p_o - p_e) / (1 - p_e)  where :math:`p_o` is the e..."),
d("confusion_matrix(y_true, y_pred, labels)", "Compute confusion matrix to evaluate the accuracy of a classification  By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}` is equal to the number of observations known to be in group :math:`i` but predicted to be in group :math:`j`.  Read more in the :ref:`User Guide <confusion_..."),
d("f1_score(y_true, y_pred, labels, pos_label, average, sample_weight)", "Compute the F1 score, also known as balanced F-score or F-measure  The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The f..."),
d("fbeta_score(y_true, y_pred, beta, labels, pos_label, average, sample_weight)", "Compute the F-beta score  The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.  The `beta` parameter determines the weight of precision in the combined score. ``beta < 1`` lends more weight to precision, while ``beta > 1`` ..."),
d("hamming_loss(y_true, y_pred, classes)", "Compute the average Hamming loss.  The Hamming loss is the fraction of labels that are incorrectly predicted.  Read more in the :ref:`User Guide <hamming_loss>`.  Parameters ---------- y_true : 1d array-like, or label indicator array / sparse matrix     Ground truth (correct) labels.  y_pred : 1d ar..."),
d("hinge_loss(y_true, pred_decision, labels, sample_weight)", "Average hinge loss (non-regularized)  In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, ``margin  y_true * pred_decision`` is always negative (since the signs disagree), implying ``1 - margin`` is always greater than 1.  The cumulated hing..."),
d("jaccard_similarity_score(y_true, y_pred, normalize, sample_weight)", "Jaccard similarity coefficient score  The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in ``y_true``.  Read ..."),
d("log_loss(y_true, y_pred, eps, normalize, sample_weight)", "Log loss, aka logistic loss or cross-entropy loss.  This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier's predictions. For a single sample with tru..."),
d("matthews_corrcoef(y_true, y_pred)", "Compute the Matthews correlation coefficient (MCC) for binary classes  The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balan..."),
d("precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)", "Compute precision, recall, F-measure and support for each class  The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is n..."),
d("precision_score(y_true, y_pred, labels, pos_label, average, sample_weight)", "Compute the precision  The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  The best value is 1 and the wors..."),
d("recall_score(y_true, y_pred, labels, pos_label, average, sample_weight)", "Compute the recall  The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of true positives and ``fn`` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.  The best value is 1 and the worst value is 0.  Read more in..."),
d("zero_one_loss(y_true, y_pred, normalize, sample_weight)", "Zero-one classification loss.  If normalize is ``True``, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.  Read more in the :ref:`User Guide <zero_one_loss>`.  Parameters ---------- y_true : 1d array-like, or label ..."),]),
c("pairwise.py", "/metrics/pairwise.py<br>", [
d("_pairwise_callable(X, Y, metric)", "Handle the callable case for pairwise_{distances,kernels}     "),
d("_parallel_pairwise(X, Y, func, n_jobs)", "Break the pairwise matrix in n_jobs even slices and compute them in parallel"),
d("_return_float_dtype(X, Y)", "1. If dtype of X and Y is float32, then dtype float32 is returned. 2. Else dtype float is returned."),
d("additive_chi2_kernel(X, Y)", "Computes the additive chi-squared kernel between observations in X and Y  The chi-squared kernel is computed between each pair of rows in X and Y.  X and Y have to be non-negative. This kernel is most commonly applied to histograms.  The chi-squared kernel is given by::      k(x, y)  -Sum [(x - y)^2..."),
d("check_paired_arrays(X, Y)", "Set X and Y appropriately and checks inputs for paired distances  All paired distance metrics should use this function first to assert that the given parameters are correct and safe to use.  Specifically, this function first ensures that both X and Y are arrays, then checks that they are at least tw..."),
d("check_pairwise_arrays(X, Y)", "Set X and Y appropriately and checks inputs  If Y is None, it is set as a pointer to X (i.e. not a copy). If Y is given, this does not happen. All distance metrics should use this function first to assert that the given parameters are correct and safe to use.  Specifically, this function first ensur..."),
d("chi2_kernel(X, Y, gamma)", "Computes the exponential chi-squared kernel X and Y.  The chi-squared kernel is computed between each pair of rows in X and Y.  X and Y have to be non-negative. This kernel is most commonly applied to histograms.  The chi-squared kernel is given by::      k(x, y)  exp(-gamma Sum [(x - y)^2 / (x + y)..."),
d("cosine_distances(X, Y)", "Compute cosine distance between samples in X and Y.  Cosine distance is defined as 1.0 minus the cosine similarity.  Read more in the :ref:`User Guide <metrics>`.  Parameters ---------- X : array_like, sparse matrix     with shape (n_samples_X, n_features).  Y : array_like, sparse matrix (optional) ..."),
d("cosine_similarity(X, Y, dense_output)", "Compute cosine similarity between samples in X and Y.  Cosine similarity, or the cosine kernel, computes similarity as the normalized dot product of X and Y:      K(X, Y)  <X, Y> / (||X||*||Y||)  On L2-normalized data, this function is equivalent to linear_kernel.  Read more in the :ref:`User Guide ..."),
d("distance_metrics()", "Valid metrics for pairwise_distances.  This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the mapping for each of the valid strings.  The valid distance metrics, and the function they map to, are:        metric           Function       'citybloc..."),
d("euclidean_distances(X, Y, Y_norm_squared, squared)", "Considering the rows of X (and YX) as vectors, compute the distance matrix between each pair of vectors.  For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as::      dist(x, y)  sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))  This formulation has two advant..."),
d("kernel_metrics()", "Valid metrics for pairwise_kernels  This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose description of the mapping for each of the valid strings.  The valid distance metrics, and the function they map to, are:         metric            Functio..."),
d("linear_kernel(X, Y)", "Compute the linear kernel between X and Y.  Read more in the :ref:`User Guide <linear_kernel>`.  Parameters ---------- X : array of shape (n_samples_1, n_features)  Y : array of shape (n_samples_2, n_features)  Returns ------- Gram matrix : array of shape (n_samples_1, n_samples_2)"),
d("manhattan_distances(X, Y, sum_over_features, size_threshold)", "Compute the L1 distances between the vectors in X and Y.  With sum_over_features equal to False it returns the componentwise distances.  Read more in the :ref:`User Guide <metrics>`.  Parameters ---------- X : array_like     An array with shape (n_samples_X, n_features).  Y : array_like, optional   ..."),
d("paired_cosine_distances(X, Y)", "Computes the paired cosine distances between X and Y  Read more in the :ref:`User Guide <metrics>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)  Y : array-like, shape (n_samples, n_features)  Returns ------- distances : ndarray, shape (n_samples, )  Notes ------ The cosine d..."),
d("paired_distances(X, Y, metric)", "Computes the paired distances between X and Y.  Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...  Read more in the :ref:`User Guide <metrics>`.  Parameters ---------- X : ndarray (n_samples, n_features)     Array 1 for distance computation.  Y : ndarray (n_samples, n_features)     A..."),
d("paired_euclidean_distances(X, Y)", "Computes the paired euclidean distances between X and Y  Read more in the :ref:`User Guide <metrics>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)  Y : array-like, shape (n_samples, n_features)  Returns ------- distances : ndarray (n_samples, )"),
d("paired_manhattan_distances(X, Y)", "Compute the L1 distances between the vectors in X and Y.  Read more in the :ref:`User Guide <metrics>`.  Parameters ---------- X : array-like, shape (n_samples, n_features)  Y : array-like, shape (n_samples, n_features)  Returns ------- distances : ndarray (n_samples, )"),
d("pairwise_distances(X, Y, metric, n_jobs)", "Compute the distance matrix from a vector array X and optional Y.  This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.  This method provide..."),
d("pairwise_distances_argmin(X, Y, axis, metric, batch_size, metric_kwargs)", "Compute minimum distances between one point and a set of points.  This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).  This is mostly equivalent to calling:      pairwise_distances(X, YY, metricmetric).argmin(axisaxis)  but uses..."),
d("pairwise_distances_argmin_min(X, Y, axis, metric, batch_size, metric_kwargs)", "Compute minimum distances between one point and a set of points.  This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.  This is mostly equivalent to calling:      (pairwise_distances(X, YY..."),
d("pairwise_kernels(X, Y, metric, filter_params, n_jobs)", "Compute the kernel between arrays X and optional array Y.  This method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector array, the kernels are computed. If the input is a kernel matrix, it is returned instead.  This method provides a safe way to t..."),
d("polynomial_kernel(X, Y, degree, gamma, coef0)", "Compute the polynomial kernel between X and Y::      K(X, Y)  (gamma <X, Y> + coef0)^degree  Read more in the :ref:`User Guide <polynomial_kernel>`.  Parameters ---------- X : ndarray of shape (n_samples_1, n_features)  Y : ndarray of shape (n_samples_2, n_features)  coef0 : int, default 1  degree :..."),
d("rbf_kernel(X, Y, gamma)", "Compute the rbf (gaussian) kernel between X and Y::      K(x, y)  exp(-gamma ||x-y||^2)  for each pair of rows x in X and y in Y.  Read more in the :ref:`User Guide <rbf_kernel>`.  Parameters ---------- X : array of shape (n_samples_X, n_features)  Y : array of shape (n_samples_Y, n_features)  gamma..."),
d("sigmoid_kernel(X, Y, gamma, coef0)", "Compute the sigmoid kernel between X and Y::      K(X, Y)  tanh(gamma <X, Y> + coef0)  Read more in the :ref:`User Guide <sigmoid_kernel>`.  Parameters ---------- X : ndarray of shape (n_samples_1, n_features)  Y : ndarray of shape (n_samples_2, n_features)  coef0 : int, default 1  Returns ------- G..."),]),
c("ranking.py", "/metrics/ranking.py<br> Metrics to assess performance on classification task given scores  Functions named as ``*_score`` return a scalar value to maximize: the higher the better  Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize: the lower the better", [
d("_binary_clf_curve(y_true, y_score, pos_label, sample_weight)", "Calculate true and false positives per binary classification threshold.  Parameters ---------- y_true : array, shape  [n_samples]     True targets of binary classification  y_score : array, shape  [n_samples]     Estimated probabilities or decision function  pos_label : int, optional (defaultNone)  ..."),
d("auc(x, y, reorder)", "Compute Area Under the Curve (AUC) using the trapezoidal rule  This is a general function, given points on a curve.  For computing the area under the ROC-curve, see :func:`roc_auc_score`.  Parameters ---------- x : array, shape  [n]     x coordinates.  y : array, shape  [n]     y coordinates.  reord..."),
d("average_precision_score(y_true, y_score, average, sample_weight)", "Compute average precision (AP) from prediction scores  This score corresponds to the area under the precision-recall curve.  Note: this implementation is restricted to the binary classification task or multilabel classification task.  Read more in the :ref:`User Guide <precision_recall_f_measure_met..."),
d("coverage_error(y_true, y_score, sample_weight)", "Coverage error measure  Compute how far we need to go through the ranked scores to cover all true labels. The best value is equal to the average number of labels in ``y_true`` per sample.  Ties in ``y_scores`` are broken by giving maximal rank that would have been assigned to all tied values.  Read ..."),
d("label_ranking_average_precision_score(y_true, y_score)", "Compute ranking-based average precision  Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.  This metric is used in multilabel ranking problem, where the goal is to give better rank to th..."),
d("label_ranking_loss(y_true, y_score, sample_weight)", "Compute Ranking loss measure  Compute the average number of label pairs that are incorrectly ordered given y_score weighted by the size of the label set and the number of labels not in the label set.  This is similar to the error set size, but weighted by the number of relevant and irrelevant labels..."),
d("precision_recall_curve(y_true, probas_pred, pos_label, sample_weight)", "Compute precision-recall pairs for different probability thresholds  Note: this implementation is restricted to the binary classification task.  The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intui..."),
d("roc_auc_score(y_true, y_score, average, sample_weight)", "Compute Area Under the Curve (AUC) from prediction scores  Note: this implementation is restricted to the binary classification task or multilabel classification task in label indicator format.  Read more in the :ref:`User Guide <roc_metrics>`.  Parameters ---------- y_true : array, shape  [n_sample..."),
d("roc_curve(y_true, y_score, pos_label, sample_weight)", "Compute Receiver operating characteristic (ROC)  Note: this implementation is restricted to the binary classification task.  Read more in the :ref:`User Guide <roc_metrics>`.  Parameters ----------  y_true : array, shape  [n_samples]     True binary labels in range {0, 1} or {-1, 1}.  If labels are ..."),]),
c("regression.py", "/metrics/regression.py<br> Metrics to assess performance on regression task  Functions named as ``*_score`` return a scalar value to maximize: the higher the better  Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize: the lower the better", [
d("_check_reg_targets(y_true, y_pred, multioutput)", "Check that y_true and y_pred belong to the same regression task  Parameters ---------- y_true : array-like,  y_pred : array-like,  multioutput : array-like or string in ['raw_values', uniform_average',     'variance_weighted'] or None     None is accepted due to backward compatibility of r2_score()...."),
d("explained_variance_score(y_true, y_pred, sample_weight, multioutput)", "Explained variance regression score function  Best possible score is 1.0, lower values are worse.  Read more in the :ref:`User Guide <explained_variance_score>`.  Parameters ---------- y_true : array-like of shape  (n_samples) or (n_samples, n_outputs)     Ground truth (correct) target values.  y_pr..."),
d("mean_absolute_error(y_true, y_pred, sample_weight, multioutput)", "Mean absolute error regression loss  Read more in the :ref:`User Guide <mean_absolute_error>`.  Parameters ---------- y_true : array-like of shape  (n_samples) or (n_samples, n_outputs)     Ground truth (correct) target values.  y_pred : array-like of shape  (n_samples) or (n_samples, n_outputs)    ..."),
d("mean_squared_error(y_true, y_pred, sample_weight, multioutput)", "Mean squared error regression loss  Read more in the :ref:`User Guide <mean_squared_error>`.  Parameters ---------- y_true : array-like of shape  (n_samples) or (n_samples, n_outputs)     Ground truth (correct) target values.  y_pred : array-like of shape  (n_samples) or (n_samples, n_outputs)     E..."),
d("median_absolute_error(y_true, y_pred)", "Median absolute error regression loss  Read more in the :ref:`User Guide <median_absolute_error>`.  Parameters ---------- y_true : array-like of shape  (n_samples)     Ground truth (correct) target values.  y_pred : array-like of shape  (n_samples)     Estimated target values.  Returns ------- loss ..."),
d("r2_score(y_true, y_pred, sample_weight, multioutput)", "R^2 (coefficient of determination) regression score function.  Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.  Read more in t..."),]),
c("scorer.py", "/metrics/scorer.py<br> The sklearn.metrics.scorer submodule implements a flexible interface for model selection and evaluation using arbitrary score functions.  A scorer object is a callable that can be passed to :class:`sklearn.grid_search.GridSearchCV` or :func:`sklearn.cross_validation.cross_val_score` as the ``scoring...", [
c("_BaseScorer()", "/metrics/scorer.py<br>", [
d("__call__(self, estimator, X, y, sample_weight)"),
d("__init__(self, score_func, sign, kwargs)"),
d("__repr__(self)"),
d("_factory_args(self)"),]),
c("_PredictScorer(_BaseScorer)", "/metrics/scorer.py<br>", [
d("__call__(self, estimator, X, y_true, sample_weight)"),]),
c("_ProbaScorer(_BaseScorer)", "/metrics/scorer.py<br>", [
d("__call__(self, clf, X, y, sample_weight)"),
d("_factory_args(self)"),]),
c("_ThresholdScorer(_BaseScorer)", "/metrics/scorer.py<br>", [
d("__call__(self, clf, X, y, sample_weight)"),
d("_factory_args(self)"),]),
d("_passthrough_scorer(estimator)", "Function that wraps estimator.score"),
d("check_scoring(estimator, scoring, allow_none)", "Determine scorer from user options.  A TypeError will be thrown if the estimator cannot be scored.  Parameters ---------- estimator : estimator object implementing 'fit'     The object to use to fit the data.  scoring : string, callable or None, optional, default: None     A string (see model evalua..."),
d("get_scorer(scoring)"),
d("make_scorer(score_func, greater_is_better, needs_proba, needs_threshold)", "Make a scorer from a performance metric or loss function.  This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as ``accuracy_score``, ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision`` and returns a callable..."),]),
c("setup.py", "/metrics/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),]),
c("mixture", "/mixture<br>", [
c("tests", "/mixture/tests<br>", [
c("test_dpgmm.py", "/mixture/tests/test_dpgmm.py<br>", [
c("DPGMMTester(GMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
d("score(self, g, train_obs)"),]),
c("TestDPGMMWithDiagCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestDPGMMWithFullCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestDPGMMWithSphericalCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestDPGMMWithTiedCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestVBGMMWithDiagCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestVBGMMWithFullCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestVBGMMWithSphericalCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("TestVBGMMWithTiedCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
]),
c("VBGMMTester(GMMTester)", "/mixture/tests/test_dpgmm.py<br>", [
d("score(self, g, train_obs)"),]),
d("do_model(self)"),
d("test_class_weights()"),
d("test_log_normalize()"),
d("test_verbose_boolean()"),
d("test_verbose_first_level()"),
d("test_verbose_second_level()"),]),
c("test_gmm.py", "/mixture/tests/test_gmm.py<br>", [
c("GMMTester()", "/mixture/tests/test_gmm.py<br>", [
d("_setUp(self)"),
d("score(self, g, X)"),
d("test_eval(self)"),
d("test_sample(self, n)"),
d("test_train(self, params)"),
d("test_train_1d(self, params)"),
d("test_train_degenerate(self, params)"),]),
c("TestGMMWithDiagonalCovars(GMMTester)", "/mixture/tests/test_gmm.py<br>", [
]),
c("TestGMMWithFullCovars(GMMTester)", "/mixture/tests/test_gmm.py<br>", [
]),
c("TestGMMWithSphericalCovars(GMMTester)", "/mixture/tests/test_gmm.py<br>", [
]),
c("TestGMMWithTiedCovars(GMMTester)", "/mixture/tests/test_gmm.py<br>", [
]),
d("_naive_lmvnpdf_diag(X, mu, cv)"),
d("assert_fit_predict_correct(model, X)"),
d("check_positive_definite_covars(covariance_type)", "Test that covariance matrices do not become non positive definite  Due to the accumulation of round-off errors, the computation of the covariance  matrices during the learning phase could lead to non-positive definite covariance matrices. Namely the use of the formula:  .. math:: C  (\sum_i w_i  x_i..."),
d("test_1d_1component()"),
d("test_GMM_attributes()"),
d("test_aic()"),
d("test_fit_predict()", "test that gmm.fit_predict is equivalent to gmm.fit + gmm.predict"),
d("test_lmvnpdf_diag()"),
d("test_lmvnpdf_full()"),
d("test_lmvnpdf_spherical()"),
d("test_lvmpdf_full_cv_non_positive_definite()"),
d("test_multiple_init()"),
d("test_n_parameters()"),
d("test_positive_definite_covars()"),
d("test_sample_gaussian()"),
d("test_verbose_first_level()"),
d("test_verbose_second_level()"),]),]),
c("dpgmm.py", "/mixture/dpgmm.py<br> Bayesian Gaussian Mixture Models and Dirichlet Process Gaussian Mixture Models", [
c("DPGMM(GMM)", "/mixture/dpgmm.py<br>Variational Inference for the Infinite Gaussian Mixture Model.  DPGMM stands for Dirichlet Process Gaussian Mixture Model, and it is an infinite mixture model with the Dirichlet Process as a prior distribution on the number of clusters. In practice the approximate inference algorithm uses a truncate...", [
d("__init__(self, n_components, covariance_type, alpha, random_state, thresh, tol, verbose, min_covar, n_iter, params, init_params)"),
d("_bound_concentration(self)"),
d("_bound_means(self)"),
d("_bound_precisions(self)"),
d("_bound_proportions(self, z)"),
d("_do_mstep(self, X, z, params)"),
d("_fit(self, X, y)"),
d("_get_covars(self)"),
d("_get_precisions(self)"),
d("_initialize_gamma(self)"),
d("_logprior(self, z)"),
d("_monitor(self, X, z, n, end)"),
d("_set_covars(self, covars)"),
d("_set_weights(self)"),
d("_update_concentration(self, z)"),
d("_update_means(self, X, z)"),
d("_update_precisions(self, X, z)"),
d("lower_bound(self, X, z)"),
d("score_samples(self, X)"),]),
c("VBGMM(DPGMM)", "/mixture/dpgmm.py<br>Variational Inference for the Gaussian Mixture Model  Variational inference for a Gaussian mixture model probability distribution. This class allows for easy and efficient inference of an approximate posterior distribution over the parameters of a Gaussian mixture model with a fixed number of compon...", [
d("__init__(self, n_components, covariance_type, alpha, random_state, thresh, tol, verbose, min_covar, n_iter, params, init_params)"),
d("_bound_concentration(self)"),
d("_bound_proportions(self, z)"),
d("_initialize_gamma(self)"),
d("_monitor(self, X, z, n, end)"),
d("_set_weights(self)"),
d("_update_concentration(self, z)"),
d("score_samples(self, X)"),]),
d("_bound_state_log_lik(X, initial_bound, precs, means, covariance_type)", "Update the bound with likelihood terms, for standard covariance types"),
d("_bound_wishart(a, B, detB)", "Returns a function of the dof, scale matrix and its determinant used as an upper bound in variational approcimation of the evidence"),
d("_sym_quad_form(x, mu, A)", "helper function to calculate symmetric quadratic form x.T * A * x"),
d("digamma(x)"),
d("gammaln(x)"),
d("log_normalize(v, axis)", "Normalized probabilities from unnormalized log-probabilites"),
d("wishart_log_det(a, b, detB, n_features)", "Expected value of the log of the determinant of a Wishart  The expected value of the logarithm of the determinant of a wishart-distributed random variable with the specified parameters."),
d("wishart_logz(v, s, dets, n_features)", "The logarithm of the normalization constant for the wishart distribution"),]),
c("gmm.py", "/mixture/gmm.py<br> Gaussian Mixture Models.  This implementation corresponds to frequentist (non-Bayesian) formulation of Gaussian Mixture Models.", [
c("GMM(BaseEstimator)", "/mixture/gmm.py<br>Gaussian Mixture Model  Representation of a Gaussian mixture model probability distribution. This class allows for easy evaluation of, sampling from, and maximum-likelihood estimation of the parameters of a GMM distribution.  Initializes parameters such that every mixture component has zero mean and...", [
d("__init__(self, n_components, covariance_type, random_state, thresh, tol, min_covar, n_iter, n_init, params, init_params, verbose)"),
d("_do_mstep(self, X, responsibilities, params, min_covar)"),
d("_fit(self, X, y, do_prediction)"),
d("_get_covars(self)"),
d("_n_parameters(self)"),
d("_set_covars(self, covars)"),
d("aic(self, X)"),
d("bic(self, X)"),
d("fit(self, X, y)"),
d("fit_predict(self, X, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("sample(self, n_samples, random_state)"),
d("score(self, X, y)"),
d("score_samples(self, X)"),]),
d("_covar_mstep_diag(gmm, X, responsibilities, weighted_X_sum, norm, min_covar)", "Performing the covariance M step for diagonal cases"),
d("_covar_mstep_full(gmm, X, responsibilities, weighted_X_sum, norm, min_covar)", "Performing the covariance M step for full cases"),
d("_covar_mstep_spherical()", "Performing the covariance M step for spherical cases"),
d("_covar_mstep_tied(gmm, X, responsibilities, weighted_X_sum, norm, min_covar)"),
d("_log_multivariate_normal_density_diag(X, means, covars)", "Compute Gaussian log-density at X for a diagonal model"),
d("_log_multivariate_normal_density_full(X, means, covars, min_covar)", "Log probability for full covariance matrices."),
d("_log_multivariate_normal_density_spherical(X, means, covars)", "Compute Gaussian log-density at X for a spherical model"),
d("_log_multivariate_normal_density_tied(X, means, covars)", "Compute Gaussian log-density at X for a tied model"),
d("_validate_covars(covars, covariance_type, n_components)", "Do basic checks on matrix covariance sizes and values     "),
d("distribute_covar_matrix_to_match_covariance_type(tied_cv, covariance_type, n_components)", "Create all the covariance matrices from a given template"),
d("log_multivariate_normal_density(X, means, covars, covariance_type)", "Compute the log probability under a multivariate Gaussian distribution.  Parameters ---------- X : array_like, shape (n_samples, n_features)     List of n_features-dimensional data points.  Each row corresponds to a     single data point.  means : array_like, shape (n_components, n_features)     Lis..."),
d("sample_gaussian(mean, covar, covariance_type, n_samples, random_state)", "Generate random samples from a Gaussian distribution.  Parameters ---------- mean : array_like, shape (n_features,)     Mean of the distribution.  covar : array_like, optional     Covariance of the distribution. The shape depends on `covariance_type`:         scalar if 'spherical',         (n_featur..."),]),]),
c("neighbors", "/neighbors<br>", [
c("tests", "/neighbors/tests<br>", [
c("test_approximate.py", "/neighbors/tests/test_approximate.py<br> Testing for the approximate neighbor search using Locality Sensitive Hashing Forest module (sklearn.neighbors.LSHForest).", [
d("test_candidates()"),
d("test_distances()"),
d("test_fit()"),
d("test_graphs()"),
d("test_hash_functions()"),
d("test_kneighbors()"),
d("test_neighbors_accuracy_with_n_candidates()"),
d("test_neighbors_accuracy_with_n_estimators()"),
d("test_partial_fit()"),
d("test_radius_neighbors()"),
d("test_radius_neighbors_boundary_handling()"),
d("test_sparse_input()"),]),
c("test_ball_tree.py", "/neighbors/tests/test_ball_tree.py<br>", [
d("brute_force_neighbors(X, Y, k, metric)"),
d("compute_kernel_slow(Y, X, kernel, h)"),
d("dist_func(x1, x2, p)"),
d("test_ball_tree_kde(n_samples, n_features)"),
d("test_ball_tree_pickle()"),
d("test_ball_tree_query()"),
d("test_ball_tree_query_boolean_metrics()"),
d("test_ball_tree_query_discrete_metrics()"),
d("test_ball_tree_query_radius(n_samples, n_features)"),
d("test_ball_tree_query_radius_distance(n_samples, n_features)"),
d("test_ball_tree_two_point(n_samples, n_features)"),
d("test_gaussian_kde(n_samples)"),
d("test_neighbors_heap(n_pts, n_nbrs)"),
d("test_node_heap(n_nodes)"),
d("test_query_haversine()"),
d("test_simultaneous_sort(n_rows, n_pts)"),]),
c("test_dist_metrics.py", "/neighbors/tests/test_dist_metrics.py<br>", [
c("TestMetrics()", "/neighbors/tests/test_dist_metrics.py<br>", [
d("__init__(self, n1, n2, d, zero_frac, rseed, dtype)"),
d("check_cdist(self, metric, kwargs, D_true)"),
d("check_cdist_bool(self, metric, D_true)"),
d("check_pdist(self, metric, kwargs, D_true)"),
d("check_pdist_bool(self, metric, D_true)"),
d("test_cdist(self)"),
d("test_pdist(self)"),]),
d("cmp_version(version1, version2)"),
d("dist_func(x1, x2, p)"),
d("test_haversine_metric()"),
d("test_pyfunc_metric()"),]),
c("test_kde.py", "/neighbors/tests/test_kde.py<br>", [
d("compute_kernel_slow(Y, X, kernel, h)"),
d("test_kde_algorithm_metric_choice()"),
d("test_kde_badargs()"),
d("test_kde_pipeline_gridsearch()"),
d("test_kde_score(n_samples, n_features)"),
d("test_kernel_density(n_samples, n_features)"),
d("test_kernel_density_sampling(n_samples, n_features)"),]),
c("test_kd_tree.py", "/neighbors/tests/test_kd_tree.py<br>", [
d("brute_force_neighbors(X, Y, k, metric)"),
d("compute_kernel_slow(Y, X, kernel, h)"),
d("test_gaussian_kde(n_samples)"),
d("test_kd_tree_kde(n_samples, n_features)"),
d("test_kd_tree_pickle()"),
d("test_kd_tree_query()"),
d("test_kd_tree_query_radius(n_samples, n_features)"),
d("test_kd_tree_query_radius_distance(n_samples, n_features)"),
d("test_kd_tree_two_point(n_samples, n_features)"),
d("test_neighbors_heap(n_pts, n_nbrs)"),
d("test_node_heap(n_nodes)"),
d("test_simultaneous_sort(n_rows, n_pts)"),]),
c("test_nearest_centroid.py", "/neighbors/tests/test_nearest_centroid.py<br> Testing for the nearest centroid module.", [
d("test_classification_toy()"),
d("test_iris()"),
d("test_iris_shrinkage()"),
d("test_manhattan_metric()"),
d("test_pickle()"),
d("test_precomputed()"),
d("test_predict_translated_data()"),
d("test_shrinkage_threshold_decoded_y()"),]),
c("test_neighbors.py", "/neighbors/tests/test_neighbors.py<br>", [
d("_weight_func(dist)", "Weight function to replace lambda d: d ** -2. The lambda function is not valid because: if d0 then 0^-2 is not valid. "),
d("check_object_arrays(nparray, list_check)"),
d("test_KNeighborsClassifier_multioutput()"),
d("test_KNeighborsRegressor_multioutput_uniform_weight()"),
d("test_RadiusNeighborsClassifier_multioutput()"),
d("test_RadiusNeighborsRegressor_multioutput(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_RadiusNeighborsRegressor_multioutput_with_uniform_weight()"),
d("test_callable_metric()"),
d("test_dtype_convert()"),
d("test_include_self_neighbors_graph()"),
d("test_k_and_radius_neighbors_X_None()"),
d("test_k_and_radius_neighbors_duplicates()"),
d("test_k_and_radius_neighbors_train_is_not_query()"),
d("test_kneighbors_classifier(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_kneighbors_classifier_float_labels(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_kneighbors_classifier_predict_proba()"),
d("test_kneighbors_classifier_sparse(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_kneighbors_graph()"),
d("test_kneighbors_graph_sparse(seed)"),
d("test_kneighbors_regressor(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_kneighbors_regressor_multioutput(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_kneighbors_regressor_sparse(n_samples, n_features, n_test_pts, n_neighbors, random_state)"),
d("test_metric_params_interface()"),
d("test_neighbors_badargs()"),
d("test_neighbors_digits()"),
d("test_neighbors_iris()"),
d("test_neighbors_metrics(n_samples, n_features, n_query_pts, n_neighbors)"),
d("test_neighbors_regressors_zero_distance()"),
d("test_non_euclidean_kneighbors()"),
d("test_predict_sparse_ball_kd_tree()"),
d("test_radius_neighbors_boundary_handling()", "Test whether points lying on boundary are handled consistently  Also ensures that even with only one query point, an object array is returned rather than a 2d array."),
d("test_radius_neighbors_classifier(n_samples, n_features, n_test_pts, radius, random_state)"),
d("test_radius_neighbors_classifier_outlier_labeling()"),
d("test_radius_neighbors_classifier_when_no_neighbors()"),
d("test_radius_neighbors_classifier_zero_distance()"),
d("test_radius_neighbors_graph()"),
d("test_radius_neighbors_graph_sparse(seed)"),
d("test_radius_neighbors_regressor(n_samples, n_features, n_test_pts, radius, random_state)"),
d("test_unsupervised_inputs()"),
d("test_unsupervised_kneighbors(n_samples, n_features, n_query_pts, n_neighbors)"),
d("test_unsupervised_radius_neighbors(n_samples, n_features, n_query_pts, radius, random_state)"),]),]),
c("approximate.py", "/neighbors/approximate.py<br> Approximate nearest neighbor search", [
c("GaussianRandomProjectionHash(ProjectionToHashMixin, GaussianRandomProjection)", "/neighbors/approximate.py<br>Use GaussianRandomProjection to produce a cosine LSH fingerprint", [
d("__init__(self, n_components, random_state)"),]),
c("LSHForest(BaseEstimator, KNeighborsMixin, RadiusNeighborsMixin)", "/neighbors/approximate.py<br>Performs approximate nearest neighbor search using LSH forest.  LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-le...", [
d("__init__(self, n_estimators, radius, n_candidates, n_neighbors, min_hash_match, radius_cutoff_ratio, random_state)"),
d("_compute_distances(self, query, candidates)"),
d("_generate_masks(self)"),
d("_get_candidates(self, query, max_depth, bin_queries, n_neighbors)"),
d("_get_radius_neighbors(self, query, max_depth, bin_queries, radius)"),
d("_query(self, X)"),
d("fit(self, X, y)"),
d("kneighbors(self, X, n_neighbors, return_distance)"),
d("partial_fit(self, X, y)"),
d("radius_neighbors(self, X, radius, return_distance)"),]),
c("ProjectionToHashMixin(object)", "/neighbors/approximate.py<br>Turn a transformed real-valued array into a hash", [
d("_to_hash(projected)"),
d("fit_transform(self, X, y)"),
d("transform(self, X, y)"),]),
d("_array_of_arrays(list_of_arrays)", "Creates an array of array from list of arrays."),
d("_find_longest_prefix_match(tree, bin_X, hash_size, left_masks, right_masks)", "Find the longest prefix match in tree for each query in bin_X  Most significant bits are considered as the prefix."),
d("_find_matching_indices(tree, bin_X, left_mask, right_mask)", "Finds indices in sorted array of integers.  Most significant h bits in the binary representations of the integers are matched with the items' most significant h bits."),]),
c("base.py", "/neighbors/base.py<br> Base and mixin classes for nearest neighbors", [
c("KNeighborsMixin(object)", "/neighbors/base.py<br>Mixin for k-neighbors searches", [
d("kneighbors(self, X, n_neighbors, return_distance)"),
d("kneighbors_graph(self, X, n_neighbors, mode)"),]),
c("NeighborsBase()", "/neighbors/base.py<br>Base class for nearest neighbors estimators.", [
d("__init__(self)"),
d("_fit(self, X)"),
d("_init_params(self, n_neighbors, radius, algorithm, leaf_size, metric, p, metric_params)"),]),
c("NeighborsWarning(UserWarning)", "/neighbors/base.py<br>", [
]),
c("RadiusNeighborsMixin(object)", "/neighbors/base.py<br>Mixin for radius-based neighbors searches", [
d("radius_neighbors(self, X, radius, return_distance)"),
d("radius_neighbors_graph(self, X, radius, mode)"),]),
c("SupervisedFloatMixin(object)", "/neighbors/base.py<br>", [
d("fit(self, X, y)"),]),
c("SupervisedIntegerMixin(object)", "/neighbors/base.py<br>", [
d("fit(self, X, y)"),]),
c("UnsupervisedMixin(object)", "/neighbors/base.py<br>", [
d("fit(self, X, y)"),]),
d("_check_weights(weights)", "Check to make sure weights are valid"),
d("_get_weights(dist, weights)", "Get the weights from an array of distances and a parameter ``weights``  Parameters  dist: ndarray     The input distances weights: {'uniform', 'distance' or a callable}     The kind of weighting used  Returns  weights_arr: array of the same shape as ``dist``     if ``weights  'uniform'``, then retur..."),]),
c("classification.py", "/neighbors/classification.py<br> Nearest Neighbor Classification", [
c("KNeighborsClassifier(NeighborsBase, KNeighborsMixin, SupervisedIntegerMixin, ClassifierMixin)", "/neighbors/classification.py<br>Classifier implementing the k-nearest neighbors vote.  Read more in the :ref:`User Guide <classification>`.  Parameters ---------- n_neighbors : int, optional (default  5)     Number of neighbors to use by default for :meth:`k_neighbors` queries.  weights : str or callable     weight function used i...", [
d("__init__(self, n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),]),
c("RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin, SupervisedIntegerMixin, ClassifierMixin)", "/neighbors/classification.py<br>Classifier implementing a vote among neighbors within a given radius  Read more in the :ref:`User Guide <classification>`.  Parameters ---------- radius : float, optional (default  1.0)     Range of parameter space to use by default for :meth`radius_neighbors`     queries.  weights : str or callable...", [
d("__init__(self, radius, weights, algorithm, leaf_size, p, metric, outlier_label, metric_params)"),
d("predict(self, X)"),]),]),
c("graph.py", "/neighbors/graph.py<br> Nearest Neighbors graph functions", [
d("_check_params(X, metric, p, metric_params)", "Check the validity of the input parameters"),
d("_query_include_self(X, include_self, mode)", "Return the query based on include_self param"),
d("kneighbors_graph(X, n_neighbors, mode, metric, p, metric_params, include_self)", "Computes the (weighted) graph of k-Neighbors for points in X  Read more in the :ref:`User Guide <unsupervised_neighbors>`.  Parameters ---------- X : array-like or BallTree, shape  [n_samples, n_features]     Sample data, in the form of a numpy array or a precomputed     :class:`BallTree`.  n_neighb..."),
d("radius_neighbors_graph(X, radius, mode, metric, p, metric_params, include_self)", "Computes the (weighted) graph of Neighbors for points in X  Neighborhoods are restricted the points at a distance lower than radius.  Read more in the :ref:`User Guide <unsupervised_neighbors>`.  Parameters ---------- X : array-like or BallTree, shape  [n_samples, n_features]     Sample data, in the..."),]),
c("kde.py", "/neighbors/kde.py<br> Kernel Density Estimation -------------------------", [
c("KernelDensity(BaseEstimator)", "/neighbors/kde.py<br>Kernel Density Estimation  Read more in the :ref:`User Guide <kernel_density>`.  Parameters ---------- bandwidth : float     The bandwidth of the kernel.  algorithm : string     The tree algorithm to use.  Valid options are     ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'.  kernel : string    ...", [
d("__init__(self, bandwidth, algorithm, kernel, metric, atol, rtol, breadth_first, leaf_size, metric_params)"),
d("_choose_algorithm(self, algorithm, metric)"),
d("fit(self, X, y)"),
d("sample(self, n_samples, random_state)"),
d("score(self, X, y)"),
d("score_samples(self, X)"),]),]),
c("nearest_centroid.py", "/neighbors/nearest_centroid.py<br> Nearest Centroid Classification", [
c("NearestCentroid(BaseEstimator, ClassifierMixin)", "/neighbors/nearest_centroid.py<br>Nearest centroid classifier.  Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.  Read more in the :ref:`User Guide <nearest_centroid_classifier>`.  Parameters ---------- metric: string, or callable     The metric to use when calculating d...", [
d("__init__(self, metric, shrink_threshold)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),]),
c("regression.py", "/neighbors/regression.py<br> Nearest Neighbor Regression", [
c("KNeighborsRegressor(NeighborsBase, KNeighborsMixin, SupervisedFloatMixin, RegressorMixin)", "/neighbors/regression.py<br>Regression based on k-nearest neighbors.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.  Parameters ---------- n_neighbors : int, optional (default  5)     Number of neighbors t...", [
d("__init__(self, n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params)"),
d("predict(self, X)"),]),
c("RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin, SupervisedFloatMixin, RegressorMixin)", "/neighbors/regression.py<br>Regression based on neighbors within a fixed radius.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.  Parameters ---------- radius : float, optional (default  1.0)     Range of p...", [
d("__init__(self, radius, weights, algorithm, leaf_size, p, metric, metric_params)"),
d("predict(self, X)"),]),]),
c("setup.py", "/neighbors/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("unsupervised.py", "/neighbors/unsupervised.py<br> Unsupervised nearest neighbors learner", [
c("NearestNeighbors(NeighborsBase, KNeighborsMixin, RadiusNeighborsMixin, UnsupervisedMixin)", "/neighbors/unsupervised.py<br>Unsupervised learner for implementing neighbor searches.  Read more in the :ref:`User Guide <unsupervised_neighbors>`.  Parameters ---------- n_neighbors : int, optional (default  5)     Number of neighbors to use by default for :meth:`k_neighbors` queries.  radius : float, optional (default  1.0)  ...", [
d("__init__(self, n_neighbors, radius, algorithm, leaf_size, metric, p, metric_params)"),]),]),]),
c("neural_network", "/neural_network<br>", [
c("tests", "/neural_network/tests<br>", [
c("test_rbm.py", "/neural_network/tests/test_rbm.py<br>", [
d("test_fit()"),
d("test_fit_gibbs()"),
d("test_fit_gibbs_sparse()"),
d("test_gibbs_smoke()"),
d("test_partial_fit()"),
d("test_rbm_verbose()"),
d("test_sample_hiddens()"),
d("test_score_samples()"),
d("test_small_sparse()"),
d("test_small_sparse_partial_fit()"),
d("test_sparse_and_verbose()"),
d("test_transform()"),]),]),
c("rbm.py", "/neural_network/rbm.py<br> Restricted Boltzmann Machine", [
c("BernoulliRBM(BaseEstimator, TransformerMixin)", "/neural_network/rbm.py<br>Bernoulli Restricted Boltzmann Machine (RBM).  A Restricted Boltzmann Machine with binary visible units and binary hiddens. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2].  The time complexity of this implementation is ``...", [
d("__init__(self, n_components, learning_rate, batch_size, n_iter, verbose, random_state)"),
d("_fit(self, v_pos, rng)"),
d("_free_energy(self, v)"),
d("_mean_hiddens(self, v)"),
d("_sample_hiddens(self, v, rng)"),
d("_sample_visibles(self, h, rng)"),
d("fit(self, X, y)"),
d("gibbs(self, v)"),
d("partial_fit(self, X, y)"),
d("score_samples(self, X)"),
d("transform(self, X)"),]),]),]),
c("preprocessing", "/preprocessing<br>", [
c("tests", "/preprocessing/tests<br>", [
c("test_data.py", "/preprocessing/tests/test_data.py<br>", [
d("_check_one_hot(X, X2, cat, n_features)"),
d("_check_transform_selected(X, X_expected, sel)"),
d("_run_one_hot(X, X2, cat)"),
d("test_add_dummy_feature()"),
d("test_add_dummy_feature_coo()"),
d("test_add_dummy_feature_csc()"),
d("test_add_dummy_feature_csr()"),
d("test_binarizer()"),
d("test_center_kernel()"),
d("test_fit_transform()"),
d("test_maxabs_scaler_large_negative_value()", "Check MaxAbsScaler on toy data with a large negative value"),
d("test_maxabs_scaler_zero_variance_features()", "Check MaxAbsScaler on toy data with zero variance features"),
d("test_min_max_scaler_1d()"),
d("test_min_max_scaler_iris()"),
d("test_min_max_scaler_zero_variance_features()"),
d("test_minmax_scale_axis1()"),
d("test_normalize()"),
d("test_normalizer_l1()"),
d("test_normalizer_l2()"),
d("test_normalizer_max()"),
d("test_one_hot_encoder_categorical_features()"),
d("test_one_hot_encoder_dense()"),
d("test_one_hot_encoder_sparse()"),
d("test_one_hot_encoder_unknown_transform()"),
d("test_polynomial_features()"),
d("test_robust_scale_axis1()"),
d("test_robust_scaler_2d_arrays()", "Test robust scaling of 2d array along first axis"),
d("test_robust_scaler_iris()"),
d("test_robust_scaler_zero_variance_features()", "Check RobustScaler on toy data with zero variance features"),
d("test_scale_function_without_centering()"),
d("test_scale_input_finiteness_validation()"),
d("test_scale_sparse_with_mean_raise_exception()"),
d("test_scaler_1d()"),
d("test_scaler_2d_arrays()"),
d("test_scaler_int()"),
d("test_scaler_without_centering()"),
d("test_scaler_without_copy()"),
d("test_standard_scaler_numerical_stability()", "Test numerical stability of scaling"),
d("test_transform_selected()"),
d("test_warning_scaling_integers()"),
d("toarray(a)"),]),
c("test_function_transformer.py", "/preprocessing/tests/test_function_transformer.py<br>", [
d("_make_func(args_store, kwargs_store, func)"),
d("test_delegate_to_func()"),
d("test_np_log()"),]),
c("test_imputation.py", "/preprocessing/tests/test_imputation.py<br>", [
d("_check_statistics(X, X_true, strategy, statistics, missing_values)", "Utility function for testing imputation for a given strategy.  Test:     - along the two axes     - with dense and sparse arrays  Check that:     - the statistics (mean, median, mode) are correct     - the missing values are imputed correctly"),
d("test_imputation_copy()"),
d("test_imputation_mean_median()"),
d("test_imputation_mean_median_only_zero()"),
d("test_imputation_median_special_cases()"),
d("test_imputation_most_frequent()"),
d("test_imputation_pickle()"),
d("test_imputation_pipeline_grid_search()"),
d("test_imputation_shape()"),]),
c("test_label.py", "/preprocessing/tests/test_label.py<br>", [
d("check_binarized_results(y, classes, pos_label, neg_label, expected)"),
d("test_invalid_input_label_binarize()"),
d("test_inverse_binarize_multiclass()"),
d("test_label_binarize_binary()"),
d("test_label_binarize_multiclass()"),
d("test_label_binarize_multilabel()"),
d("test_label_binarize_with_class_order()"),
d("test_label_binarizer()"),
d("test_label_binarizer_errors()"),
d("test_label_binarizer_set_label_encoding()"),
d("test_label_binarizer_unseen_labels()"),
d("test_label_encoder()"),
d("test_label_encoder_errors()"),
d("test_label_encoder_fit_transform()"),
d("test_multilabel_binarizer()"),
d("test_multilabel_binarizer_empty_sample()"),
d("test_multilabel_binarizer_given_classes()"),
d("test_multilabel_binarizer_inverse_validation()"),
d("test_multilabel_binarizer_non_integer_labels()"),
d("test_multilabel_binarizer_non_unique()"),
d("test_multilabel_binarizer_same_length_sequence()"),
d("test_multilabel_binarizer_unknown_class()"),
d("test_sparse_output_multilabel_binarizer()"),
d("toarray(a)"),]),
c("test_weights.py", "/preprocessing/tests/test_weights.py<br>", [
d("test_balance_weights()"),]),]),
c("data.py", "/preprocessing/data.py<br>", [
c("Binarizer(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Binarize data (set feature values to 0 or 1) according to a threshold  Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.  Binarization is a common operation on text count data where t...", [
d("__init__(self, threshold, copy)"),
d("fit(self, X, y)"),
d("transform(self, X, y, copy)"),]),
c("KernelCenterer(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Center a kernel matrix  Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.S...", [
d("fit(self, K, y)"),
d("transform(self, K, y, copy)"),]),
c("MaxAbsScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Scale each feature by its maximum absolute value.  This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.  This scaler can also be ...", [
d("__init__(self, copy)"),
d("fit(self, X, y)"),
d("inverse_transform(self, X)"),
d("transform(self, X, y)"),]),
c("MinMaxScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Transforms features by scaling each feature to a given range.  This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.  The transformation is given by::      X_std  (X - X.min(axis0)) / (X.max(axis0) - X.min(ax...", [
d("__init__(self, feature_range, copy)"),
d("fit(self, X, y)"),
d("inverse_transform(self, X)"),
d("transform(self, X)"),]),
c("Normalizer(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Normalize samples individually to unit norm.  Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one.  This transformer is able to work both with dense numpy arrays and scipy.sparse matrix ...", [
d("__init__(self, norm, copy)"),
d("fit(self, X, y)"),
d("transform(self, X, y, copy)"),]),
c("OneHotEncoder(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Encode categorical integer features using a one-hot aka one-of-K scheme.  The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix where each column corresponds to one possible value of one featu...", [
d("__init__(self, n_values, categorical_features, dtype, sparse, handle_unknown)"),
d("_fit_transform(self, X)"),
d("_transform(self, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),]),
c("PolynomialFeatures(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Generate polynomial and interaction features.  Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features a...", [
d("__init__(self, degree, interaction_only, include_bias)"),
d("_combinations(n_features, degree, interaction_only, include_bias)"),
d("fit(self, X, y)"),
d("powers_(self)"),
d("transform(self, X, y)"),]),
c("RobustScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Scale features using statistics that are robust to outliers.  This Scaler removes the median and scales the data according to the Interquartile Range (IQR). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).  Centering and scaling happen independently...", [
d("__init__(self, with_centering, with_scaling, copy)"),
d("_check_array(self, X, copy)"),
d("fit(self, X, y)"),
d("inverse_transform(self, X)"),
d("transform(self, X, y)"),]),
c("StandardScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py<br>Standardize features by removing the mean and scaling to unit variance  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using the `transform` metho...", [
d("__init__(self, copy, with_mean, with_std)"),
d("fit(self, X, y)"),
d("inverse_transform(self, X, copy)"),
d("transform(self, X, y, copy)"),]),
d("_handle_zeros_in_scale(scale)", "Makes sure that whenever scale is zero, we handle it correctly.  This happens in most scalers when we have constant features."),
d("_mean_and_std(X, axis, with_mean, with_std)", "Compute mean and std deviation for centering, scaling.  Zero valued std components are reset to 1.0 to avoid NaNs when scaling."),
d("_transform_selected(X, transform, selected, copy)", "Apply a transform function to portion of selected features  Parameters ---------- X : array-like or sparse matrix, shape(n_samples, n_features)     Dense array or sparse matrix.  transform : callable     A callable transform(X) -> X_transformed  copy : boolean, optional     Copy X even if it could b..."),
d("add_dummy_feature(X, value)", "Augment dataset with an additional dummy feature.  This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.  Parameters ---------- X : array or scipy.sparse matrix with shape [n_samples, n_features]     Data.  value : float     Value to use for the du..."),
d("binarize(X, threshold, copy)", "Boolean thresholding of array-like or scipy.sparse matrix  Read more in the :ref:`User Guide <preprocessing_binarization>`.  Parameters ---------- X : array or scipy.sparse matrix with shape [n_samples, n_features]     The data to binarize, element by element.     scipy.sparse matrices should be in ..."),
d("maxabs_scale(X, axis, copy)", "Scale each feature to the [-1, 1] range without breaking the sparsity.  This estimator scales each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.  This scaler can also be applied to sparse CSR or CSC matrices.  Parameters ---------- axis : ..."),
d("minmax_scale(X, feature_range, axis, copy)", "Transforms features by scaling each feature to a given range.  This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.  The transformation is given by::      X_std  (X - X.min(axis0)) / (X.max(axis0) - X.min(ax..."),
d("normalize(X, norm, axis, copy)", "Scale input vectors individually to unit norm (vector length).  Read more in the :ref:`User Guide <preprocessing_normalization>`.  Parameters ---------- X : array or scipy.sparse matrix with shape [n_samples, n_features]     The data to normalize, element by element.     scipy.sparse matrices should..."),
d("robust_scale(X, axis, with_centering, with_scaling, copy)", "Standardize a dataset along any axis  Center to the median and component wise scale according to the interquartile range.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- X : array-like.     The data to center and scale.  axis : int (0 by default)     axis used to c..."),
d("scale(X, axis, with_mean, with_std, copy)", "Standardize a dataset along any axis  Center to the mean and component wise scale to unit variance.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- X : array-like or CSR matrix.     The data to center and scale.  axis : int (0 by default)     axis used to compute t..."),]),
c("imputation.py", "/preprocessing/imputation.py<br>", [
c("Imputer(BaseEstimator, TransformerMixin)", "/preprocessing/imputation.py<br>Imputation transformer for completing missing values.  Read more in the :ref:`User Guide <imputation>`.  Parameters ---------- missing_values : integer or 'NaN', optional (default'NaN')     The placeholder for the missing values. All occurrences of     `missing_values` will be imputed. For missing v...", [
d("__init__(self, missing_values, strategy, axis, verbose, copy)"),
d("_dense_fit(self, X, strategy, missing_values, axis)"),
d("_sparse_fit(self, X, strategy, missing_values, axis)"),
d("fit(self, X, y)"),
d("transform(self, X)"),]),
d("_get_mask(X, value_to_mask)", "Compute the boolean mask X  missing_values."),
d("_most_frequent(array, extra_value, n_repeat)", "Compute the most frequent value in a 1d array extended with [extra_value] * n_repeat, where extra_value is assumed to be not part of the array."),]),
c("label.py", "/preprocessing/label.py<br>", [
c("LabelBinarizer(BaseEstimator, TransformerMixin)", "/preprocessing/label.py<br>Binarize labels in a one-vs-all fashion  Several regression and binary classification algorithms are available in the scikit. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  At learning time, this simply consists in learning ...", [
d("__init__(self, neg_label, pos_label, sparse_output)"),
d("fit(self, y)"),
d("inverse_transform(self, Y, threshold)"),
d("transform(self, y)"),]),
c("LabelEncoder(BaseEstimator, TransformerMixin)", "/preprocessing/label.py<br>Encode labels with value between 0 and n_classes-1.  Read more in the :ref:`User Guide <preprocessing_targets>`.  Attributes ---------- classes_ : array of shape (n_class,)     Holds the label for each class.  Examples -------- `LabelEncoder` can be used to normalize labels.  >>> from sklearn import...", [
d("fit(self, y)"),
d("fit_transform(self, y)"),
d("inverse_transform(self, y)"),
d("transform(self, y)"),]),
c("MultiLabelBinarizer(BaseEstimator, TransformerMixin)", "/preprocessing/label.py<br>Transform between iterable of iterables and a multilabel format  Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matr...", [
d("__init__(self, classes, sparse_output)"),
d("_transform(self, y, class_mapping)"),
d("fit(self, y)"),
d("fit_transform(self, y)"),
d("inverse_transform(self, yt)"),
d("transform(self, y)"),]),
d("_check_numpy_unicode_bug(labels)", "Check that user is not subject to an old numpy bug  Fixed in master before 1.7.0:    https://github.com/numpy/numpy/pull/243"),
d("_inverse_binarize_multiclass(y, classes)", "Inverse label binarization transformation for multiclass.  Multiclass uses the maximal score instead of a threshold."),
d("_inverse_binarize_thresholding(y, output_type, classes, threshold)", "Inverse label binarization transformation using thresholding."),
d("label_binarize(y, classes, neg_label, pos_label, sparse_output)", "Binarize labels in a one-vs-all fashion  Several regression and binary classification algorithms are available in the scikit. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  This function makes it possible to compute this tra..."),]),
c("_function_transformer.py", "/preprocessing/_function_transformer.py<br>", [
c("FunctionTransformer(BaseEstimator, TransformerMixin)", "/preprocessing/_function_transformer.py<br>Constructs a transformer from an arbitrary callable.  A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. This is useful for stateless transformations such as taking the log of frequencies, doing cus...", [
d("__init__(self, func, validate, accept_sparse, pass_y)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
d("_identity(X)", "The identity function.     "),]),
c("_weights.py", "/preprocessing/_weights.py<br>", [
d("_balance_weights(y)", "Compute sample weights such that the class distribution of y becomes    balanced.  Parameters ---------- y : array-like     Labels for the samples.  Returns ------- weights : array-like     The sample weights."),]),]),
c("semi_supervised", "/semi_supervised<br>", [
c("tests", "/semi_supervised/tests<br>", [
c("test_label_propagation.py", "/semi_supervised/tests/test_label_propagation.py<br> test the label propagation module ", [
d("test_distribution()"),
d("test_fit_transduction()"),
d("test_predict()"),
d("test_predict_proba()"),]),]),
c("label_propagation.py", "/semi_supervised/label_propagation.py<br> Label propagation in the context of this module refers to a set of semisupervised classification algorithms. In the high level, these algorithms work by forming a fully-connected graph between all points given and solving for the steady-state distribution of labels at each point.  These algorithms p...", [
c("BaseLabelPropagation()", "/semi_supervised/label_propagation.py<br>Base class for label propagation module.  Parameters ---------- kernel : {'knn', 'rbf'}     String identifier for kernel function to use.     Only 'rbf' and 'knn' kernels are currently supported..  gamma : float     Parameter for rbf kernel  alpha : float     Clamping factor  max_iter : float     Ch...", [
d("__init__(self, kernel, gamma, n_neighbors, alpha, max_iter, tol)"),
d("_build_graph(self)"),
d("_get_kernel(self, X, y)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),]),
c("LabelPropagation(BaseLabelPropagation)", "/semi_supervised/label_propagation.py<br>Label Propagation classifier  Read more in the :ref:`User Guide <label_propagation>`.  Parameters ---------- kernel : {'knn', 'rbf'}     String identifier for kernel function to use.     Only 'rbf' and 'knn' kernels are currently supported..  gamma : float     Parameter for rbf kernel  n_neighbors :...", [
d("_build_graph(self)"),]),
c("LabelSpreading(BaseLabelPropagation)", "/semi_supervised/label_propagation.py<br>LabelSpreading model for semi-supervised learning  This model is similar to the basic Label Propgation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.  Read more in the :ref:`User Guide <label_propagation>`.  Parameters ---------- kern...", [
d("__init__(self, kernel, gamma, n_neighbors, alpha, max_iter, tol)"),
d("_build_graph(self)"),]),
d("_not_converged(y_truth, y_prediction, tol)", "basic convergence check"),]),]),
c("svm", "/svm<br>", [
c("tests", "/svm/tests<br>", [
c("test_bounds.py", "/svm/tests/test_bounds.py<br>", [
d("check_l1_min_c(X, y, loss, fit_intercept, intercept_scaling)"),
d("test_ill_posed_min_c()"),
d("test_l1_min_c()"),
d("test_l2_deprecation()"),
d("test_unsupported_loss()"),]),
c("test_sparse.py", "/svm/tests/test_sparse.py<br>", [
d("check_svm_model_equal(dense_svm, sparse_svm, X_train, y_train, X_test)"),
d("test_consistent_proba()"),
d("test_error()"),
d("test_linearsvc()"),
d("test_linearsvc_iris()"),
d("test_sample_weights()"),
d("test_sparse_decision_function()"),
d("test_sparse_liblinear_intercept_handling()"),
d("test_sparse_realdata()"),
d("test_sparse_svc_clone_with_callable_kernel()"),
d("test_svc()", "Check that sparse SVC gives the same result as SVC"),
d("test_svc_iris()"),
d("test_svc_with_custom_kernel()"),
d("test_timeout()"),
d("test_unsorted_indices()"),
d("test_weight()"),]),
c("test_svm.py", "/svm/tests/test_svm.py<br> Testing for Support Vector Machine module (sklearn.svm)  TODO: remove hard coded numerical results when possible", [
d("test_auto_weight()"),
d("test_bad_input()"),
d("test_consistent_proba()"),
d("test_crammer_singer_binary()"),
d("test_decision_function()"),
d("test_decision_function_shape()"),
d("test_dense_liblinear_intercept_handling(classifier)"),
d("test_hasattr_predict_proba()"),
d("test_immutable_coef_property()"),
d("test_liblinear_set_coef()"),
d("test_libsvm_iris()"),
d("test_libsvm_parameters()"),
d("test_linear_svc_convergence_warnings()"),
d("test_linear_svc_intercept_scaling()"),
d("test_linear_svx_uppercase_loss_penalty()"),
d("test_linearsvc()"),
d("test_linearsvc_crammer_singer()"),
d("test_linearsvc_iris()"),
d("test_linearsvc_parameters()"),
d("test_linearsvc_verbose()"),
d("test_linearsvr()"),
d("test_linearsvx_loss_penalty_deprecations()"),
d("test_lsvc_intercept_scaling_zero()"),
d("test_oneclass()"),
d("test_oneclass_decision_function()"),
d("test_precomputed()"),
d("test_probability()"),
d("test_sample_weights()"),
d("test_single_sample_1d()"),
d("test_sparse_precomputed()"),
d("test_svc_bad_kernel()"),
d("test_svc_clone_with_callable_kernel()"),
d("test_svr()"),
d("test_svr_coef_sign()"),
d("test_svr_decision_function()"),
d("test_svr_errors()"),
d("test_timeout()"),
d("test_tweak_params()"),
d("test_unfitted()"),
d("test_weight()"),]),]),
c("base.py", "/svm/base.py<br>", [
c("BaseLibSVM()", "/svm/base.py<br>Base class for estimators that use libsvm as backing library  This implements support vector machine classification and regression.  Parameter documentation is in the derived `SVC` class.", [
d("__init__(self, impl, kernel, degree, gamma, coef0, tol, C, nu, epsilon, shrinking, probability, cache_size, class_weight, verbose, max_iter, random_state)"),
d("_compute_kernel(self, X)"),
d("_decision_function(self, X)"),
d("_dense_decision_function(self, X)"),
d("_dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed)"),
d("_dense_predict(self, X)"),
d("_get_coef(self)"),
d("_pairwise(self)"),
d("_sparse_decision_function(self, X)"),
d("_sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed)"),
d("_sparse_predict(self, X)"),
d("_validate_for_predict(self, X)"),
d("_validate_targets(self, y)"),
d("_warn_from_fit_status(self)"),
d("coef_(self)"),
d("decision_function(self, X)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("BaseSVC()", "/svm/base.py<br>ABC for LibSVM-based classifiers.", [
d("__init__(self, impl, kernel, degree, gamma, coef0, tol, C, nu, shrinking, probability, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state)"),
d("_check_proba(self)"),
d("_dense_predict_proba(self, X)"),
d("_get_coef(self)"),
d("_predict_log_proba(self, X)"),
d("_predict_proba(self, X)"),
d("_sparse_predict_proba(self, X)"),
d("_validate_targets(self, y)"),
d("decision_function(self, X)"),
d("predict(self, X)"),
d("predict_log_proba(self)"),
d("predict_proba(self)"),]),
d("_fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)", "Used by Logistic Regression (and CV) and LinearSVC.  Preprocessing is done in this function before supplying it to liblinear.  Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features)     Training vector, where n_samples in the number of samples and     n_features is the ..."),
d("_get_liblinear_solver_type(multi_class, penalty, loss, dual)", "Find the liblinear magic number for the solver.  This number depends on the values of the following attributes:   - multi_class   - penalty   - loss   - dual  The same number is also internally used by LibLinear to determine which solver to use."),
d("_one_vs_one_coef(dual_coef, n_support, support_vectors)", "Generate primal coefficients from dual coefficients for the one-vs-one multi class LibSVM in the case of a linear kernel."),]),
c("bounds.py", "/svm/bounds.py<br> Determination of parameter bounds", [
d("l1_min_c(X, y, loss, fit_intercept, intercept_scaling)", "Return the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty. This applies to l1 penalized classifiers, such as LinearSVC with penalty'l1' and linear_model.LogisticRegression with penalty'l1'.  This value is valid if class_weight parameter in fit() is..."),]),
c("classes.py", "/svm/classes.py<br>", [
c("LinearSVC(BaseEstimator, LinearClassifierMixin, _LearntSelectorMixin, SparseCoefMixin)", "/svm/classes.py<br>Linear Support Vector Classification.  Similar to SVC with parameter kernel'linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  This class supports both dense...", [
d("__init__(self, penalty, loss, dual, tol, C, multi_class, fit_intercept, intercept_scaling, class_weight, verbose, random_state, max_iter)"),
d("fit(self, X, y)"),]),
c("LinearSVR(LinearModel, RegressorMixin)", "/svm/classes.py<br>Linear Support Vector Regression.  Similar to SVR with parameter kernel'linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  This class supports both dense and...", [
d("__init__(self, epsilon, tol, C, loss, fit_intercept, intercept_scaling, dual, verbose, random_state, max_iter)"),
d("fit(self, X, y)"),]),
c("NuSVC(BaseSVC)", "/svm/classes.py<br>Nu-Support Vector Classification.  Similar to SVC but uses a parameter to control the number of support vectors.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- nu : float, optional (default0.5)     An upper bound on the fracti...", [
d("__init__(self, nu, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state)"),]),
c("NuSVR(BaseLibSVM, RegressorMixin)", "/svm/classes.py<br>Nu Support Vector Regression.  Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.  The implementation is based on libsvm.  Read more in the :ref:`User Guide...", [
d("__init__(self, nu, C, kernel, degree, gamma, coef0, shrinking, tol, cache_size, verbose, max_iter)"),]),
c("OneClassSVM(BaseLibSVM)", "/svm/classes.py<br>Unsupervised Outlier Detection.  Estimate the support of a high-dimensional distribution.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_outlier_detection>`.  Parameters ---------- kernel : string, optional (default'rbf')      Specifies the kernel type to be used in ...", [
d("__init__(self, kernel, degree, gamma, coef0, tol, nu, shrinking, cache_size, verbose, max_iter, random_state)"),
d("decision_function(self, X)"),
d("fit(self, X, y, sample_weight)"),]),
c("SVC(BaseSVC)", "/svm/classes.py<br>C-Support Vector Classification.  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.  The multiclass support is handled according to a one-vs-one scheme.  F...", [
d("__init__(self, C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state)"),]),
c("SVR(BaseLibSVM, RegressorMixin)", "/svm/classes.py<br>Epsilon-Support Vector Regression.  The free parameters in the model are C and epsilon.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_regression>`.  Parameters ---------- C : float, optional (default1.0)     Penalty parameter C of the error term.  epsilon : float, o...", [
d("__init__(self, kernel, degree, gamma, coef0, tol, C, epsilon, shrinking, cache_size, verbose, max_iter)"),]),]),
c("setup.py", "/svm/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),]),
c("tests", "/tests<br>", [
c("test_base.py", "/tests/test_base.py<br>", [
c("Buggy(BaseEstimator)", "/tests/test_base.py<br>A buggy estimator that does not set its parameters right. ", [
d("__init__(self, a)"),]),
c("DeprecatedAttributeEstimator(BaseEstimator)", "/tests/test_base.py<br>", [
d("__init__(self, a, b)"),
d("b(self)"),]),
c("K(BaseEstimator)", "/tests/test_base.py<br>", [
d("__init__(self, c, d)"),]),
c("MyEstimator(BaseEstimator)", "/tests/test_base.py<br>", [
d("__init__(self, l1, empty)"),]),
c("NoEstimator(object)", "/tests/test_base.py<br>", [
d("__init__(self)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("T(BaseEstimator)", "/tests/test_base.py<br>", [
d("__init__(self, a, b)"),]),
c("VargEstimator(BaseEstimator)", "/tests/test_base.py<br>Sklearn estimators shouldn't have vargs.", [
d("__init__(self)"),]),
d("test_clone()"),
d("test_clone_2()"),
d("test_clone_buggy()"),
d("test_clone_empty_array()"),
d("test_clone_nan()"),
d("test_get_params()"),
d("test_get_params_deprecated()"),
d("test_is_classifier()"),
d("test_repr()"),
d("test_score_sample_weight()"),
d("test_set_params()"),
d("test_str()"),]),
c("test_calibration.py", "/tests/test_calibration.py<br>", [
d("test_calibration()", "Test calibration objects with isotonic and sigmoid"),
d("test_calibration_curve()", "Check calibration_curve function"),
d("test_calibration_multiclass()", "Test calibration for multiclass "),
d("test_calibration_nan_imputer()", "Test that calibration can accept nan"),
d("test_calibration_prefit()", "Test calibration for prefitted classifiers"),
d("test_sample_weight_warning()"),
d("test_sigmoid_calibration()", "Test calibration values with Platt sigmoid model"),]),
c("test_check_build.py", "/tests/test_check_build.py<br> Smoke Test the check_build module", [
d("test_raise_build_error()"),]),
c("test_common.py", "/tests/test_common.py<br> General tests for all estimators in sklearn.", [
d("test_all_estimator_no_base_class()"),
d("test_all_estimators()"),
d("test_class_weight_balanced_linear_classifiers()"),
d("test_configure()"),
d("test_get_params_invariance()"),
d("test_import_all_consistency()"),
d("test_non_meta_estimators()"),
d("test_non_transformer_estimators_n_iter()"),
d("test_root_import_all_completeness()"),
d("test_transformer_n_iter()"),]),
c("test_cross_validation.py", "/tests/test_cross_validation.py<br> Test the cross_validation module", [
c("MockClassifier(object)", "/tests/test_cross_validation.py<br>Dummy classifier to test the cross-validation", [
d("__init__(self, a, allow_nd)"),
d("fit(self, X, Y, sample_weight, class_prior, sparse_sample_weight, sparse_param, dummy_int, dummy_str, dummy_obj, callback)"),
d("get_params(self, deep)"),
d("predict(self, T)"),
d("score(self, X, Y)"),]),
d("check_cv_coverage(cv, expected_n_iter, n_samples)"),
d("check_valid_split(train, test, n_samples)"),
d("test_check_cv_return_types()"),
d("test_check_is_partition()"),
d("test_cross_val_generator_with_default_indices()"),
d("test_cross_val_generator_with_indices()"),
d("test_cross_val_predict()"),
d("test_cross_val_predict_input_types()"),
d("test_cross_val_predict_pandas()"),
d("test_cross_val_score()"),
d("test_cross_val_score_allow_nans()"),
d("test_cross_val_score_errors()"),
d("test_cross_val_score_fit_params()"),
d("test_cross_val_score_mask()"),
d("test_cross_val_score_multilabel()"),
d("test_cross_val_score_pandas()"),
d("test_cross_val_score_precomputed()"),
d("test_cross_val_score_score_func()"),
d("test_cross_val_score_with_score_func_classification()"),
d("test_cross_val_score_with_score_func_regression()"),
d("test_kfold_balance()"),
d("test_kfold_can_detect_dependent_samples_on_digits()"),
d("test_kfold_indices()"),
d("test_kfold_no_shuffle()"),
d("test_kfold_valueerrors()"),
d("test_leave_label_out_changing_labels()"),
d("test_permutation_score()"),
d("test_permutation_test_score_allow_nans()"),
d("test_predefinedsplit_with_kfold_split()"),
d("test_safe_split_with_precomputed_kernel()"),
d("test_shuffle_kfold()"),
d("test_shuffle_split()"),
d("test_shuffle_stratifiedkfold()"),
d("test_shufflesplit_errors()"),
d("test_shufflesplit_reproducible()"),
d("test_sparse_fit_params()"),
d("test_stratified_kfold_no_shuffle()"),
d("test_stratified_kfold_ratios()"),
d("test_stratified_shuffle_split_even()"),
d("test_stratified_shuffle_split_init()"),
d("test_stratified_shuffle_split_iter()"),
d("test_stratifiedkfold_balance()"),
d("test_train_test_split()"),
d("test_train_test_split_allow_nans()"),
d("test_train_test_split_errors()"),
d("train_test_split_mock_pandas()"),
d("train_test_split_pandas()"),]),
c("test_dummy.py", "/tests/test_dummy.py<br>", [
d("_check_behavior_2d(clf)"),
d("_check_behavior_2d_for_constant(clf)"),
d("_check_equality_regressor(statistic, y_learn, y_pred_learn, y_test, y_pred_test)"),
d("_check_predict_proba(clf, X, y)"),
d("test_classification_sample_weight()"),
d("test_classifier_exceptions()"),
d("test_constant_size_multioutput_regressor()"),
d("test_constant_strategy()"),
d("test_constant_strategy_exceptions()"),
d("test_constant_strategy_multioutput()"),
d("test_constant_strategy_multioutput_regressor()"),
d("test_constant_strategy_regressor()"),
d("test_constant_strategy_sparse_target()"),
d("test_constants_not_specified_regressor()"),
d("test_dummy_regressor_sample_weight(n_samples)"),
d("test_mean_strategy_multioutput_regressor()"),
d("test_mean_strategy_regressor()"),
d("test_median_strategy_multioutput_regressor()"),
d("test_median_strategy_regressor()"),
d("test_most_frequent_and_prior_strategy()"),
d("test_most_frequent_and_prior_strategy_multioutput()"),
d("test_most_frequent_and_prior_strategy_sparse_target()"),
d("test_quantile_invalid()"),
d("test_quantile_strategy_empty_train()"),
d("test_quantile_strategy_multioutput_regressor()"),
d("test_quantile_strategy_regressor()"),
d("test_regressor_exceptions()"),
d("test_stratified_strategy()"),
d("test_stratified_strategy_multioutput()"),
d("test_stratified_strategy_sparse_target()"),
d("test_string_labels()"),
d("test_uniform_strategy()"),
d("test_uniform_strategy_multioutput()"),
d("test_uniform_strategy_sparse_target_warning()"),
d("test_unknown_strategey_regressor()"),
d("test_y_mean_attribute_regressor()"),]),
c("test_grid_search.py", "/tests/test_grid_search.py<br> Testing for grid search module (sklearn.grid_search)", [
c("BrokenClassifier(BaseEstimator)", "/tests/test_grid_search.py<br>Broken classifier that cannot be fit twice", [
d("__init__(self, parameter)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("FailingClassifier(BaseEstimator)", "/tests/test_grid_search.py<br>Classifier that raises a ValueError on fit()", [
d("__init__(self, parameter)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("LinearSVCNoScore(LinearSVC)", "/tests/test_grid_search.py<br>An LinearSVC classifier that has no score method.", [
d("score(self)"),]),
c("MockClassifier(object)", "/tests/test_grid_search.py<br>Dummy classifier to test the cross-validation", [
d("__init__(self, foo_param)"),
d("fit(self, X, Y)"),
d("get_params(self, deep)"),
d("predict(self, T)"),
d("score(self, X, Y)"),
d("set_params(self)"),]),
d("assert_grid_iter_equals_getitem(grid)"),
d("test_X_as_list()"),
d("test_grid_search()"),
d("test_grid_search_allows_nans()"),
d("test_grid_search_bad_param_grid()"),
d("test_grid_search_error()"),
d("test_grid_search_failing_classifier()"),
d("test_grid_search_failing_classifier_raise()"),
d("test_grid_search_iid()"),
d("test_grid_search_no_score()"),
d("test_grid_search_one_grid_point()"),
d("test_grid_search_precomputed_kernel()"),
d("test_grid_search_precomputed_kernel_error_kernel_function()"),
d("test_grid_search_precomputed_kernel_error_nonsquare()"),
d("test_grid_search_score_consistency()"),
d("test_grid_search_score_method()"),
d("test_grid_search_sparse()"),
d("test_grid_search_sparse_scoring()"),
d("test_grid_search_with_multioutput_data()"),
d("test_gridsearch_nd()"),
d("test_gridsearch_no_predict()"),
d("test_no_refit()"),
d("test_pandas_input()"),
d("test_param_sampler()"),
d("test_parameter_grid()"),
d("test_parameters_sampler_replacement()"),
d("test_pickle()"),
d("test_predict_proba_disabled()"),
d("test_randomized_search_grid_scores()"),
d("test_refit()"),
d("test_trivial_grid_scores()"),
d("test_unsupervised_grid_search()"),
d("test_y_as_list()"),]),
c("test_init.py", "/tests/test_init.py<br>", [
d("test_import_skl()"),]),
c("test_isotonic.py", "/tests/test_isotonic.py<br>", [
d("test_assert_raises_exceptions()"),
d("test_check_ci_warn()"),
d("test_check_increasing_down()"),
d("test_check_increasing_down_extreme()"),
d("test_check_increasing_up()"),
d("test_check_increasing_up_extreme()"),
d("test_isotonic_duplicate_min_entry()"),
d("test_isotonic_min_max_boundaries()"),
d("test_isotonic_regression()"),
d("test_isotonic_regression_auto_decreasing()"),
d("test_isotonic_regression_auto_increasing()"),
d("test_isotonic_regression_oob_bad()"),
d("test_isotonic_regression_oob_bad_after()"),
d("test_isotonic_regression_oob_clip()"),
d("test_isotonic_regression_oob_nan()"),
d("test_isotonic_regression_oob_raise()"),
d("test_isotonic_regression_pickle()"),
d("test_isotonic_regression_reversed()"),
d("test_isotonic_regression_ties_max()"),
d("test_isotonic_regression_ties_min()"),
d("test_isotonic_regression_ties_secondary_()", "Test isotonic regression fit, transform  and fit_transform against the 'secondary' ties method and 'pituitary' data from R  'isotone' package, as detailed in: J. d. Leeuw, K. Hornik, P. Mair,  Isotone Optimization in R: Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods  Set values base..."),
d("test_isotonic_sample_weight()"),
d("test_isotonic_sample_weight_parameter_default_value()"),
d("test_isotonic_zero_weight_loop()"),
d("test_permutation_invariance()"),]),
c("test_kernel_approximation.py", "/tests/test_kernel_approximation.py<br>", [
d("test_additive_chi2_sampler()"),
d("test_input_validation()"),
d("test_nystroem_approximation()"),
d("test_nystroem_callable()"),
d("test_nystroem_poly_kernel_params()"),
d("test_nystroem_singular_kernel()"),
d("test_rbf_sampler()"),
d("test_skewed_chi2_sampler()"),]),
c("test_kernel_ridge.py", "/tests/test_kernel_ridge.py<br>", [
d("test_kernel_ridge()"),
d("test_kernel_ridge_csc()"),
d("test_kernel_ridge_csr()"),
d("test_kernel_ridge_multi_output()"),
d("test_kernel_ridge_precomputed()"),
d("test_kernel_ridge_precomputed_kernel_unchanged()"),
d("test_kernel_ridge_sample_weights()"),
d("test_kernel_ridge_singular_kernel()"),]),
c("test_lda.py", "/tests/test_lda.py<br>", [
d("test_covariance()"),
d("test_lda_coefs()"),
d("test_lda_orthogonality()"),
d("test_lda_predict()"),
d("test_lda_scaling()"),
d("test_lda_transform()"),]),
c("test_learning_curve.py", "/tests/test_learning_curve.py<br>", [
c("MockEstimatorWithParameter(BaseEstimator)", "/tests/test_learning_curve.py<br>Dummy classifier to test the validation curve", [
d("__init__(self, param)"),
d("_is_training_data(self, X)"),
d("fit(self, X_subset, y_subset)"),
d("predict(self, X)"),
d("score(self, X, y)"),]),
c("MockImprovingEstimator(BaseEstimator)", "/tests/test_learning_curve.py<br>Dummy classifier to test the learning curve", [
d("__init__(self, n_max_train_sizes)"),
d("_is_training_data(self, X)"),
d("fit(self, X_subset, y_subset)"),
d("predict(self, X)"),
d("score(self, X, Y)"),]),
c("MockIncrementalImprovingEstimator(MockImprovingEstimator)", "/tests/test_learning_curve.py<br>Dummy classifier that provides partial_fit", [
d("__init__(self, n_max_train_sizes)"),
d("_is_training_data(self, X)"),
d("partial_fit(self, X, y)"),]),
d("test_learning_curve()"),
d("test_learning_curve_batch_and_incremental_learning_are_equal()"),
d("test_learning_curve_incremental_learning()"),
d("test_learning_curve_incremental_learning_not_possible()"),
d("test_learning_curve_incremental_learning_unsupervised()"),
d("test_learning_curve_n_sample_range_out_of_bounds()"),
d("test_learning_curve_remove_duplicate_sample_sizes()"),
d("test_learning_curve_unsupervised()"),
d("test_learning_curve_verbose()"),
d("test_learning_curve_with_boolean_indices()"),
d("test_validation_curve()"),]),
c("test_metaestimators.py", "/tests/test_metaestimators.py<br> Common tests for metaestimators", [
c("DelegatorData(object)", "/tests/test_metaestimators.py<br>", [
d("__init__(self, name, construct, skip_methods, fit_args)"),]),
d("test_metaestimator_delegation()"),]),
c("test_multiclass.py", "/tests/test_multiclass.py<br>", [
d("test_deprecated()"),
d("test_ecoc_exceptions()"),
d("test_ecoc_fit_predict()"),
d("test_ecoc_gridsearch()"),
d("test_ovo_decision_function()"),
d("test_ovo_exceptions()"),
d("test_ovo_fit_on_list()"),
d("test_ovo_fit_predict()"),
d("test_ovo_gridsearch()"),
d("test_ovo_string_y()"),
d("test_ovo_ties()"),
d("test_ovo_ties2()"),
d("test_ovr_always_present()"),
d("test_ovr_binary()"),
d("test_ovr_coef_()"),
d("test_ovr_coef_exceptions()"),
d("test_ovr_exceptions()"),
d("test_ovr_fit_predict()"),
d("test_ovr_fit_predict_sparse()"),
d("test_ovr_fit_predict_svc()"),
d("test_ovr_gridsearch()"),
d("test_ovr_multiclass()"),
d("test_ovr_multilabel()"),
d("test_ovr_multilabel_dataset()"),
d("test_ovr_multilabel_decision_function()"),
d("test_ovr_multilabel_predict_proba()"),
d("test_ovr_ovo_regressor()"),
d("test_ovr_pipeline()"),
d("test_ovr_single_label_decision_function()"),
d("test_ovr_single_label_predict_proba()"),]),
c("test_naive_bayes.py", "/tests/test_naive_bayes.py<br>", [
d("check_partial_fit(cls)"),
d("check_sample_weight_multiclass(cls)"),
d("test_bnb()"),
d("test_check_accuracy_on_digits()"),
d("test_coef_intercept_shape()"),
d("test_discrete_prior()"),
d("test_discretenb_partial_fit()"),
d("test_discretenb_pickle()"),
d("test_discretenb_predict_proba()"),
d("test_discretenb_provide_prior()"),
d("test_discretenb_provide_prior_with_partial_fit()"),
d("test_discretenb_uniform_prior()"),
d("test_feature_log_prob_bnb()"),
d("test_gnb()"),
d("test_gnb_partial_fit()"),
d("test_gnb_prior()"),
d("test_gnb_sample_weight()", "Test whether sample weights are properly used in GNB. "),
d("test_input_check_fit()"),
d("test_input_check_partial_fit()"),
d("test_mnnb()"),
d("test_sample_weight_mnb()"),
d("test_sample_weight_multiclass()"),]),
c("test_pipeline.py", "/tests/test_pipeline.py<br> Test the pipeline module.", [
c("FitParamT(object)", "/tests/test_pipeline.py<br>Mock classifier     ", [
d("__init__(self)"),
d("fit(self, X, y, should_succeed)"),
d("predict(self, X)"),]),
c("IncorrectT(object)", "/tests/test_pipeline.py<br>Small class to test parameter dispatching.     ", [
d("__init__(self, a, b)"),]),
c("T(IncorrectT)", "/tests/test_pipeline.py<br>", [
d("fit(self, X, y)"),
d("get_params(self, deep)"),
d("set_params(self)"),]),
c("TransfT(T)", "/tests/test_pipeline.py<br>", [
d("transform(self, X, y)"),]),
d("test_classes_property()"),
d("test_feature_union()"),
d("test_feature_union_feature_names()"),
d("test_feature_union_parallel()"),
d("test_feature_union_weights()"),
d("test_fit_predict_on_pipeline()"),
d("test_fit_predict_on_pipeline_without_fit_predict()"),
d("test_make_pipeline()"),
d("test_make_union()"),
d("test_pipeline_fit_params()"),
d("test_pipeline_fit_transform()"),
d("test_pipeline_init()"),
d("test_pipeline_methods_anova()"),
d("test_pipeline_methods_pca_svm()"),
d("test_pipeline_methods_preprocessing_svm()"),
d("test_pipeline_raise_set_params_error()"),
d("test_pipeline_transform()"),]),
c("test_qda.py", "/tests/test_qda.py<br>", [
d("test_qda()"),
d("test_qda_priors()"),
d("test_qda_regularization()"),
d("test_qda_store_covariances()"),]),
c("test_random_projection.py", "/tests/test_random_projection.py<br>", [
d("check_input_size_random_matrix(random_matrix)"),
d("check_input_with_sparse_random_matrix(random_matrix)"),
d("check_size_generated(random_matrix)"),
d("check_zero_mean_and_unit_norm(random_matrix)"),
d("densify(matrix)"),
d("make_sparse_random_data(n_samples, n_features, n_nonzeros)"),
d("test_SparseRandomProjection_output_representation()"),
d("test_basic_property_of_random_matrix()"),
d("test_correct_RandomProjection_dimensions_embedding()"),
d("test_gaussian_random_matrix()"),
d("test_input_size_jl_min_dim()"),
d("test_invalid_jl_domain()"),
d("test_random_projection_embedding_quality()"),
d("test_random_projection_transformer_invalid_input()"),
d("test_sparse_random_matrix()"),
d("test_sparse_random_projection_transformer_invalid_density()"),
d("test_too_many_samples_to_find_a_safe_embedding()"),
d("test_try_to_transform_before_fit()"),
d("test_warning_n_components_greater_than_n_features()"),
d("test_works_with_sparse_data()"),]),]),
c("tree", "/tree<br>", [
c("tests", "/tree/tests<br>", [
c("test_export.py", "/tree/tests/test_export.py<br> Testing for export functions of decision trees (sklearn.tree.export).", [
d("test_friedman_mse_in_graphviz()"),
d("test_graphviz_errors()"),
d("test_graphviz_toy()"),]),
c("test_tree.py", "/tree/tests/test_tree.py<br> Testing for the tree module (sklearn.tree).", [
d("_check_min_weight_leaf_split_level(TreeEstimator, X, y, sample_weight)"),
d("assert_tree_equal(d, s, message)"),
d("check_class_weight_errors(name)"),
d("check_class_weights(name)", "Check class_weights resemble sample_weights behavior."),
d("check_explicit_sparse_zeros(tree, max_depth, n_features)"),
d("check_min_weight_fraction_leaf(name, datasets, sparse)", "Test if leaves contain at least min_weight_fraction_leaf of the training set"),
d("check_min_weight_leaf_split_level(name)"),
d("check_public_apply(name)"),
d("check_public_apply_sparse(name)"),
d("check_raise_error_on_1d_input(name)"),
d("check_sparse_criterion(tree, dataset)"),
d("check_sparse_input(tree, dataset, max_depth)"),
d("check_sparse_parameters(tree, dataset)"),
d("test_1d_input()"),
d("test_arrayrepr()"),
d("test_arrays_persist()"),
d("test_big_input()"),
d("test_boston()"),
d("test_class_weight_errors()"),
d("test_class_weights()"),
d("test_classes_shape()"),
d("test_classification_toy()"),
d("test_error()"),
d("test_explicit_sparse_zeros()"),
d("test_huge_allocations()"),
d("test_importances()"),
d("test_importances_gini_equal_mse()"),
d("test_importances_raises()"),
d("test_iris()"),
d("test_max_features()"),
d("test_max_leaf_nodes()"),
d("test_max_leaf_nodes_max_depth()"),
d("test_memory_layout()"),
d("test_min_samples_leaf()"),
d("test_min_weight_fraction_leaf()"),
d("test_min_weight_leaf_split_level()"),
d("test_multioutput()"),
d("test_numerical_stability()"),
d("test_only_constant_features()"),
d("test_pickle()"),
d("test_probability()"),
d("test_public_apply()"),
d("test_pure_set()"),
d("test_realloc()"),
d("test_regression_toy()"),
d("test_sample_weight()"),
d("test_sample_weight_invalid()"),
d("test_sparse_criterion()"),
d("test_sparse_input()"),
d("test_sparse_parameters()"),
d("test_unbalanced_iris()"),
d("test_weighted_classification_toy()"),
d("test_with_only_one_non_constant_features()"),
d("test_xor()"),]),]),
c("export.py", "/tree/export.py<br> This module defines export functions for decision trees.", [
d("_color_brew(n)", "Generate n colors with equally spaced hues.  Parameters ---------- n : int     The number of colors required.  Returns ------- color_list : list, length n     List of n tuples of form (R, G, B) being the components of each color."),
d("export_graphviz(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters)", "Export a decision tree in DOT format.  This function generates a GraphViz representation of the decision tree, which is then written into `out_file`. Once exported, graphical renderings can be generated using, for example::      $ dot -Tps tree.dot -o tree.ps      (PostScript format)     $ dot -Tpng..."),]),
c("setup.py", "/tree/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("tree.py", "/tree/tree.py<br> This module gathers tree-based methods, including decision, regression and randomized trees. Single and multi-output problems are both handled.", [
c("BaseDecisionTree()", "/tree/tree.py<br>Base class for decision trees.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, random_state, class_weight)"),
d("_validate_X_predict(self, X, check_input)"),
d("apply(self, X, check_input)"),
d("feature_importances_(self)"),
d("fit(self, X, y, sample_weight, check_input)"),
d("predict(self, X, check_input)"),]),
c("DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin)", "/tree/tree.py<br>A decision tree classifier.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : string, optional (default'gini')     The function to measure the quality of a split. Supported criteria are     'gini' for the Gini impurity and 'entropy' for the information gain.  splitter : ...", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, class_weight)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X, check_input)"),]),
c("DecisionTreeRegressor(BaseDecisionTree, RegressorMixin)", "/tree/tree.py<br>A decision tree regressor.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : string, optional (default'mse')     The function to measure the quality of a split. The only supported     criterion is 'mse' for the mean squared error, which is equal to     variance reduction...", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes)"),]),
c("ExtraTreeClassifier(DecisionTreeClassifier)", "/tree/tree.py<br>An extremely randomized tree classifier.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split ...", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, class_weight)"),]),
c("ExtraTreeRegressor(DecisionTreeRegressor)", "/tree/tree.py<br>An extremely randomized tree regressor.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split a...", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes)"),]),]),]),
c("utils", "/utils<br>", [
c("sparsetools", "/utils/sparsetools<br>", [
c("tests", "/utils/sparsetools/tests<br>", [
c("test_traversal.py", "/utils/sparsetools/tests/test_traversal.py<br>", [
d("test_graph_breadth_first()"),
d("test_graph_depth_first()"),]),]),
c("setup.py", "/utils/sparsetools/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("_graph_validation.py", "/utils/sparsetools/_graph_validation.py<br>", [
d("validate_graph(csgraph, directed, dtype, csr_output, dense_output, copy_if_dense, copy_if_sparse, null_value_in, null_value_out, infinity_null, nan_null)", "Routine for validation and conversion of csgraph inputs"),]),]),
c("tests", "/utils/tests<br>", [
c("test_bench.py", "/utils/tests/test_bench.py<br>", [
d("test_total_seconds()"),]),
c("test_class_weight.py", "/utils/tests/test_class_weight.py<br>", [
d("test_compute_class_weight()"),
d("test_compute_class_weight_auto_negative()"),
d("test_compute_class_weight_auto_unordered()"),
d("test_compute_class_weight_invariance()"),
d("test_compute_class_weight_not_present()"),
d("test_compute_sample_weight()"),
d("test_compute_sample_weight_errors()"),
d("test_compute_sample_weight_with_subsample()"),]),
c("test_estimator_checks.py", "/utils/tests/test_estimator_checks.py<br>", [
c("BaseBadClassifier(BaseEstimator, ClassifierMixin)", "/utils/tests/test_estimator_checks.py<br>", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("CorrectNotFittedError(ValueError)", "/utils/tests/test_estimator_checks.py<br>Exception class to raise if estimator is used before fitting.  Like NotFittedError, it inherits from ValueError, but not from AttributeError. Used for testing only.", [
]),
c("CorrectNotFittedErrorClassifier(BaseBadClassifier)", "/utils/tests/test_estimator_checks.py<br>", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("NoCheckinPredict(BaseBadClassifier)", "/utils/tests/test_estimator_checks.py<br>", [
d("fit(self, X, y)"),]),
c("NoSparseClassifier(BaseBadClassifier)", "/utils/tests/test_estimator_checks.py<br>", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
d("test_check_estimator()"),
d("test_check_estimators_unfitted()"),]),
c("test_extmath.py", "/utils/tests/test_extmath.py<br>", [
d("test_cartesian()"),
d("test_density()"),
d("test_fast_dot()"),
d("test_incremental_variance_ddof()"),
d("test_incremental_variance_update_formulas()"),
d("test_logistic_sigmoid()"),
d("test_logsumexp()"),
d("test_norm_squared_norm()"),
d("test_random_weights()"),
d("test_randomized_svd_infinite_rank()"),
d("test_randomized_svd_low_rank()"),
d("test_randomized_svd_low_rank_with_noise()"),
d("test_randomized_svd_sign_flip()"),
d("test_randomized_svd_transpose_consistency()"),
d("test_row_norms()"),
d("test_svd_flip()"),
d("test_uniform_weights()"),
d("test_vector_sign_flip()"),]),
c("test_fast_dict.py", "/utils/tests/test_fast_dict.py<br> Test fast_dict.", [
d("test_int_float_dict()"),
d("test_int_float_dict_argmin()"),]),
c("test_fixes.py", "/utils/tests/test_fixes.py<br>", [
d("test_astype_copy_memory()"),
d("test_divide()"),
d("test_expit()"),]),
c("test_graph.py", "/utils/tests/test_graph.py<br>", [
d("test_graph_laplacian()"),]),
c("test_linear_assignment.py", "/utils/tests/test_linear_assignment.py<br>", [
d("test_hungarian()"),]),
c("test_metaestimators.py", "/utils/tests/test_metaestimators.py<br>", [
c("MockMetaEstimator(object)", "/utils/tests/test_metaestimators.py<br>This is a mock meta estimator", [
d("func(self)"),]),
c("Prefix(object)", "/utils/tests/test_metaestimators.py<br>", [
d("func()"),]),
d("test_delegated_docstring()"),]),
c("test_multiclass.py", "/utils/tests/test_multiclass.py<br>", [
c("NotAnArray(object)", "/utils/tests/test_multiclass.py<br>An object that is convertable to an array. This is useful to simulate a Pandas timeseries.", [
d("__array__(self)"),
d("__init__(self, data)"),]),
d("test_class_distribution()"),
d("test_is_multilabel()"),
d("test_type_of_target()"),
d("test_unique_labels()"),
d("test_unique_labels_mixed_types()"),
d("test_unique_labels_non_specific()"),]),
c("test_murmurhash.py", "/utils/tests/test_murmurhash.py<br>", [
d("test_mmhash3_bytes()"),
d("test_mmhash3_int()"),
d("test_mmhash3_int_array()"),
d("test_mmhash3_unicode()"),
d("test_no_collision_on_byte_range()"),
d("test_uniform_distribution()"),]),
c("test_optimize.py", "/utils/tests/test_optimize.py<br>", [
d("test_newton_cg()"),]),
c("test_random.py", "/utils/tests/test_random.py<br>", [
d("check_edge_case_of_sample_int(sample_without_replacement)"),
d("check_sample_int(sample_without_replacement)"),
d("check_sample_int_distribution(sample_without_replacement)"),
d("test_invalid_sample_without_replacement_algorithm()"),
d("test_random_choice_csc(n_samples, random_state)"),
d("test_random_choice_csc_errors()"),
d("test_sample_without_replacement_algorithms()"),]),
c("test_shortest_path.py", "/utils/tests/test_shortest_path.py<br>", [
d("floyd_warshall_slow(graph, directed)"),
d("generate_graph(N)"),
d("test_dijkstra()"),
d("test_dijkstra_bug_fix()"),
d("test_floyd_warshall()"),
d("test_shortest_path()"),]),
c("test_sparsefuncs.py", "/utils/tests/test_sparsefuncs.py<br>", [
d("test_count_nonzero()"),
d("test_csc_row_median()"),
d("test_densify_rows()"),
d("test_inplace_column_scale()"),
d("test_inplace_row_scale()"),
d("test_inplace_swap_column()"),
d("test_inplace_swap_row()"),
d("test_mean_variance_axis0()"),
d("test_mean_variance_axis1()"),
d("test_mean_variance_illegal_axis()"),
d("test_min_max_axis0()"),
d("test_min_max_axis1()"),
d("test_min_max_axis_errors()"),]),
c("test_stats.py", "/utils/tests/test_stats.py<br>", [
d("test_cases()"),]),
c("test_testing.py", "/utils/tests/test_testing.py<br>", [
c("TestWarns()", "/utils/tests/test_testing.py<br>", [
d("test_warn(self)"),
d("test_warn_wrong_warning(self)"),]),
d("test_assert_greater_equal()"),
d("test_assert_less_equal()"),
d("test_assert_raise_message()"),
d("test_set_random_state()"),]),
c("test_utils.py", "/utils/tests/test_utils.py<br>", [
d("test_column_or_1d()"),
d("test_deprecated()"),
d("test_gen_even_slices()"),
d("test_make_rng()"),
d("test_pinvh_nonpositive()"),
d("test_pinvh_simple_complex()"),
d("test_pinvh_simple_real()"),
d("test_resample_noarg()"),
d("test_resample_value_errors()"),
d("test_safe_indexing()"),
d("test_safe_indexing_mock_pandas()"),
d("test_safe_indexing_pandas()"),
d("test_safe_mask()"),
d("test_shuffle_dont_convert_to_array()"),
d("test_shuffle_on_ndim_equals_three()"),]),
c("test_validation.py", "/utils/tests/test_validation.py<br> Tests for input validation functions", [
d("test_as_float_array()"),
d("test_check_array()"),
d("test_check_array_dtype_stability()"),
d("test_check_array_dtype_warning()"),
d("test_check_array_min_samples_and_features_messages()"),
d("test_check_array_pandas_dtype_object_conversion()"),
d("test_check_consistent_length()"),
d("test_check_is_fitted()"),
d("test_check_symmetric()"),
d("test_has_fit_parameter()"),
d("test_memmap()"),
d("test_np_matrix()"),
d("test_ordering()"),]),]),
c("arpack.py", "/utils/arpack.py<br> This contains a copy of the future version of scipy.sparse.linalg.eigen.arpack.eigsh It's an upgraded wrapper of the ARPACK library which allows the use of shift-invert mode for symmetric matrices.   Find a few eigenvectors and eigenvalues of a matrix.   Uses ARPACK: http://www.caam.rice.edu/softwar...", [
c("ArpackError(RuntimeError)", "/utils/arpack.py<br>ARPACK error", [
d("__init__(self, info, infodict)"),]),
c("ArpackNoConvergence(ArpackError)", "/utils/arpack.py<br>ARPACK iteration did not converge  Attributes ---------- eigenvalues : ndarray     Partial result. Converged eigenvalues. eigenvectors : ndarray     Partial result. Converged eigenvectors.", [
d("__init__(self, msg, eigenvalues, eigenvectors)"),]),
c("IterInv(LinearOperator)", "/utils/arpack.py<br>IterInv:    helper class to repeatedly solve M*xb    using an iterative method.", [
d("__init__(self, M, ifunc, tol)"),
d("_matvec(self, x)"),]),
c("IterOpInv(LinearOperator)", "/utils/arpack.py<br>IterOpInv:    helper class to repeatedly solve [A-sigma*M]*x  b    using an iterative method", [
d("__init__(self, A, M, sigma, ifunc, tol)"),
d("_matvec(self, x)"),
d("mult_func(self, x)"),
d("mult_func_M_None(self, x)"),]),
c("LuInv(LinearOperator)", "/utils/arpack.py<br>LuInv:    helper class to repeatedly solve M*xb    using an LU-decomposition of M", [
d("__init__(self, M)"),
d("_matvec(self, x)"),]),
c("SpLuInv(LinearOperator)", "/utils/arpack.py<br>SpLuInv:    helper class to repeatedly solve M*xb    using a sparse LU-decopposition of M", [
d("__init__(self, M)"),
d("_matvec(self, x)"),]),
c("_ArpackParams(object)", "/utils/arpack.py<br>", [
d("__init__(self, n, k, tp, mode, sigma, ncv, v0, maxiter, which, tol)"),
d("_raise_no_convergence(self)"),]),
c("_SymmetricArpackParams(_ArpackParams)", "/utils/arpack.py<br>", [
d("__init__(self, n, k, tp, matvec, mode, M_matvec, Minv_matvec, sigma, ncv, v0, maxiter, which, tol)"),
d("extract(self, return_eigenvectors)"),
d("iterate(self)"),]),
c("_UnsymmetricArpackParams(_ArpackParams)", "/utils/arpack.py<br>", [
d("__init__(self, n, k, tp, matvec, mode, M_matvec, Minv_matvec, sigma, ncv, v0, maxiter, which, tol)"),
d("extract(self, return_eigenvectors)"),
d("iterate(self)"),]),
d("_aslinearoperator_with_dtype(m)"),
d("_eigs(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, OPpart)", "Find k eigenvalues and eigenvectors of the square matrix A.  Solves ``A * x[i]  w[i] * x[i]``, the standard eigenvalue problem for w[i] eigenvalues with corresponding eigenvectors x[i].  If M is specified, solves ``A * x[i]  w[i] * M * x[i]``, the generalized eigenvalue problem for w[i] eigenvalues ..."),
d("_eigsh(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)", "Find k eigenvalues and eigenvectors of the real symmetric square matrix or complex hermitian matrix A.  Solves ``A * x[i]  w[i] * x[i]``, the standard eigenvalue problem for w[i] eigenvalues with corresponding eigenvectors x[i].  If M is specified, solves ``A * x[i]  w[i] * M * x[i]``, the generaliz..."),
d("_svds(A, k, ncv, tol)", "Compute k singular values/vectors for a sparse matrix using ARPACK.  Parameters ---------- A : sparse matrix     Array to compute the SVD on k : int, optional     Number of singular values and vectors to compute. ncv : integer     The number of Lanczos vectors generated     ncv must be greater than ..."),
d("get_OPinv_matvec(A, M, sigma, symmetric, tol)"),
d("get_inv_matvec(M, symmetric, tol)"),]),
c("bench.py", "/utils/bench.py<br> Helper functions for benchmarking", [
d("total_seconds(delta)", "helper function to emulate function total_seconds, introduced in python2.7  http://docs.python.org/library/datetime.html#datetime.timedelta.total_seconds"),]),
c("class_weight.py", "/utils/class_weight.py<br>", [
d("compute_class_weight(class_weight, classes, y)", "Estimate class weights for unbalanced datasets.  Parameters ---------- class_weight : dict, 'balanced' or None     If 'balanced', class weights will be given by     ``n_samples / (n_classes * np.bincount(y))``.     If a dictionary is given, keys are classes and values     are corresponding class wei..."),
d("compute_sample_weight(class_weight, y, indices)", "Estimate sample weights by class for unbalanced datasets.  Parameters ---------- class_weight : dict, list of dicts, 'balanced', or None, optional     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one. For     multi-o..."),]),
c("estimator_checks.py", "/utils/estimator_checks.py<br>", [
c("NotAnArray(object)", "/utils/estimator_checks.py<br>An object that is convertable to an array", [
d("__array__(self, dtype)"),
d("__init__(self, data)"),]),
d("_boston_subset(n_samples)"),
d("_check_transformer(name, Transformer, X, y)"),
d("_is_32bit()", "Detect if process is 32bit Python."),
d("_yield_all_checks(name, Estimator)"),
d("_yield_classifier_checks(name, Classifier)"),
d("_yield_clustering_checks(name, Clusterer)"),
d("_yield_non_meta_checks(name, Estimator)"),
d("_yield_regressor_checks(name, Regressor)"),
d("_yield_transformer_checks(name, Transformer)"),
d("check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train, X_test, y_test, weights)"),
d("check_class_weight_balanced_linear_classifier(name, Classifier)", "Test class weights with non-contiguous class labels."),
d("check_class_weight_classifiers(name, Classifier)"),
d("check_classifier_data_not_an_array(name, Estimator)"),
d("check_classifiers_classes(name, Classifier)"),
d("check_classifiers_one_label(name, Classifier)"),
d("check_classifiers_train(name, Classifier)"),
d("check_clusterer_compute_labels_predict(name, Clusterer)", "Check that predict is invariant of compute_labels"),
d("check_clustering(name, Alg)"),
d("check_dtype_object(name, Estimator)"),
d("check_estimator(Estimator)", "Check if estimator adheres to sklearn conventions.  This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base. ..."),
d("check_estimator_sparse_data(name, Estimator)"),
d("check_estimators_data_not_an_array(name, Estimator, X, y)"),
d("check_estimators_dtypes(name, Estimator)"),
d("check_estimators_empty_data_messages(name, Estimator)"),
d("check_estimators_fit_returns_self(name, Estimator)", "Check if self is returned when calling fit"),
d("check_estimators_nan_inf(name, Estimator)"),
d("check_estimators_overwrite_params(name, Estimator)"),
d("check_estimators_partial_fit_n_features(name, Alg)"),
d("check_estimators_pickle(name, Estimator)", "Test that we can pickle all estimators"),
d("check_estimators_unfitted(name, Estimator)", "Check that predict raises an exception in an unfitted estimator.  Unfitted estimators should raise either AttributeError or ValueError. The specific exception type NotFittedError inherits from both and can therefore be adequately raised for that purpose."),
d("check_fit_score_takes_y(name, Estimator)"),
d("check_get_params_invariance(name, estimator)"),
d("check_non_transformer_estimators_n_iter(name, estimator, multi_output)"),
d("check_parameters_default_constructible(name, Estimator)"),
d("check_pipeline_consistency(name, Estimator)"),
d("check_regressor_data_not_an_array(name, Estimator)"),
d("check_regressors_int(name, Regressor)"),
d("check_regressors_no_decision_function(name, Regressor)"),
d("check_regressors_train(name, Regressor)"),
d("check_sparsify_coefficients(name, Estimator)"),
d("check_supervised_y_2d(name, Estimator)"),
d("check_transformer_data_not_an_array(name, Transformer)"),
d("check_transformer_general(name, Transformer)"),
d("check_transformer_n_iter(name, estimator)"),
d("check_transformers_unfitted(name, Transformer)"),
d("multioutput_estimator_convert_y_2d(name, y)"),
d("set_fast_parameters(estimator)"),]),
c("extmath.py", "/utils/extmath.py<br> Extended math utilities.", [
d("_batch_mean_variance_update(X, old_mean, old_variance, old_sample_count)", "Calculate an average mean update and a Youngs and Cramer variance update.  From the paper 'Algorithms for computing the sample variance: analysis and recommendations', by Chan, Golub, and LeVeque.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Data to use for variance updat..."),
d("_deterministic_vector_sign_flip(u)", "Modify the sign of vectors for reproducibility  Flips the sign of elements of all the vectors (rows of u) such that the absolute maximum element of each vector is positive.  Parameters ---------- u : ndarray     Array with vectors as its rows.  Returns ------- u_flipped : ndarray with same shape as ..."),
d("_fast_dot(A, B)"),
d("_have_blas_gemm()"),
d("_impose_f_order(X)", "Helper Function"),
d("cartesian(arrays, out)", "Generate a cartesian product of input arrays.  Parameters ---------- arrays : list of array-like     1-D arrays to form the cartesian product of. out : ndarray     Array to place the cartesian product in.  Returns ------- out : ndarray     2-D array of shape (M, len(arrays)) containing cartesian pro..."),
d("density(w)", "Compute density of a sparse vector  Return a value between 0 and 1"),
d("fast_logdet(A)", "Compute log(det(A)) for A symmetric  Equivalent to : np.log(nl.det(A)) but more robust. It returns -Inf if det(A) is non positive or is not defined."),
d("log_logistic(X, out)", "Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.  This implementation is numerically stable because it splits positive and negative values::      -log(1 + exp(-x_i))     if x_i > 0     x_i - log(1 + exp(x_i)) if x_i < 0  For the ordinary logistic function, use ``sklearn.utils.fi..."),
d("logsumexp(arr, axis)", "Computes the sum of arr assuming arr is in the log domain.  Returns log(sum(exp(arr))) while minimizing the possibility of over/underflow.  Examples --------  >>> import numpy as np >>> from sklearn.utils.extmath import logsumexp >>> a  np.arange(10) >>> np.log(np.sum(np.exp(a))) 9.4586297444267107 ..."),
d("make_nonnegative(X, min_value)", "Ensure `X.min()` > `min_value`."),
d("norm(x)", "Compute the Euclidean or Frobenius norm of x.  Returns the Euclidean norm when x is a vector, the Frobenius norm when x is a matrix (2-d array). More precise than sqrt(squared_norm(x))."),
d("pinvh(a, cond, rcond, lower)", "Compute the (Moore-Penrose) pseudo-inverse of a hermetian matrix.  Calculate a generalized inverse of a symmetric matrix using its eigenvalue decomposition and including all 'large' eigenvalues.  Parameters ---------- a : array, shape (N, N)     Real symmetric or complex hermetian matrix to be pseud..."),
d("randomized_range_finder(A, size, n_iter, random_state)", "Computes an orthonormal matrix whose range approximates the range of A.  Parameters ---------- A: 2D array     The input data matrix size: integer     Size of the return array n_iter: integer     Number of power iterations used to stabilize the result random_state: RandomState or an int seed (0 by d..."),
d("randomized_svd(M, n_components, n_oversamples, n_iter, transpose, flip_sign, random_state)", "Computes a truncated randomized SVD  Parameters ---------- M: ndarray or sparse matrix     Matrix to decompose  n_components: int     Number of singular values and vectors to extract.  n_oversamples: int (default is 10)     Additional number of random vectors to sample the range of M so as     to en..."),
d("row_norms(X, squared)", "Row-wise (squared) Euclidean norm of X.  Equivalent to np.sqrt((X * X).sum(axis1)), but also supports CSR sparse matrices and does not create an X.shape-sized temporary.  Performs no input validation."),
d("safe_min(X)", "Returns the minimum value of a dense or a CSR/CSC matrix.  Adapated from http://stackoverflow.com/q/13426580"),
d("safe_sparse_dot(a, b, dense_output)", "Dot product that handle the sparse matrix case correctly  Uses BLAS GEMM as replacement for numpy.dot where possible to avoid unnecessary copies."),
d("squared_norm(x)", "Squared Euclidean or Frobenius norm of x.  Returns the Euclidean norm when x is a vector, the Frobenius norm when x is a matrix (2-d array). Faster than norm(x) ** 2."),
d("svd_flip(u, v, u_based_decision)", "Sign correction to ensure deterministic output from SVD.  Adjusts the columns of u and the rows of v such that the loadings in the columns in u that are largest in absolute value are always positive.  Parameters ---------- u, v : ndarray     u and v are the output of `linalg.svd` or     `sklearn.uti..."),
d("weighted_mode(a, w, axis)", "Returns an array of the weighted modal (most common) value in a  If there is more than one such value, only the first is returned. The bin-count for the modal bins is also returned.  This is an extension of the algorithm in scipy.stats.mode.  Parameters ---------- a : array_like     n-dimensional ar..."),]),
c("fixes.py", "/utils/fixes.py<br> Compatibility fixes for older version of python, numpy and scipy  If you add content to this file, please give the version of the package at which the fixe is no longer needed.", [
d("_parse_version(version_string)"),]),
c("graph.py", "/utils/graph.py<br> Graph utilities and algorithms  Graphs are represented with their adjacency matrices, preferably using sparse matrices.", [
d("_laplacian_dense(graph, normed, return_diag)"),
d("_laplacian_sparse(graph, normed, return_diag)"),
d("graph_laplacian(csgraph, normed, return_diag)", "Return the Laplacian matrix of a directed graph.  For non-symmetric graphs the out-degree is used in the computation.  Parameters ---------- csgraph : array_like or sparse matrix, 2 dimensions     compressed-sparse graph, with shape (N, N). normed : bool, optional     If True, then compute normalize..."),
d("single_source_shortest_path_length(graph, source, cutoff)", "Return the shortest path length from source to all reachable nodes.  Returns a dictionary of shortest path lengths keyed by target.  Parameters ---------- graph: sparse matrix or 2D array (preferably LIL matrix)     Adjacency matrix of the graph source : node label    Starting node for path cutoff :..."),]),
c("linear_assignment_.py", "/utils/linear_assignment_.py<br> Solve the unique lowest-cost assignment problem using the Hungarian algorithm (also known as Munkres algorithm).", [
c("_HungarianState(object)", "/utils/linear_assignment_.py<br>State of one execution of the Hungarian algorithm.  Parameters ---------- cost_matrix : 2D matrix     The cost matrix. Does not have to be square.", [
d("__init__(self, cost_matrix)"),
d("_clear_covers(self)"),
d("_find_prime_in_row(self, row)"),]),
d("_hungarian(cost_matrix)", "The Hungarian algorithm.  Calculate the Munkres solution to the classical assignment problem and return the indices for the lowest-cost pairings.  Parameters ---------- cost_matrix : 2D matrix     The cost matrix. Does not have to be square.  Returns ------- indices : 2D array of indices     The pai..."),
d("_step1(state)", "Steps 1 and 2 in the Wikipedia page."),
d("_step3(state)", "Cover each column containing a starred zero. If n columns are covered, the starred zeros describe a complete set of unique assignments. In this case, Go to DONE, otherwise, Go to Step 4."),
d("_step4(state)", "Find a noncovered zero and prime it. If there is no starred zero in the row containing this primed zero, Go to Step 5. Otherwise, cover this row and uncover the column containing the starred zero. Continue in this manner until there are no uncovered zeros left. Save the smallest uncovered value and ..."),
d("_step5(state)", "Construct a series of alternating primed and starred zeros as follows. Let Z0 represent the uncovered primed zero found in Step 4. Let Z1 denote the starred zero in the column of Z0 (if any). Let Z2 denote the primed zero in the row of Z1 (there will always be one). Continue until the series termina..."),
d("_step6(state)", "Add the value found in Step 4 to every element of each covered row, and subtract it from every element of each uncovered column. Return to Step 4 without altering any stars, primes, or covered lines."),
d("linear_assignment(X)", "Solve the linear assignment problem using the Hungarian algorithm.  The problem is also known as maximum weight matching in bipartite graphs. The method is also known as the Munkres or Kuhn-Munkres algorithm.  Parameters ---------- X : array     The cost matrix of the bipartite graph  Returns ------..."),]),
c("metaestimators.py", "/utils/metaestimators.py<br> Utilities for meta-estimators", [
c("_IffHasAttrDescriptor(object)", "/utils/metaestimators.py<br>Implements a conditional property using the descriptor protocol.  Using this class to create a decorator will raise an ``AttributeError`` if the ``attribute_name`` is not present on the base object.  This allows ducktyping of the decorated method based on ``attribute_name``.  See https://docs.python...", [
d("__get__(self, obj, type)"),
d("__init__(self, fn, attribute_name)"),]),
d("if_delegate_has_method(delegate)", "Create a decorator for methods that are delegated to a sub-estimator  This enables ducktyping by hasattr returning True according to the sub-estimator.  >>> from sklearn.utils.metaestimators import if_delegate_has_method >>> >>> >>> class MetaEst(object): ...     def __init__(self, sub_est): ...    ..."),]),
c("mocking.py", "/utils/mocking.py<br>", [
c("ArraySlicingWrapper(object)", "/utils/mocking.py<br>", [
d("__getitem__(self, aslice)"),
d("__init__(self, array)"),]),
c("CheckingClassifier(BaseEstimator, ClassifierMixin)", "/utils/mocking.py<br>Dummy classifier to test pipelining and meta-estimators.  Checks some property of X and y in fit / predict. This allows testing whether pipelines / cross-validation or metaestimators changed the input.", [
d("__init__(self, check_y, check_X, foo_param)"),
d("fit(self, X, y)"),
d("predict(self, T)"),
d("score(self, X, Y)"),]),
c("MockDataFrame(object)", "/utils/mocking.py<br>", [
d("__array__(self)"),
d("__init__(self, array)"),
d("__len__(self)"),]),]),
c("multiclass.py", "/utils/multiclass.py<br> Multi-class / multi-label utility function ", [
d("_check_partial_fit_first_call(clf, classes)", "Private helper function for factorizing common classes param logic  Estimators that implement the ``partial_fit`` API need to be provided with the list of possible classes at the first call to partial_fit.  Subsequent calls to partial_fit should check that ``classes`` is still consistent with a prev..."),
d("_is_integral_float(y)"),
d("_unique_indicator(y)"),
d("_unique_multiclass(y)"),
d("class_distribution(y, sample_weight)", "Compute class priors from multioutput-multiclass target data  Parameters ---------- y : array like or sparse matrix of size (n_samples, n_outputs)     The labels for each example.  sample_weight : array-like of shape  (n_samples,), optional     Sample weights.  Returns ------- classes : list of size..."),
d("is_multilabel(y)", "Check if ``y`` is in a multilabel format.  Parameters ---------- y : numpy array of shape [n_samples]     Target values.  Returns ------- out : bool,     Return ``True``, if ``y`` is in a multilabel format, else ```False``.  Examples -------- >>> import numpy as np >>> from sklearn.utils.multiclass ..."),
d("type_of_target(y)", "Determine the type of data indicated by target `y`  Parameters ---------- y : array-like  Returns ------- target_type : string     One of:     * 'continuous': `y` is an array-like of floats that are not all       integers, and is 1d or a column vector.     * 'continuous-multioutput': `y` is a 2d arr..."),
d("unique_labels()", "Extract an ordered array of unique labels  We don't allow:     - mix of multilabel and multiclass (single label) targets     - mix of label indicator matrix and anything else,       because there are no explicit labels)     - mix of label indicator matrices of different sizes     - mix of string and..."),]),
c("optimize.py", "/utils/optimize.py<br> Our own implementation of the Newton algorithm  Unlike the scipy.optimize version, this version of the Newton conjugate gradient solver uses only one function call to retrieve the func value, the gradient value and a callable for the Hessian matvec product. If the function call is very expensive (e....", [
c("_LineSearchError(RuntimeError)", "/utils/optimize.py<br>", [
]),
d("_cg(fhess_p, fgrad, maxiter, tol)", "Solve iteratively the linear system 'fhess_p . xsupi  fgrad' with a conjugate gradient descent.  Parameters ---------- fhess_p : callable     Function that takes the gradient as a parameter and returns the     matrix product of the Hessian and gradient  fgrad : ndarray, shape (n_features,) or (n_fea..."),
d("_line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval)", "Same as line_search_wolfe1, but fall back to line_search_wolfe2 if suitable step length is not found, and raise an exception if a suitable step length is not found.  Raises ------ _LineSearchError     If no suitable step size is found"),
d("newton_cg(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)", "Minimization of scalar function of one or more variables using the Newton-CG algorithm.  Parameters ---------- grad_hess : callable     Should return the gradient and a callable returning the matvec product     of the Hessian.  func : callable     Should return the value of the function.  grad : cal..."),]),
c("random.py", "/utils/random.py<br>", [
d("choice(a, size, replace, p, random_state)", "choice(a, sizeNone, replaceTrue, pNone)  Generates a random sample from a given 1-D array  .. versionadded:: 1.7.0  Parameters ----------- a : 1-D array-like or int     If an ndarray, a random sample is generated from its elements.     If an int, the random sample is generated as if a was np.arange(..."),
d("random_choice_csc(n_samples, classes, class_probability, random_state)", "Generate a sparse random matrix given column class distributions  Parameters ---------- n_samples : int,     Number of samples to draw in each column.  classes : list of size n_outputs of arrays of size (n_classes,)     List of classes for each column.  class_probability : list of size n_outputs of ..."),]),
c("setup.py", "/utils/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("sparsefuncs.py", "/utils/sparsefuncs.py<br>", [
d("_get_elem_at_rank(rank, data, n_negative, n_zeros)", "Find the value in data augmented with n_zeros for the given rank"),
d("_get_median(data, n_zeros)", "Compute the median of data with n_zeros additional zeros.  This function is used to support sparse matrices; it modifies data in-place"),
d("_raise_typeerror(X)", "Raises a TypeError if X is not a CSR or CSC matrix"),
d("count_nonzero(X, axis, sample_weight)", "A variant of X.getnnz() with extension to weighting on axis 0  Useful in efficiently calculating multilabel metrics.  Parameters ---------- X : CSR sparse matrix, shape  (n_samples, n_labels)     Input data.  axis : None, 0 or 1     The axis on which the data is aggregated.  sample_weight : array, s..."),
d("csc_median_axis_0(X)", "Find the median across axis 0 of a CSC matrix. It is equivalent to doing np.median(X, axis0).  Parameters ---------- X : CSC sparse matrix, shape (n_samples, n_features)     Input data.  Returns ------- median : ndarray, shape (n_features,)     Median. "),
d("inplace_column_scale(X, scale)", "Inplace column scaling of a CSC/CSR matrix.  Scale each feature of the data matrix by multiplying with specific scale provided by the caller assuming a (n_samples, n_features) shape.  Parameters ---------- X: CSC or CSR matrix with shape (n_samples, n_features)     Matrix to normalize using the vari..."),
d("inplace_csr_column_scale(X, scale)", "Inplace column scaling of a CSR matrix.  Scale each feature of the data matrix by multiplying with specific scale provided by the caller assuming a (n_samples, n_features) shape.  Parameters ---------- X : CSR matrix with shape (n_samples, n_features)     Matrix to normalize using the variance of th..."),
d("inplace_csr_row_scale(X, scale)", "Inplace row scaling of a CSR matrix.  Scale each sample of the data matrix by multiplying with specific scale provided by the caller assuming a (n_samples, n_features) shape.  Parameters ---------- X : CSR sparse matrix, shape (n_samples, n_features)     Matrix to be scaled.  scale : float array wit..."),
d("inplace_row_scale(X, scale)", "Inplace row scaling of a CSR or CSC matrix.  Scale each row of the data matrix by multiplying with specific scale provided by the caller assuming a (n_samples, n_features) shape.  Parameters ---------- X : CSR or CSC sparse matrix, shape (n_samples, n_features)     Matrix to be scaled.  scale : floa..."),
d("inplace_swap_column(X, m, n)", "Swaps two columns of a CSC/CSR matrix in-place.  Parameters ---------- X : CSR or CSC sparse matrix, shape(n_samples, n_features)     Matrix whose two columns are to be swapped.  m: int     Index of the column of X to be swapped.  n : int     Index of the column of X to be swapped."),
d("inplace_swap_row(X, m, n)", "Swaps two rows of a CSC/CSR matrix in-place.  Parameters ---------- X : CSR or CSC sparse matrix, shape(n_samples, n_features)     Matrix whose two rows are to be swapped.  m: int     Index of the row of X to be swapped.  n: int     Index of the row of X to be swapped."),
d("inplace_swap_row_csc(X, m, n)", "Swaps two rows of a CSC matrix in-place.  Parameters ---------- X: scipy.sparse.csc_matrix, shape(n_samples, n_features)     Matrix whose two rows are to be swapped.  m: int     Index of the row of X to be swapped.  n: int     Index of the row of X to be swapped."),
d("inplace_swap_row_csr(X, m, n)", "Swaps two rows of a CSR matrix in-place.  Parameters ---------- X: scipy.sparse.csr_matrix, shape(n_samples, n_features)     Matrix whose two rows are to be swapped.  m: int     Index of the row of X to be swapped.  n: int     Index of the row of X to be swapped."),
d("mean_variance_axis(X, axis)", "Compute mean and variance along axis 0 on a CSR or CSC matrix  Parameters ---------- X: CSR or CSC sparse matrix, shape (n_samples, n_features)     Input data.  axis: int (either 0 or 1)     Axis along which the axis should be computed.  Returns -------  means: float array with shape (n_features,)  ..."),
d("min_max_axis(X, axis)", "Compute minimum and maximum along an axis on a CSR or CSC matrix  Parameters ---------- X : CSR or CSC sparse matrix, shape (n_samples, n_features)     Input data.  axis: int (either 0 or 1)     Axis along which the axis should be computed.  Returns -------  mins: float array with shape (n_features,..."),]),
c("stats.py", "/utils/stats.py<br>", [
d("_rankdata(a, method)", "Assign ranks to data, dealing with ties appropriately.  Ranks begin at 1. The method argument controls how ranks are assigned to equal values.  Parameters ---------- a : array_like     The array of values to be ranked. The array is first flattened.  method : str, optional     The method used to assi..."),
d("_weighted_percentile(array, sample_weight, percentile)", "Compute the weighted ``percentile`` of ``array`` with ``sample_weight``. "),]),
c("testing.py", "/utils/testing.py<br> Testing utilities.", [
c("TempMemmap(object)", "/utils/testing.py<br>", [
d("__enter__(self)"),
d("__exit__(self, exc_type, exc_val, exc_tb)"),
d("__init__(self, data, mmap_mode)"),]),
c("_IgnoreWarnings(object)", "/utils/testing.py<br>Improved and simplified Python warnings context manager  Copied from Python 2.7.5 and modified as required.", [
d("__enter__(self)"),
d("__exit__(self)"),
d("__init__(self)"),
d("__repr__(self)"),]),
d("_assert_allclose(actual, desired, rtol, atol, err_msg, verbose)"),
d("_assert_greater(a, b, msg)"),
d("_assert_less(a, b, msg)"),
d("_delete_folder(folder_path, warn)", "Utility function to cleanup a temporary folder if still existing. Copy from joblib.pool (for independance)"),
d("_ignore_warnings(fn)", "Decorator to catch and hide warnings without visual nesting"),
d("all_estimators(include_meta_estimators, include_other, type_filter, include_dont_test)", "Get a list of all estimators from sklearn.  This function crawls the module and gets all classes that inherit from BaseEstimator. Classes that are defined in test-modules are not included. By default meta_estimators such as GridSearchCV are also not included.  Parameters ---------- include_meta_esti..."),
d("assert_greater_equal(a, b, msg)"),
d("assert_less_equal(a, b, msg)"),
d("assert_no_warnings(func)"),
d("assert_raise_message(exceptions, message, function)", "Helper function to test error messages in exceptions  Parameters ---------- exceptions : exception or tuple of exception     Name of the estimator  func : callable     Calable object to raise error  *args : the positional arguments to `func`.  **kw : the keyword arguments to `func`"),
d("assert_warns(warning_class, func)", "Test that a certain warning occurs.  Parameters ---------- warning_class : the warning class     The class to test for, e.g. UserWarning.  func : callable     Calable object to trigger warnings.  *args : the positional arguments to `func`.  **kw : the keyword arguments to `func`  Returns -------  re..."),
d("assert_warns_message(warning_class, message, func)", "Test that a certain warning occurs and with a certain message.  Parameters ---------- warning_class : the warning class     The class to test for, e.g. UserWarning.  message : str | callable     The entire message or a substring to  test for. If callable,     it takes a string as argument and will t..."),
d("check_skip_network()"),
d("check_skip_travis()", "Skip test if being run on Travis."),
d("clean_warning_registry()", "Safe way to reset warnings "),
d("fake_mldata(columns_dict, dataname, matfile, ordering)", "Create a fake mldata data set.  Parameters ---------- columns_dict : dict, keysstr, valuesndarray     Contains data as columns_dict[column_name]  array of data.  dataname : string     Name of data set.  matfile : string or file object     The file name string or the file-like object of the output fi..."),
d("if_matplotlib(func)", "Test decorator that skips test if matplotlib not installed. "),
d("if_not_mac_os(versions, message)", "Test decorator that skips test if OS is Mac OS X and its major version is one of ``versions``."),
d("ignore_warnings(obj)", "Context manager and decorator to ignore warnings  Note. Using this (in both variants) will clear all warnings from all python modules loaded. In case you need to test cross-module-warning-logging this is not your tool of choice.  Examples -------- >>> with ignore_warnings(): ...     warnings.warn('b..."),
d("install_mldata_mock(mock_datasets)"),
c("mock_mldata_urlopen(object)", "/utils/testing.py<br>", [
d("__call__(self, urlname)"),
d("__init__(self, mock_datasets)"),]),
d("set_random_state(estimator, random_state)"),
d("uninstall_mldata_mock()"),]),
c("validation.py", "/utils/validation.py<br> Utilities for input validation", [
c("DataConversionWarning(UserWarning)", "/utils/validation.py<br>A warning on implicit data conversions happening in the code", [
]),
c("NonBLASDotWarning(UserWarning)", "/utils/validation.py<br>A warning on implicit dispatch to numpy.dot", [
]),
c("NotFittedError(ValueError, AttributeError)", "/utils/validation.py<br>Exception class to raise if estimator is used before fitting  This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.", [
]),
d("_assert_all_finite(X)", "Like assert_all_finite, but only for ndarray."),
d("_ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite)", "Convert a sparse matrix to a given format.  Checks the sparse format of spmatrix and converts if necessary.  Parameters ---------- spmatrix : scipy sparse matrix     Input to validate and convert.  accept_sparse : string, list of string or None (defaultNone)     String[s] representing allowed sparse..."),
d("_is_arraylike(x)", "Returns whether the input is array-like"),
d("_num_samples(x)", "Return number of samples in array-like x."),
d("_shape_repr(shape)", "Return a platform independent reprensentation of an array shape  Under Python 2, the `long` type introduces an 'L' suffix when using the default %r format for tuples of integers (typically used to store the shape of an array).  Under Windows 64 bit (and Python 2), the `long` type is used by default ..."),
d("as_float_array(X, copy, force_all_finite)", "Converts an array-like to an array of floats  The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.  Parameters ---------- X : {array-like, sparse matrix}  copy : bool, optional     If Tru..."),
d("assert_all_finite(X)", "Throw a ValueError if X contains NaN or infinity.  Input MUST be an np.ndarray instance or a scipy.sparse matrix."),
d("check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)", "Input validation for standard estimators.  Checks X and y for consistent length, enforces X 2d and y 1d. Standard input checks are only applied to y. For multi-label y, set multi_outputTrue to allow 2d and sparse y. If the dtype of X is object, attempt converting to float, raising on failure.  Param..."),
d("check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)", "Input validation on an array, list, sparse matrix or similar.  By default, the input is converted to an at least 2nd numpy array. If the dtype of the array is object, attempt converting to float, raising on failure.  Parameters ---------- array : object     Input object to check / convert.  accept_s..."),
d("check_consistent_length()", "Check that all arrays have consistent first dimensions.  Checks whether all objects in arrays have the same shape or length.  Parameters ---------- *arrays : list or tuple of input objects.     Objects that will be checked for consistent length."),
d("check_is_fitted(estimator, attributes, msg, all_or_any)", "Perform is_fitted validation for estimator.  Checks if the estimator is fitted by verifying the presence of 'all_or_any' of the passed attributes and raises a NotFittedError with the given message.  Parameters ---------- estimator : estimator instance.     estimator instance for which the check is p..."),
d("check_non_negative(X, whom)", "Check if there is any negative value in an array.  Parameters ---------- X : array-like or sparse matrix     Input data.  whom : string     Who passed X to this function."),
d("check_random_state(seed)", "Turn seed into a np.random.RandomState instance  If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError."),
d("check_symmetric(array, tol, raise_warning, raise_exception)", "Make sure that array is 2D, square and symmetric.  If the array is not symmetric, then a symmetrized version is returned. Optionally, a warning or exception is raised if the matrix is not symmetric.  Parameters ---------- array : nd-array or sparse matrix     Input object to check / convert. Must be..."),
d("column_or_1d(y, warn)", "Ravel column or 1d numpy array, else raises an error  Parameters ---------- y : array-like  warn : boolean, default False    To control display of warnings.  Returns ------- y : array"),
d("has_fit_parameter(estimator, parameter)", "Checks whether the estimator's fit method supports the given parameter.  Examples -------- >>> from sklearn.svm import SVC >>> has_fit_parameter(SVC(), 'sample_weight') True"),
d("indexable()", "Make arrays indexable for cross-validation.  Checks consistent length, passes through None, and ensures that everything can be indexed by converting sparse matrices to csr and converting non-interable objects to arrays.  Parameters ---------- *iterables : lists, dataframes, arrays, sparse matrices  ..."),]),
c("_scipy_sparse_lsqr_backport.py", "/utils/_scipy_sparse_lsqr_backport.py<br> Sparse Equations and Least Squares.  The original Fortran code was written by C. C. Paige and M. A. Saunders as described in  C. C. Paige and M. A. Saunders, LSQR: An algorithm for sparse linear equations and sparse least squares, TOMS 8(1), 43--71 (1982).  C. C. Paige and M. A. Saunders, Algorithm ...", [
d("_sym_ortho(a, b)", "Stable implementation of Givens rotation.  Notes ----- The routine 'SymOrtho' was added for numerical stability. This is recommended by S.-C. Choi in [1]_.  It removes the unpleasant potential of ``1/eps`` in some important places (see, for example text following 'Compute the next plane rotation Qk'..."),
d("lsqr(A, b, damp, atol, btol, conlim, iter_lim, show, calc_var)", "Find the least-squares solution to a large, sparse, linear system of equations.  The function solves ``Ax  b``  or  ``min ||b - Ax||^2`` or ``min ||Ax - b||^2 + d^2 ||x||^2``.  The matrix A may be square or rectangular (over-determined or under-determined), and may have any rank.  ::    1. Unsymmetr..."),]),
c("__init__.py", "/utils/__init__.py<br> The sklearn.utils module includes various utilities.", [
c("ConvergenceWarning(UserWarning)", "/utils/__init__.py<br>Custom warning to capture convergence problems", [
]),
c("DataDimensionalityWarning(UserWarning)", "/utils/__init__.py<br>Custom warning to notify potential issues with data dimensionality", [
]),
d("_get_n_jobs(n_jobs)", "Get number of jobs for the computation.  This function reimplements the logic of joblib to determine the actual number of jobs depending on the cpu count. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + ..."),
c("deprecated(object)", "/utils/__init__.py<br>Decorator to mark a function or class as deprecated.  Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.  The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for ex...", [
d("__call__(self, obj)"),
d("__init__(self, extra)"),
d("_decorate_class(self, cls)"),
d("_decorate_fun(self, fun)"),
d("_update_doc(self, olddoc)"),]),
d("gen_batches(n, batch_size)", "Generator to create slices containing batch_size elements, from 0 to n.  The last slice may contain less than batch_size elements, when batch_size does not divide n.  Examples -------- >>> from sklearn.utils import gen_batches >>> list(gen_batches(7, 3)) [slice(0, 3, None), slice(3, 6, None), slice(..."),
d("gen_even_slices(n, n_packs, n_samples)", "Generator to create n_packs slices going up to n.  Pass n_samples when the slices are to be used for sparse matrix indexing; slicing off-the-end raises an exception, while it works for NumPy arrays.  Examples -------- >>> from sklearn.utils import gen_even_slices >>> list(gen_even_slices(10, 1)) [sl..."),
d("resample()", "Resample arrays or sparse matrices in a consistent way  The default strategy implements one step of the bootstrapping procedure.  Parameters ---------- *arrays : sequence of indexable data-structures     Indexable data-structures can be arrays, lists, dataframes or scipy     sparse matrices with con..."),
d("safe_indexing(X, indices)", "Return items or rows from X using indices.  Allows simple indexing of lists or arrays.  Parameters ---------- X : array-like, sparse-matrix, list.     Data from which to sample rows or items.  indices : array-like, list     Indices according to which X will be subsampled."),
d("safe_mask(X, mask)", "Return a mask which is safe to use on X.  Parameters ---------- X : {array-like, sparse matrix}     Data on which to apply mask.  mask: array     Mask to be used on X.  Returns -------     mask"),
d("safe_sqr(X, copy)", "Element wise squaring of array-likes and sparse matrices.  Parameters ---------- X : array like, matrix, sparse matrix  copy : boolean, optional, default True     Whether to create a copy of X and operate on it or to perform     inplace computation (default behaviour).  Returns ------- X ** 2 : elem..."),
d("shuffle()", "Shuffle arrays or sparse matrices in a consistent way  This is a convenience alias to ``resample(*arrays, replaceFalse)`` to do random permutations of the collections.  Parameters ---------- *arrays : sequence of indexable data-structures     Indexable data-structures can be arrays, lists, dataframe..."),
d("tosequence(x)", "Cast iterable x to a Sequence, avoiding a copy if possible."),]),]),
c("__check_build", "/__check_build<br>", [
c("setup.py", "/__check_build/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("__init__.py", "/__check_build/__init__.py<br> Module to give helpful messages to the user that did not compile the scikit properly.", [
d("raise_build_error(e)"),]),]),
c("base.py", "/base.py<br> Base classes for all estimators.", [
c("BaseEstimator(object)", "/base.py<br>Base class for all estimators in scikit-learn  Notes ----- All estimators should specify all the parameters that can be set at the class level in their ``__init__`` as explicit keyword arguments (no ``*args`` or ``**kwargs``).", [
d("__repr__(self)"),
d("_get_param_names(cls)"),
d("get_params(self, deep)"),
d("set_params(self)"),]),
c("BiclusterMixin(object)", "/base.py<br>Mixin class for all bicluster estimators in scikit-learn", [
d("biclusters_(self)"),
d("get_indices(self, i)"),
d("get_shape(self, i)"),
d("get_submatrix(self, i, data)"),]),
c("ChangedBehaviorWarning(UserWarning)", "/base.py<br>", [
]),
c("ClassifierMixin(object)", "/base.py<br>Mixin class for all classifiers in scikit-learn.", [
d("score(self, X, y, sample_weight)"),]),
c("ClusterMixin(object)", "/base.py<br>Mixin class for all cluster estimators in scikit-learn.", [
d("fit_predict(self, X, y)"),]),
c("MetaEstimatorMixin(object)", "/base.py<br>Mixin class for all meta estimators in scikit-learn.", [
]),
c("RegressorMixin(object)", "/base.py<br>Mixin class for all regression estimators in scikit-learn.", [
d("score(self, X, y, sample_weight)"),]),
c("TransformerMixin(object)", "/base.py<br>Mixin class for all transformers in scikit-learn.", [
d("fit_transform(self, X, y)"),]),
d("_pprint(params, offset, printer)", "Pretty print the dictionary 'params'  Parameters ---------- params: dict     The dictionary to pretty print  offset: int     The offset in characters to add at the begin of each line.  printer:     The function to convert entries to strings, typically     the builtin str or repr"),
d("clone(estimator, safe)", "Constructs a new estimator with the same parameters.  Clone does a deep copy of the model in an estimator without actually copying attached data. It yields a new estimator with the same parameters that has not been fit on any data.  Parameters ---------- estimator: estimator object, or list, tuple o..."),
d("is_classifier(estimator)", "Returns True if the given estimator is (probably) a classifier."),
d("is_regressor(estimator)", "Returns True if the given estimator is (probably) a regressor."),]),
c("calibration.py", "/calibration.py<br> Calibration of predicted probabilities.", [
c("CalibratedClassifierCV(BaseEstimator, ClassifierMixin)", "/calibration.py<br>Probability calibration with isotonic regression or sigmoid.  With this class, the base_estimator is fit on the train set of the cross-validation generator and the test set is used for calibration. The probabilities for each of the folds are then averaged for prediction. In case that cv'prefit' is p...", [
d("__init__(self, base_estimator, method, cv)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),]),
c("_CalibratedClassifier(object)", "/calibration.py<br>Probability calibration with isotonic regression or sigmoid.  It assumes that base_estimator has already been fit, and trains the calibration on the input set of the fit function. Note that this class should not be used as an estimator directly. Use CalibratedClassifierCV with cv'prefit' instead.  P...", [
d("__init__(self, base_estimator, method)"),
d("_preproc(self, X)"),
d("fit(self, X, y, sample_weight)"),
d("predict_proba(self, X)"),]),
c("_SigmoidCalibration(BaseEstimator, RegressorMixin)", "/calibration.py<br>Sigmoid regression model.  Attributes ---------- a_ : float     The slope.  b_ : float     The intercept.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, T)"),]),
d("_sigmoid_calibration(df, y, sample_weight)", "Probability Calibration with sigmoid method (Platt 2000)  Parameters ---------- df : ndarray, shape (n_samples,)     The decision function or predict proba for the samples.  y : ndarray, shape (n_samples,)     The targets.  sample_weight : array-like, shape  [n_samples] or None     Sample weights. I..."),
d("calibration_curve(y_true, y_prob, normalize, n_bins)", "Compute true and predicted probabilities for a calibration curve.  Read more in the :ref:`User Guide <calibration>`.  Parameters ---------- y_true : array, shape (n_samples,)     True targets.  y_prob : array, shape (n_samples,)     Probabilities of the positive class.  normalize : bool, optional, d..."),]),
c("cross_validation.py", "/cross_validation.py<br> The sklearn.cross_validation module includes utilities for cross- validation and performance evaluation.", [
c("BaseShuffleSplit()", "/cross_validation.py<br>Base class for ShuffleSplit and StratifiedShuffleSplit", [
d("__init__(self, n, n_iter, test_size, train_size, random_state)"),
d("__iter__(self)"),
d("_iter_indices(self)"),]),
c("FitFailedWarning(RuntimeWarning)", "/cross_validation.py<br>", [
]),
c("KFold(_BaseKFold)", "/cross_validation.py<br>K-Folds cross validation iterator.  Provides train/test indices to split data in train test sets. Split dataset into k consecutive folds (without shuffling).  Each fold is then used a validation set once while the k - 1 remaining fold form the training set.  Read more in the :ref:`User Guide <cross_...", [
d("__init__(self, n, n_folds, shuffle, random_state)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_indices(self)"),]),
c("LeaveOneLabelOut(_PartitionIterator)", "/cross_validation.py<br>Leave-One-Label_Out cross-validation iterator  Provides train/test indices to split data according to a third-party provided label. This label information can be used to encode arbitrary domain specific stratifications of the samples as integers.  For instance the labels could be the year of collect...", [
d("__init__(self, labels)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_masks(self)"),]),
c("LeaveOneOut(_PartitionIterator)", "/cross_validation.py<br>Leave-One-Out cross validation iterator.  Provides train/test indices to split data in train test sets. Each sample is used once as a test set (singleton) while the remaining samples form the training set.  Note: ``LeaveOneOut(n)`` is equivalent to ``KFold(n, n_foldsn)`` and ``LeavePOut(n, p1)``.  D...", [
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_indices(self)"),]),
c("LeavePLabelOut(_PartitionIterator)", "/cross_validation.py<br>Leave-P-Label_Out cross-validation iterator  Provides train/test indices to split data according to a third-party provided label. This label information can be used to encode arbitrary domain specific stratifications of the samples as integers.  For instance the labels could be the year of collectio...", [
d("__init__(self, labels, p)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_masks(self)"),]),
c("LeavePOut(_PartitionIterator)", "/cross_validation.py<br>Leave-P-Out cross validation iterator  Provides train/test indices to split data in train test sets. This results in testing on all distinct samples of size p, while the remaining n - p samples form the training set in each iteration.  Note: ``LeavePOut(n, p)`` is NOT equivalent to ``KFold(n, n_fold...", [
d("__init__(self, n, p)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_indices(self)"),]),
c("PredefinedSplit(_PartitionIterator)", "/cross_validation.py<br>Predefined split cross validation iterator  Splits the data into training/test set folds according to a predefined scheme. Each sample can be assigned to at most one test set fold, as specified by the user through the ``test_fold`` parameter.  Read more in the :ref:`User Guide <cross_validation>`.  ...", [
d("__init__(self, test_fold)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_indices(self)"),]),
c("ShuffleSplit(BaseShuffleSplit)", "/cross_validation.py<br>Random permutation cross-validation iterator.  Yields indices to split data into training and test sets.  Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.  Read more in the ...", [
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_indices(self)"),]),
c("StratifiedKFold(_BaseKFold)", "/cross_validation.py<br>Stratified K-Folds cross validation iterator  Provides train/test indices to split data in train test sets.  This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.  Read more in the :ref:`User Gui...", [
d("__init__(self, y, n_folds, shuffle, random_state)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_test_masks(self)"),]),
c("StratifiedShuffleSplit(BaseShuffleSplit)", "/cross_validation.py<br>Stratified ShuffleSplit cross validation iterator  Provides train/test indices to split data in train test sets.  This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for eac...", [
d("__init__(self, y, n_iter, test_size, train_size, random_state)"),
d("__len__(self)"),
d("__repr__(self)"),
d("_iter_indices(self)"),]),
c("_BaseKFold()", "/cross_validation.py<br>Base class to validate KFold approaches", [
d("__init__(self, n, n_folds, shuffle, random_state)"),]),
c("_PartitionIterator()", "/cross_validation.py<br>Base class for CV iterators where train_mask  ~test_mask  Implementations must define `_iter_test_masks` or `_iter_test_indices`.  Parameters ---------- n : int     Total number of elements in dataset.", [
d("__init__(self, n)"),
d("__iter__(self)"),
d("_empty_mask(self)"),
d("_iter_test_indices(self)"),
d("_iter_test_masks(self)"),]),
d("_check_is_partition(locs, n)", "Check whether locs is a reordering of the array np.arange(n)  Parameters ---------- locs : ndarray     integer array to test n : int     number of expected elements  Returns ------- is_partition : bool     True iff sorted(locs) is range(n)"),
d("_fit_and_predict(estimator, X, y, train, test, verbose, fit_params)", "Fit estimator and predict values for a given dataset split.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- estimator : estimator object implementing 'fit' and 'predict'     The object to use to fit the data.  X : array-like of shape at least 2D     The data to fit.  y..."),
d("_fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)", "Fit estimator and compute scores for a given dataset split.  Parameters ---------- estimator : estimator object implementing 'fit'     The object to use to fit the data.  X : array-like of shape at least 2D     The data to fit.  y : array-like, optional, default: None     The target variable to try ..."),
d("_index_param_value(X, v, indices)", "Private helper function for parameter value indexing."),
d("_permutation_test_score(estimator, X, y, cv, scorer)", "Auxiliary function for permutation_test_score"),
d("_safe_split(estimator, X, y, indices, train_indices)", "Create subset of dataset and properly handle kernels."),
d("_score(estimator, X_test, y_test, scorer)", "Compute the score of an estimator on a given test set."),
d("_shuffle(y, labels, random_state)", "Return a shuffled copy of y eventually shuffle among same labels."),
d("_validate_shuffle_split(n, test_size, train_size)"),
d("check_cv(cv, X, y, classifier)", "Input checker utility for building a CV in a user friendly way.  Parameters ---------- cv : int, a cv generator instance, or None     The input specifying which cv generator to use. It can be an     integer, in which case it is the number of folds in a KFold,     None, in which case 3 fold is used, ..."),
d("cross_val_predict(estimator, X, y, cv, n_jobs, verbose, fit_params, pre_dispatch)", "Generate cross-validated estimates for each input data point  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- estimator : estimator object implementing 'fit' and 'predict'     The object to use to fit the data.  X : array-like     The data to fit. Can be, for example a ..."),
d("cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)", "Evaluate a score by cross-validation  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- estimator : estimator object implementing 'fit'     The object to use to fit the data.  X : array-like     The data to fit. Can be, for example a list, or an array at least 2d.  y : ar..."),
d("permutation_test_score(estimator, X, y, cv, n_permutations, n_jobs, labels, random_state, verbose, scoring)", "Evaluate the significance of a cross-validated score with permutations  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- estimator : estimator object implementing 'fit'     The object to use to fit the data.  X : array-like of shape at least 2D     The data to fit.  y : ..."),
d("train_test_split()", "Split arrays or matrices into random train and test subsets  Quick utility that wraps input validation and ``next(iter(ShuffleSplit(n_samples)))`` and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner.  Read more in the :ref:`User Guide <cross..."),]),
c("dummy.py", "/dummy.py<br>", [
c("DummyClassifier(BaseEstimator, ClassifierMixin)", "/dummy.py<br>DummyClassifier is a classifier that makes predictions using simple rules.  This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.  Parameters ---------- strategy : str     Stra...", [
d("__init__(self, strategy, random_state, constant)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),
c("DummyRegressor(BaseEstimator, RegressorMixin)", "/dummy.py<br>DummyRegressor is a regressor that makes predictions using simple rules.  This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.  Parameters ---------- strategy : str     Strategy...", [
d("__init__(self, strategy, constant, quantile)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),]),
c("grid_search.py", "/grid_search.py<br> The sklearn.grid_search includes utilities to fine-tune the parameters of an estimator.", [
c("BaseSearchCV()", "/grid_search.py<br>Base class for hyper parameter search with cross-validation.", [
d("__init__(self, estimator, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)"),
d("_estimator_type(self)"),
d("_fit(self, X, y, parameter_iterable)"),
d("decision_function(self, X)"),
d("inverse_transform(self, Xt)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),
d("score(self, X, y)"),
d("transform(self, X)"),]),
c("GridSearchCV(BaseSearchCV)", "/grid_search.py<br>Exhaustive search over specified parameter values for an estimator.  Important members are fit, predict.  GridSearchCV implements a 'fit' method and a 'predict' method like any classifier except that the parameters of the classifier used to predict is optimized by cross-validation.  Read more in the...", [
d("__init__(self, estimator, param_grid, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)"),
d("fit(self, X, y)"),]),
c("ParameterGrid(object)", "/grid_search.py<br>Grid of parameters with a discrete number of values for each.  Can be used to iterate over parameter value combinations with the Python built-in function iter.  Read more in the :ref:`User Guide <grid_search>`.  Parameters ---------- param_grid : dict of string to sequence, or sequence of such     T...", [
d("__getitem__(self, ind)"),
d("__init__(self, param_grid)"),
d("__iter__(self)"),
d("__len__(self)"),]),
c("ParameterSampler(object)", "/grid_search.py<br>Generator on parameters sampled from given distributions.  Non-deterministic iterable over random candidate combinations for hyper- parameter search. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling w...", [
d("__init__(self, param_distributions, n_iter, random_state)"),
d("__iter__(self)"),
d("__len__(self)"),]),
c("RandomizedSearchCV(BaseSearchCV)", "/grid_search.py<br>Randomized search on hyper parameters.  RandomizedSearchCV implements a 'fit' method and a 'predict' method like any classifier except that the parameters of the classifier used to predict is optimized by cross-validation.  In contrast to GridSearchCV, not all parameter values are tried out, but rat...", [
d("__init__(self, estimator, param_distributions, n_iter, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, random_state, error_score)"),
d("fit(self, X, y)"),]),
c("_CVScoreTuple()", "/grid_search.py<br>", [
d("__repr__(self)"),]),
d("_check_param_grid(param_grid)"),
d("fit_grid_point(X, y, estimator, parameters, train, test, scorer, verbose, error_score)", "Run fit on one set of parameters.  Parameters ---------- X : array-like, sparse matrix or list     Input data.  y : array-like or None     Targets for input data.  estimator : estimator object     This estimator will be cloned and then fitted.  parameters : dict     Parameters to be set on estimator..."),]),
c("isotonic.py", "/isotonic.py<br>", [
c("IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin)", "/isotonic.py<br>Isotonic regression model.  The isotonic regression optimization problem is defined by::      min sum w_i (y[i] - y_[i]) ** 2      subject to y_[i] < y_[j] whenever X[i] < X[j]     and min(y_)  y_min, max(y_)  y_max  where:     - ``y[i]`` are inputs (real numbers)     - ``y_[i]`` are fitted     - ``...", [
d("__getstate__(self)"),
d("__init__(self, y_min, y_max, increasing, out_of_bounds)"),
d("__setstate__(self, state)"),
d("_build_f(self, X, y)"),
d("_build_y(self, X, y, sample_weight)"),
d("_check_fit_data(self, X, y, sample_weight)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, T)"),
d("transform(self, T)"),]),
d("check_increasing(x, y)", "Determine whether y is monotonically correlated with x.  y is found increasing or decreasing with respect to x based on a Spearman correlation test.  Parameters ---------- x : array-like, shape(n_samples,)         Training data.  y : array-like, shape(n_samples,)     Training target.  Returns ------..."),
d("isotonic_regression(y, sample_weight, y_min, y_max, increasing)", "Solve the isotonic regression model::      min sum w[i] (y[i] - y_[i]) ** 2      subject to y_min  y_[1] < y_[2] ... < y_[n]  y_max  where:     - y[i] are inputs (real numbers)     - y_[i] are fitted     - w[i] are optional strictly positive weights (default to 1.0)  Read more in the :ref:`User Guid..."),]),
c("kernel_approximation.py", "/kernel_approximation.py<br> The sklearn.kernel_approximation module implements several approximate kernel feature maps base on Fourier transforms.", [
c("AdditiveChi2Sampler(BaseEstimator, TransformerMixin)", "/kernel_approximation.py<br>Approximate feature map for additive chi2 kernel.  Uses sampling the fourier transform of the kernel characteristic at regular intervals.  Since the kernel that is to be approximated is additive, the components of the input vectors can be treated separately.  Each entry in the original space is tran...", [
d("__init__(self, sample_steps, sample_interval)"),
d("_transform_dense(self, X)"),
d("_transform_sparse(self, X)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("Nystroem(BaseEstimator, TransformerMixin)", "/kernel_approximation.py<br>Approximate a kernel map using a subset of the training data.  Constructs an approximate feature map for an arbitrary kernel using a subset of the data as basis.  Read more in the :ref:`User Guide <nystroem_kernel_approx>`.  Parameters ---------- kernel : string or callable, default'rbf'     Kernel ...", [
d("__init__(self, kernel, gamma, coef0, degree, kernel_params, n_components, random_state)"),
d("_get_kernel_params(self)"),
d("fit(self, X, y)"),
d("transform(self, X)"),]),
c("RBFSampler(BaseEstimator, TransformerMixin)", "/kernel_approximation.py<br>Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.  It implements a variant of Random Kitchen Sinks.[1]  Read more in the :ref:`User Guide <rbf_kernel_approx>`.  Parameters ---------- gamma : float     Parameter of RBF kernel: exp(-gamma * x^2)  n_compon...", [
d("__init__(self, gamma, n_components, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("SkewedChi2Sampler(BaseEstimator, TransformerMixin)", "/kernel_approximation.py<br>Approximates feature map of the 'skewed chi-squared' kernel by Monte Carlo approximation of its Fourier transform.  Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.  Parameters ---------- skewedness : float     'skewedness' parameter of the kernel. Needs to be cross-validated.  n_compo...", [
d("__init__(self, skewedness, n_components, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),]),
c("kernel_ridge.py", "/kernel_ridge.py<br> Module sklearn.kernel_ridge implements kernel ridge regression.", [
c("KernelRidge(BaseEstimator, RegressorMixin)", "/kernel_ridge.py<br>Kernel ridge regression.  Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-l...", [
d("__init__(self, alpha, kernel, gamma, degree, coef0, kernel_params)"),
d("_get_kernel(self, X, Y)"),
d("_pairwise(self)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),]),
c("lda.py", "/lda.py<br> Linear Discriminant Analysis (LDA)", [
c("LDA(BaseEstimator, LinearClassifierMixin, TransformerMixin)", "/lda.py<br>Linear Discriminant Analysis (LDA).  A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.  The fitted model can a...", [
d("__init__(self, solver, shrinkage, priors, n_components, store_covariance, tol)"),
d("_solve_eigen(self, X, y, shrinkage)"),
d("_solve_lsqr(self, X, y, shrinkage)"),
d("_solve_svd(self, X, y, store_covariance, tol)"),
d("fit(self, X, y, store_covariance, tol)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),
d("transform(self, X)"),]),
d("_class_cov(X, y, priors, shrinkage)", "Compute class covariance matrix.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Input data.  y : array-like, shape (n_samples,) or (n_samples, n_targets)     Target values.  priors : array-like, shape (n_classes,)     Class priors.  shrinkage : string or float, optional    ..."),
d("_class_means(X, y)", "Compute class means.  Parameters ---------- X : array-like, shape (n_samples, n_features)     Input data.  y : array-like, shape (n_samples,) or (n_samples, n_targets)     Target values.  Returns ------- means : array-like, shape (n_features,)     Class means."),
d("_cov(X, shrinkage)", "Estimate covariance matrix (using optional shrinkage).  Parameters ---------- X : array-like, shape (n_samples, n_features)     Input data.  shrinkage : string or float, optional     Shrinkage parameter, possible values:       - None or 'empirical': no shrinkage (default).       - 'auto': automatic ..."),]),
c("learning_curve.py", "/learning_curve.py<br> Utilities to evaluate models with respect to a variable", [
d("_incremental_fit_estimator(estimator, X, y, classes, train, test, train_sizes, scorer, verbose)", "Train estimator on training subsets incrementally and compute scores."),
d("_translate_train_sizes(train_sizes, n_max_training_samples)", "Determine absolute sizes of training subsets and validate 'train_sizes'.  Examples:     _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]     _translate_train_sizes([5, 10], 10) -> [5, 10]  Parameters ---------- train_sizes : array-like, shape (n_ticks,), dtype float or int     Numbers of training e..."),
d("learning_curve(estimator, X, y, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose)", "Learning curve.  Determines cross-validated training and test scores for different training set sizes.  A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set with varying sizes will be used to train the estimator and a score for each tra..."),
d("validation_curve(estimator, X, y, param_name, param_range, cv, scoring, n_jobs, pre_dispatch, verbose)", "Validation curve.  Determine training and test scores for varying parameter values.  Compute scores for an estimator with different values of a specified parameter. This is similar to grid search with one parameter. However, this will also compute training scores and is merely a utility for plotting..."),]),
c("multiclass.py", "/multiclass.py<br> Multiclass and multilabel classification strategies   This module implements multiclass learning algorithms:     - one-vs-the-rest / one-vs-all     - one-vs-one     - error correcting output codes  The estimators provided in this module are meta-estimators: they require a base estimator to be provid...", [
c("OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin)", "/multiclass.py<br>One-vs-one multiclass strategy  This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers, this method is usually slower than one-vs-the-rest, due to...", [
d("__init__(self, estimator, n_jobs)"),
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin)", "/multiclass.py<br>One-vs-the-rest (OvR) multiclass/multilabel strategy  Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only `n_classes` classifiers are needed), o...", [
d("__init__(self, estimator, n_jobs)"),
d("classes_(self)"),
d("coef_(self)"),
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("intercept_(self)"),
d("multilabel_(self)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),]),
c("OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin)", "/multiclass.py<br>(Error-Correcting) Output-Code multiclass strategy  Output-code based strategies consist in representing each class with a binary code (an array of 0s and 1s). At fitting time, one binary classifier per bit in the code book is fitted.  At prediction time, the classifiers are used to project new poin...", [
d("__init__(self, estimator, code_size, random_state, n_jobs)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("_ConstantPredictor(BaseEstimator)", "/multiclass.py<br>", [
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),]),
d("_check_estimator(estimator)", "Make sure that an estimator implements the necessary methods."),
d("_fit_binary(estimator, X, y, classes)", "Fit a single binary estimator."),
d("_fit_ovo_binary(estimator, X, y, i, j)", "Fit a single binary estimator (one-vs-one)."),
d("_ovr_decision_function(predictions, confidences, n_classes)", "Compute a continuous, tie-breaking ovr decision function.  It is important to include a continuous value, not only votes, to make computing AUC or calibration meaningful.  Parameters ---------- predictions : array-like, shape (n_samples, n_classifiers)     Predicted classes for each binary classifie..."),
d("_predict_binary(estimator, X)", "Make predictions using a single binary estimator."),
d("fit_ecoc(estimator, X, y, code_size, random_state, n_jobs)", "Fit an error-correcting output-code strategy.  Parameters ---------- estimator : estimator object     An estimator object implementing `fit` and one of `decision_function`     or `predict_proba`.  code_size : float, optional     Percentage of the number of classes to be used to create the code book...."),
d("fit_ovo(estimator, X, y, n_jobs)"),
d("fit_ovr(estimator, X, y, n_jobs)", "Fit a one-vs-the-rest strategy.  Parameters ---------- estimator : estimator object     An estimator object implementing `fit` and one of `decision_function`     or `predict_proba`.  X : (sparse) array-like, shape  [n_samples, n_features]     Data.  y : (sparse) array-like, shape  [n_samples] or [n_..."),
d("predict_ecoc(estimators, classes, code_book, X)", "Make predictions using the error-correcting output-code strategy."),
d("predict_ovo(estimators, classes, X)", "Make predictions using the one-vs-one strategy."),
d("predict_ovr(estimators, label_binarizer, X)", "Predict multi-class targets using the one vs rest strategy.  Parameters ---------- estimators : list of `n_classes` estimators, Estimators used for     predictions. The list must be homogeneous with respect to the type of     estimators. fit_ovr supplies this list as part of its output.  label_binar..."),
d("predict_proba_ovr(estimators, X, is_multilabel)"),]),
c("naive_bayes.py", "/naive_bayes.py<br> The sklearn.naive_bayes module implements Naive Bayes algorithms. These are supervised learning methods based on applying Bayes' theorem with strong (naive) feature independence assumptions.", [
c("BaseDiscreteNB(BaseNB)", "/naive_bayes.py<br>Abstract base class for naive Bayes on discrete/categorical data  Any estimator based on this class should provide:  __init__ _joint_log_likelihood(X) as per BaseNB", [
d("_get_coef(self)"),
d("_get_intercept(self)"),
d("_update_class_log_prior(self, class_prior)"),
d("fit(self, X, y, sample_weight)"),
d("partial_fit(self, X, y, classes, sample_weight)"),]),
c("BaseNB()", "/naive_bayes.py<br>Abstract base class for naive Bayes estimators", [
d("_joint_log_likelihood(self, X)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),
c("BernoulliNB(BaseDiscreteNB)", "/naive_bayes.py<br>Naive Bayes classifier for multivariate Bernoulli models.  Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.  Read more in the :ref:`User Guide <bernoulli_na...", [
d("__init__(self, alpha, binarize, fit_prior, class_prior)"),
d("_count(self, X, Y)"),
d("_joint_log_likelihood(self, X)"),
d("_update_feature_log_prob(self)"),]),
c("GaussianNB(BaseNB)", "/naive_bayes.py<br>Gaussian Naive Bayes (GaussianNB)  Can perform online updates to model parameters via `partial_fit` method. For details on algorithm used to update feature means and variance online, see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:      http://i.stanford.edu/pub/cstr/reports/c...", [
d("_joint_log_likelihood(self, X)"),
d("_partial_fit(self, X, y, classes, _refit, sample_weight)"),
d("_update_mean_variance(n_past, mu, var, X, sample_weight)"),
d("fit(self, X, y, sample_weight)"),
d("partial_fit(self, X, y, classes, sample_weight)"),]),
c("MultinomialNB(BaseDiscreteNB)", "/naive_bayes.py<br>Naive Bayes classifier for multinomial models  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such...", [
d("__init__(self, alpha, fit_prior, class_prior)"),
d("_count(self, X, Y)"),
d("_joint_log_likelihood(self, X)"),
d("_update_feature_log_prob(self)"),]),]),
c("pipeline.py", "/pipeline.py<br> The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.", [
c("FeatureUnion(BaseEstimator, TransformerMixin)", "/pipeline.py<br>Concatenates results of multiple transformer objects.  This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.  Read more in the :ref:`User Guide <fea...", [
d("__init__(self, transformer_list, n_jobs, transformer_weights)"),
d("_update_transformer_list(self, transformers)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("get_feature_names(self)"),
d("get_params(self, deep)"),
d("transform(self, X)"),]),
c("Pipeline(BaseEstimator)", "/pipeline.py<br>Pipeline of transforms with a final estimator.  Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be 'transforms', that is, they must implement fit and transform methods. The final estimator only needs to implement fit.  The purpose of the pipelin...", [
d("__init__(self, steps)"),
d("_estimator_type(self)"),
d("_final_estimator(self)"),
d("_pairwise(self)"),
d("_pre_transform(self, X, y)"),
d("classes_(self)"),
d("decision_function(self, X)"),
d("fit(self, X, y)"),
d("fit_predict(self, X, y)"),
d("fit_transform(self, X, y)"),
d("get_params(self, deep)"),
d("inverse_transform(self, X)"),
d("named_steps(self)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),
d("score(self, X, y)"),
d("transform(self, X)"),]),
d("_fit_one_transformer(transformer, X, y)"),
d("_fit_transform_one(transformer, name, X, y, transformer_weights)"),
d("_name_estimators(estimators)", "Generate names for estimators."),
d("_transform_one(transformer, name, X, transformer_weights)"),
d("make_pipeline()", "Construct a Pipeline from the given estimators.  This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, they will be given names automatically based on their types.  Examples -------- >>> from sklearn.naive_bayes import GaussianNB ..."),
d("make_union()", "Construct a FeatureUnion from the given transformers.  This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.  Examples -------- >..."),]),
c("qda.py", "/qda.py<br> Quadratic Discriminant Analysis", [
c("QDA(BaseEstimator, ClassifierMixin)", "/qda.py<br>Quadratic Discriminant Analysis (QDA)  A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class.  Read more in the :ref:`User Guide <lda_qda>`.  Parameters ---------- priors :...", [
d("__init__(self, priors, reg_param)"),
d("_decision_function(self, X)"),
d("decision_function(self, X)"),
d("fit(self, X, y, store_covariances, tol)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),]),
c("random_projection.py", "/random_projection.py<br> Random Projection transformers  Random Projections are a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes.  The dimensions and distribution of Rand...", [
c("BaseRandomProjection()", "/random_projection.py<br>Base class for random projections.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, n_components, eps, dense_output, random_state)"),
d("_make_random_matrix(n_components, n_features)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("GaussianRandomProjection(BaseRandomProjection)", "/random_projection.py<br>Reduce dimensionality through Gaussian random projection  The components of the random matrix are drawn from N(0, 1 / n_components).  Read more in the :ref:`User Guide <gaussian_random_matrix>`.  Parameters ---------- n_components : int or 'auto', optional (default  'auto')     Dimensionality of the...", [
d("__init__(self, n_components, eps, random_state)"),
d("_make_random_matrix(self, n_components, n_features)"),]),
c("SparseRandomProjection(BaseRandomProjection)", "/random_projection.py<br>Reduce dimensionality through sparse random projection  Sparse random matrix is an alternative to dense random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.  If we note `s  1 / density` the co...", [
d("__init__(self, n_components, density, eps, dense_output, random_state)"),
d("_make_random_matrix(self, n_components, n_features)"),]),
d("_check_density(density, n_features)", "Factorize density check according to Li et al."),
d("_check_input_size(n_components, n_features)", "Factorize argument checking for random matrix generation"),
d("gaussian_random_matrix(n_components, n_features, random_state)", "Generate a dense Gaussian random matrix.  The components of the random matrix are drawn from      N(0, 1.0 / n_components).  Read more in the :ref:`User Guide <gaussian_random_matrix>`.  Parameters ---------- n_components : int,     Dimensionality of the target projection space.  n_features : int,  ..."),
d("johnson_lindenstrauss_min_dim(n_samples, eps)", "Find a 'safe' number of components to randomly project to  The distortion introduced by a random projection `p` only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection `p` is an eps-embedding as defined by:    (1 - eps) ||u - v|..."),
d("sparse_random_matrix(n_components, n_features, density, random_state)", "Generalized Achlioptas random sparse matrix for random projection  Setting density to 1 / 3 will yield the original matrix by Dimitris Achlioptas while setting a lower value will yield the generalization by Ping Li et al.  If we note :math:`s  1 / density`, the components of the random matrix are dr..."),]),
c("setup.py", "/setup.py<br>", [
d("configuration(parent_package, top_path)"),]),
c("_build_utils.py", "/_build_utils.py<br> Utilities useful during the build.", [
d("get_blas_info()"),]),
c("__init__.py", "/__init__.py<br> Machine learning module for Python   sklearn is a Python module integrating classical machine learning algorithms in the tightly-knit world of scientific Python packages (numpy, scipy, matplotlib).  It aims to provide simple and efficient solutions to learning problems that are accessible to everybo...", [
d("setup_module(module)", "Fixture for the tests to assure globally controllable seeding of RNGs"),]),
]});