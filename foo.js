var d = diagrams.box.generateDefinition,
  c = diagrams.box.generateContainer,
  s = diagrams.shared.get;

diagrams.box({
  name: s('project') + ' packages dependencies',
  body: [c("QDA(BaseEstimator, ClassifierMixin)", "/qda.py; Quadratic Discriminant Analysis (QDA)  A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class.  Read more in the :ref:`User Guide <lda_qda>`.  Parameters ---------- priors : array, optional, shape = [n_classes]     Priors on classes  reg_param : float, optional     Regularizes the covariance estimate as     ``(1-reg_param)*Sigma + reg_param*np.eye(n_features)``  Attributes ---------- covariances_ : list of array-like, shape = [n_features, n_features]     Covariance matrices of each class.  means_ : array-like, shape = [n_classes, n_features]     Class means.  priors_ : array-like, shape = [n_classes]     Class priors (sum to 1).  rotations_ : list of arrays     For each class k an array of shape [n_features, n_k], with     ``n_k = min(n_features, number of elements in class k)``     It is the rotation of the Gaussian distribution, i.e. its     principal axis.  scalings_ : list of arrays     For each class k an array of shape [n_k]. It contains the scaling     of the Gaussian distributions along its principal axes, i.e. the     variance in the rotated coordinate system.  Examples -------- >>> from sklearn.qda import QDA >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> clf = QDA() >>> clf.fit(X, y) QDA(priors=None, reg_param=0.0) >>> print(clf.predict([[-0.8, -1]])) [1]  See also -------- sklearn.lda.LDA: Linear discriminant analysis", [
d("__init__(self, priors, reg_param)"),
d("fit(self, X, y, store_covariances, tol)"),
d("_decision_function(self, X)"),
d("decision_function(self, X)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("BaseNB()", "/naive_bayes.py; Abstract base class for naive Bayes estimators", [
d("_joint_log_likelihood(self, X)"),
d("predict(self, X)"),
d("predict_log_proba(self, X)"),
d("predict_proba(self, X)"),]),
c("GaussianNB(BaseNB)", "/naive_bayes.py; Gaussian Naive Bayes (GaussianNB)  Can perform online updates to model parameters via `partial_fit` method. For details on algorithm used to update feature means and variance online, see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.  Attributes ---------- class_prior_ : array, shape (n_classes,)     probability of each class.  class_count_ : array, shape (n_classes,)     number of training samples observed in each class.  theta_ : array, shape (n_classes, n_features)     mean of each feature per class  sigma_ : array, shape (n_classes, n_features)     variance of each feature per class  Examples -------- >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> Y = np.array([1, 1, 1, 2, 2, 2]) >>> from sklearn.naive_bayes import GaussianNB >>> clf = GaussianNB() >>> clf.fit(X, Y) GaussianNB() >>> print(clf.predict([[-0.8, -1]])) [1] >>> clf_pf = GaussianNB() >>> clf_pf.partial_fit(X, Y, np.unique(Y)) GaussianNB() >>> print(clf_pf.predict([[-0.8, -1]])) [1]", [
d("fit(self, X, y, sample_weight)"),
d("_update_mean_variance(n_past, mu, var, X, sample_weight)"),
d("partial_fit(self, X, y, classes, sample_weight)"),
d("_partial_fit(self, X, y, classes, _refit, sample_weight)"),
d("_joint_log_likelihood(self, X)"),]),
c("BaseDiscreteNB(BaseNB)", "/naive_bayes.py; Abstract base class for naive Bayes on discrete/categorical data  Any estimator based on this class should provide:  __init__ _joint_log_likelihood(X) as per BaseNB", [
d("_update_class_log_prior(self, class_prior)"),
d("partial_fit(self, X, y, classes, sample_weight)"),
d("fit(self, X, y, sample_weight)"),
d("_get_coef(self)"),
d("_get_intercept(self)"),]),
c("MultinomialNB(BaseDiscreteNB)", "/naive_bayes.py; Naive Bayes classifier for multinomial models  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.  Parameters ---------- alpha : float, optional (default=1.0)     Additive (Laplace/Lidstone) smoothing parameter     (0 for no smoothing).  fit_prior : boolean     Whether to learn class prior probabilities or not.     If false, a uniform prior will be used.  class_prior : array-like, size (n_classes,)     Prior probabilities of the classes. If specified the priors are not     adjusted according to the data.  Attributes ---------- class_log_prior_ : array, shape (n_classes, )     Smoothed empirical log probability for each class.  intercept_ : property     Mirrors ``class_log_prior_`` for interpreting MultinomialNB     as a linear model.  feature_log_prob_ : array, shape (n_classes, n_features)     Empirical log probability of features     given a class, ``P(x_i|y)``.  coef_ : property     Mirrors ``feature_log_prob_`` for interpreting MultinomialNB     as a linear model.  class_count_ : array, shape (n_classes,)     Number of samples encountered for each class during fitting. This     value is weighted by the sample weight when provided.  feature_count_ : array, shape (n_classes, n_features)     Number of samples encountered for each (class, feature)     during fitting. This value is weighted by the sample weight when     provided.  Examples -------- >>> import numpy as np >>> X = np.random.randint(5, size=(6, 100)) >>> y = np.array([1, 2, 3, 4, 5, 6]) >>> from sklearn.naive_bayes import MultinomialNB >>> clf = MultinomialNB() >>> clf.fit(X, y) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) >>> print(clf.predict(X[2])) [3]  Notes ----- For the rationale behind the names `coef_` and `intercept_`, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.  References ---------- C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html", [
d("__init__(self, alpha, fit_prior, class_prior)"),
d("_count(self, X, Y)"),
d("_update_feature_log_prob(self)"),
d("_joint_log_likelihood(self, X)"),]),
c("BernoulliNB(BaseDiscreteNB)", "/naive_bayes.py; Naive Bayes classifier for multivariate Bernoulli models.  Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.  Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.  Parameters ---------- alpha : float, optional (default=1.0)     Additive (Laplace/Lidstone) smoothing parameter     (0 for no smoothing).  binarize : float or None, optional     Threshold for binarizing (mapping to booleans) of sample features.     If None, input is presumed to already consist of binary vectors.  fit_prior : boolean     Whether to learn class prior probabilities or not.     If false, a uniform prior will be used.  class_prior : array-like, size=[n_classes,]     Prior probabilities of the classes. If specified the priors are not     adjusted according to the data.  Attributes ---------- class_log_prior_ : array, shape = [n_classes]     Log probability of each class (smoothed).  feature_log_prob_ : array, shape = [n_classes, n_features]     Empirical log probability of features given a class, P(x_i|y).  class_count_ : array, shape = [n_classes]     Number of samples encountered for each class during fitting. This     value is weighted by the sample weight when provided.  feature_count_ : array, shape = [n_classes, n_features]     Number of samples encountered for each (class, feature)     during fitting. This value is weighted by the sample weight when     provided.  Examples -------- >>> import numpy as np >>> X = np.random.randint(2, size=(6, 100)) >>> Y = np.array([1, 2, 3, 4, 4, 5]) >>> from sklearn.naive_bayes import BernoulliNB >>> clf = BernoulliNB() >>> clf.fit(X, Y) BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) >>> print(clf.predict(X[2])) [3]  References ----------  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html  A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.  V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).", [
d("__init__(self, alpha, binarize, fit_prior, class_prior)"),
d("_count(self, X, Y)"),
d("_update_feature_log_prob(self)"),
d("_joint_log_likelihood(self, X)"),]),
c("_PartitionIterator()", "/cross_validation.py; Base class for CV iterators where train_mask = ~test_mask  Implementations must define `_iter_test_masks` or `_iter_test_indices`.  Parameters ---------- n : int     Total number of elements in dataset.", [
d("__init__(self, n)"),
d("__iter__(self)"),
d("_iter_test_masks(self)"),
d("_iter_test_indices(self)"),
d("_empty_mask(self)"),]),
c("LeaveOneOut(_PartitionIterator)", "/cross_validation.py; Leave-One-Out cross validation iterator.  Provides train/test indices to split data in train test sets. Each sample is used once as a test set (singleton) while the remaining samples form the training set.  Note: ``LeaveOneOut(n)`` is equivalent to ``KFold(n, n_folds=n)`` and ``LeavePOut(n, p=1)``.  Due to the high number of test sets (which is the same as the number of samples) this cross validation method can be very costly. For large datasets one should favor KFold, StratifiedKFold or ShuffleSplit.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- n : int     Total number of elements in dataset.  Examples -------- >>> from sklearn import cross_validation >>> X = np.array([[1, 2], [3, 4]]) >>> y = np.array([1, 2]) >>> loo = cross_validation.LeaveOneOut(2) >>> len(loo) 2 >>> print(loo) sklearn.cross_validation.LeaveOneOut(n=2) >>> for train_index, test_index in loo: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] ...    print(X_train, X_test, y_train, y_test) TRAIN: [1] TEST: [0] [[3 4]] [[1 2]] [2] [1] TRAIN: [0] TEST: [1] [[1 2]] [[3 4]] [1] [2]  See also -------- LeaveOneLabelOut for splitting the data according to explicit, domain-specific stratification of the dataset.", [
d("_iter_test_indices(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("LeavePOut(_PartitionIterator)", "/cross_validation.py; Leave-P-Out cross validation iterator  Provides train/test indices to split data in train test sets. This results in testing on all distinct samples of size p, while the remaining n - p samples form the training set in each iteration.  Note: ``LeavePOut(n, p)`` is NOT equivalent to ``KFold(n, n_folds=n // p)`` which creates non-overlapping test sets.  Due to the high number of iterations which grows combinatorically with the number of samples this cross validation method can be very costly. For large datasets one should favor KFold, StratifiedKFold or ShuffleSplit.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- n : int     Total number of elements in dataset.  p : int     Size of the test sets.  Examples -------- >>> from sklearn import cross_validation >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) >>> y = np.array([1, 2, 3, 4]) >>> lpo = cross_validation.LeavePOut(4, 2) >>> len(lpo) 6 >>> print(lpo) sklearn.cross_validation.LeavePOut(n=4, p=2) >>> for train_index, test_index in lpo: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] TRAIN: [2 3] TEST: [0 1] TRAIN: [1 3] TEST: [0 2] TRAIN: [1 2] TEST: [0 3] TRAIN: [0 3] TEST: [1 2] TRAIN: [0 2] TEST: [1 3] TRAIN: [0 1] TEST: [2 3]", [
d("__init__(self, n, p)"),
d("_iter_test_indices(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("_BaseKFold()", "/cross_validation.py; Base class to validate KFold approaches", [
d("__init__(self, n, n_folds, shuffle, random_state)"),]),
c("KFold(_BaseKFold)", "/cross_validation.py; K-Folds cross validation iterator.  Provides train/test indices to split data in train test sets. Split dataset into k consecutive folds (without shuffling).  Each fold is then used a validation set once while the k - 1 remaining fold form the training set.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- n : int     Total number of elements.  n_folds : int, default=3     Number of folds. Must be at least 2.  shuffle : boolean, optional     Whether to shuffle the data before splitting into batches.  random_state : None, int or RandomState     Pseudo-random number generator state used for random     sampling. If None, use default numpy RNG for shuffling  Examples -------- >>> from sklearn import cross_validation >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) >>> y = np.array([1, 2, 3, 4]) >>> kf = cross_validation.KFold(4, n_folds=2) >>> len(kf) 2 >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE sklearn.cross_validation.KFold(n=4, n_folds=2, shuffle=False,                                random_state=None) >>> for train_index, test_index in kf: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] TRAIN: [2 3] TEST: [0 1] TRAIN: [0 1] TEST: [2 3]  Notes ----- The first n % n_folds folds have size n // n_folds + 1, other folds have size n // n_folds.  See also -------- StratifiedKFold: take label information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks).", [
d("__init__(self, n, n_folds, shuffle, random_state)"),
d("_iter_test_indices(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("StratifiedKFold(_BaseKFold)", "/cross_validation.py; Stratified K-Folds cross validation iterator  Provides train/test indices to split data in train test sets.  This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- y : array-like, [n_samples]     Samples to split in K folds.  n_folds : int, default=3     Number of folds. Must be at least 2.  shuffle : boolean, optional     Whether to shuffle each stratification of the data before splitting     into batches.  random_state : None, int or RandomState     Pseudo-random number generator state used for random     sampling. If None, use default numpy RNG for shuffling  Examples -------- >>> from sklearn import cross_validation >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) >>> y = np.array([0, 0, 1, 1]) >>> skf = cross_validation.StratifiedKFold(y, n_folds=2) >>> len(skf) 2 >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE sklearn.cross_validation.StratifiedKFold(labels=[0 0 1 1], n_folds=2,                                          shuffle=False, random_state=None) >>> for train_index, test_index in skf: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] TRAIN: [1 3] TEST: [0 2] TRAIN: [0 2] TEST: [1 3]  Notes ----- All the folds have size trunc(n_samples / n_folds), the last one has the complementary.", [
d("__init__(self, y, n_folds, shuffle, random_state)"),
d("_iter_test_masks(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("LeaveOneLabelOut(_PartitionIterator)", "/cross_validation.py; Leave-One-Label_Out cross-validation iterator  Provides train/test indices to split data according to a third-party provided label. This label information can be used to encode arbitrary domain specific stratifications of the samples as integers.  For instance the labels could be the year of collection of the samples and thus allow for cross-validation against time-based splits.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- labels : array-like of int with shape (n_samples,)     Arbitrary domain-specific stratification of the data to be used     to draw the splits.  Examples -------- >>> from sklearn import cross_validation >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) >>> y = np.array([1, 2, 1, 2]) >>> labels = np.array([1, 1, 2, 2]) >>> lol = cross_validation.LeaveOneLabelOut(labels) >>> len(lol) 2 >>> print(lol) sklearn.cross_validation.LeaveOneLabelOut(labels=[1 1 2 2]) >>> for train_index, test_index in lol: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] ...    print(X_train, X_test, y_train, y_test) TRAIN: [2 3] TEST: [0 1] [[5 6]  [7 8]] [[1 2]  [3 4]] [1 2] [1 2] TRAIN: [0 1] TEST: [2 3] [[1 2]  [3 4]] [[5 6]  [7 8]] [1 2] [1 2]", [
d("__init__(self, labels)"),
d("_iter_test_masks(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("LeavePLabelOut(_PartitionIterator)", "/cross_validation.py; Leave-P-Label_Out cross-validation iterator  Provides train/test indices to split data according to a third-party provided label. This label information can be used to encode arbitrary domain specific stratifications of the samples as integers.  For instance the labels could be the year of collection of the samples and thus allow for cross-validation against time-based splits.  The difference between LeavePLabelOut and LeaveOneLabelOut is that the former builds the test sets with all the samples assigned to ``p`` different values of the labels while the latter uses samples all assigned the same labels.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- labels : array-like of int with shape (n_samples,)     Arbitrary domain-specific stratification of the data to be used     to draw the splits.  p : int     Number of samples to leave out in the test split.  Examples -------- >>> from sklearn import cross_validation >>> X = np.array([[1, 2], [3, 4], [5, 6]]) >>> y = np.array([1, 2, 1]) >>> labels = np.array([1, 2, 3]) >>> lpl = cross_validation.LeavePLabelOut(labels, p=2) >>> len(lpl) 3 >>> print(lpl) sklearn.cross_validation.LeavePLabelOut(labels=[1 2 3], p=2) >>> for train_index, test_index in lpl: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] ...    print(X_train, X_test, y_train, y_test) TRAIN: [2] TEST: [0 1] [[5 6]] [[1 2]  [3 4]] [1] [1 2] TRAIN: [1] TEST: [0 2] [[3 4]] [[1 2]  [5 6]] [2] [1 1] TRAIN: [0] TEST: [1 2] [[1 2]] [[3 4]  [5 6]] [1] [2 1]", [
d("__init__(self, labels, p)"),
d("_iter_test_masks(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("BaseShuffleSplit()", "/cross_validation.py; Base class for ShuffleSplit and StratifiedShuffleSplit", [
d("__init__(self, n, n_iter, test_size, train_size, random_state)"),
d("__iter__(self)"),
d("_iter_indices(self)"),]),
c("ShuffleSplit(BaseShuffleSplit)", "/cross_validation.py; Random permutation cross-validation iterator.  Yields indices to split data into training and test sets.  Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- n : int     Total number of elements in the dataset.  n_iter : int (default 10)     Number of re-shuffling & splitting iterations.  test_size : float (default 0.1), int, or None     If float, should be between 0.0 and 1.0 and represent the     proportion of the dataset to include in the test split. If     int, represents the absolute number of test samples. If None,     the value is automatically set to the complement of the train size.  train_size : float, int, or None (default is None)     If float, should be between 0.0 and 1.0 and represent the     proportion of the dataset to include in the train split. If     int, represents the absolute number of train samples. If None,     the value is automatically set to the complement of the test size.  random_state : int or RandomState     Pseudo-random number generator state used for random sampling.  Examples -------- >>> from sklearn import cross_validation >>> rs = cross_validation.ShuffleSplit(4, n_iter=3, ...     test_size=.25, random_state=0) >>> len(rs) 3 >>> print(rs) ... # doctest: +ELLIPSIS ShuffleSplit(4, n_iter=3, test_size=0.25, ...) >>> for train_index, test_index in rs: ...    print('TRAIN:', train_index, 'TEST:', test_index) ... TRAIN: [3 1 0] TEST: [2] TRAIN: [2 1 3] TEST: [0] TRAIN: [0 2 1] TEST: [3]  >>> rs = cross_validation.ShuffleSplit(4, n_iter=3, ...     train_size=0.5, test_size=.25, random_state=0) >>> for train_index, test_index in rs: ...    print('TRAIN:', train_index, 'TEST:', test_index) ... TRAIN: [3 1] TEST: [2] TRAIN: [2 1] TEST: [0] TRAIN: [0 2] TEST: [3]", [
d("_iter_indices(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("StratifiedShuffleSplit(BaseShuffleSplit)", "/cross_validation.py; Stratified ShuffleSplit cross validation iterator  Provides train/test indices to split data in train test sets.  This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.  Note: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- y : array, [n_samples]     Labels of samples.  n_iter : int (default 10)     Number of re-shuffling & splitting iterations.  test_size : float (default 0.1), int, or None     If float, should be between 0.0 and 1.0 and represent the     proportion of the dataset to include in the test split. If     int, represents the absolute number of test samples. If None,     the value is automatically set to the complement of the train size.  train_size : float, int, or None (default is None)     If float, should be between 0.0 and 1.0 and represent the     proportion of the dataset to include in the train split. If     int, represents the absolute number of train samples. If None,     the value is automatically set to the complement of the test size.  random_state : int or RandomState     Pseudo-random number generator state used for random sampling.  Examples -------- >>> from sklearn.cross_validation import StratifiedShuffleSplit >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) >>> y = np.array([0, 0, 1, 1]) >>> sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0) >>> len(sss) 3 >>> print(sss)       # doctest: +ELLIPSIS StratifiedShuffleSplit(labels=[0 0 1 1], n_iter=3, ...) >>> for train_index, test_index in sss: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] TRAIN: [1 2] TEST: [3 0] TRAIN: [0 2] TEST: [1 3] TRAIN: [0 2] TEST: [3 1]", [
d("__init__(self, y, n_iter, test_size, train_size, random_state)"),
d("_iter_indices(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("PredefinedSplit(_PartitionIterator)", "/cross_validation.py; Predefined split cross validation iterator  Splits the data into training/test set folds according to a predefined scheme. Each sample can be assigned to at most one test set fold, as specified by the user through the ``test_fold`` parameter.  Read more in the :ref:`User Guide <cross_validation>`.  Parameters ---------- test_fold : 'array-like, shape (n_samples,)     test_fold[i] gives the test set fold of sample i. A value of -1     indicates that the corresponding sample is not part of any test set     folds, but will instead always be put into the training fold.  Examples -------- >>> from sklearn.cross_validation import PredefinedSplit >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) >>> y = np.array([0, 0, 1, 1]) >>> ps = PredefinedSplit(test_fold=[0, 1, -1, 1]) >>> len(ps) 2 >>> print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS sklearn.cross_validation.PredefinedSplit(test_fold=[ 0  1 -1  1]) >>> for train_index, test_index in ps: ...    print('TRAIN:', train_index, 'TEST:', test_index) ...    X_train, X_test = X[train_index], X[test_index] ...    y_train, y_test = y[train_index], y[test_index] TRAIN: [1 2 3] TEST: [0] TRAIN: [0 2] TEST: [1 3]", [
d("__init__(self, test_fold)"),
d("_iter_test_indices(self)"),
d("__repr__(self)"),
d("__len__(self)"),]),
c("FitFailedWarning(RuntimeWarning)", "/cross_validation.py; ", []),
c("KernelRidge(BaseEstimator, RegressorMixin)", "/kernel_ridge.py; Kernel ridge regression.  Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.  The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other  hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon > 0, at prediction-time.  This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).  Read more in the :ref:`User Guide <kernel_ridge>`.  Parameters ---------- alpha : {float, array-like}, shape = [n_targets]     Small positive values of alpha improve the conditioning of the problem     and reduce the variance of the estimates.  Alpha corresponds to     ``(2*C)^-1`` in other linear models such as LogisticRegression or     LinearSVC. If an array is passed, penalties are assumed to be specific     to the targets. Hence they must correspond in number.  kernel : string or callable, default='linear'     Kernel mapping used internally. A callable should accept two arguments     and the keyword arguments passed to this object as kernel_params, and     should return a floating point number.  gamma : float, default=None     Gamma parameter for the RBF, polynomial, exponential chi2 and     sigmoid kernels. Interpretation of the default value is left to     the kernel; see the documentation for sklearn.metrics.pairwise.     Ignored by other kernels.  degree : float, default=3     Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1     Zero coefficient for polynomial and sigmoid kernels.     Ignored by other kernels.  kernel_params : mapping of string to any, optional     Additional parameters (keyword arguments) for kernel function passed     as callable object.  Attributes ---------- dual_coef_ : array, shape = [n_features] or [n_targets, n_features]     Weight vector(s) in kernel space  X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features]     Training data, which is also required for prediction  References ---------- * Kevin P. Murphy   'Machine Learning: A Probabilistic Perspective', The MIT Press   chapter 14.4.3, pp. 492-493  See also -------- Ridge     Linear ridge regression. SVR     Support Vector Regression implemented using libsvm.  Examples -------- >>> from sklearn.kernel_ridge import KernelRidge >>> import numpy as np >>> n_samples, n_features = 10, 5 >>> rng = np.random.RandomState(0) >>> y = rng.randn(n_samples) >>> X = rng.randn(n_samples, n_features) >>> clf = KernelRidge(alpha=1.0) >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel='linear',             kernel_params=None)", [
d("__init__(self, alpha, kernel, gamma, degree, coef0, kernel_params)"),
d("_get_kernel(self, X, Y)"),
d("_pairwise(self)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin)", "/isotonic.py; Isotonic regression model.  The isotonic regression optimization problem is defined by::      min sum w_i (y[i] - y_[i]) ** 2      subject to y_[i] <= y_[j] whenever X[i] <= X[j]     and min(y_) = y_min, max(y_) = y_max  where:     - ``y[i]`` are inputs (real numbers)     - ``y_[i]`` are fitted     - ``X`` specifies the order.       If ``X`` is non-decreasing then ``y_`` is non-decreasing.     - ``w[i]`` are optional strictly positive weights (default to 1.0)  Read more in the :ref:`User Guide <isotonic>`.  Parameters ---------- y_min : optional, default: None     If not None, set the lowest value of the fit to y_min.  y_max : optional, default: None     If not None, set the highest value of the fit to y_max.  increasing : boolean or string, optional, default: True     If boolean, whether or not to fit the isotonic regression with y     increasing or decreasing.      The string value 'auto' determines whether y should     increase or decrease based on the Spearman correlation estimate's     sign.  out_of_bounds : string, optional, default: 'nan'     The ``out_of_bounds`` parameter handles how x-values outside of the     training domain are handled.  When set to 'nan', predicted y-values     will be NaN.  When set to 'clip', predicted y-values will be     set to the value corresponding to the nearest train interval endpoint.     When set to 'raise', allow ``interp1d`` to throw ValueError.   Attributes ---------- X_ : ndarray (n_samples, )     A copy of the input X.  y_ : ndarray (n_samples, )     Isotonic fit of y.  X_min_ : float     Minimum value of input array `X_` for left bound.  X_max_ : float     Maximum value of input array `X_` for right bound.  f_ : function     The stepwise interpolating function that covers the domain `X_`.  Notes ----- Ties are broken using the secondary method from Leeuw, 1977.  References ---------- Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308  Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009  Correctness of Kruskal's algorithms for monotone regression with ties Leeuw, Psychometrica, 1977", [
d("__init__(self, y_min, y_max, increasing, out_of_bounds)"),
d("_check_fit_data(self, X, y, sample_weight)"),
d("_build_f(self, X, y)"),
d("_build_y(self, X, y, sample_weight)"),
d("fit(self, X, y, sample_weight)"),
d("transform(self, T)"),
d("predict(self, T)"),
d("__getstate__(self)"),
d("__setstate__(self, state)"),]),
c("_ConstantPredictor(BaseEstimator)", "/multiclass.py; ", [
d("fit(self, X, y)"),
d("predict(self, X)"),
d("decision_function(self, X)"),
d("predict_proba(self, X)"),]),
c("OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin)", "/multiclass.py; One-vs-the-rest (OvR) multiclass/multilabel strategy  Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only `n_classes` classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.  This strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.  In the multilabel learning literature, OvR is also known as the binary relevance method.  Read more in the :ref:`User Guide <ovr_classification>`.  Parameters ---------- estimator : estimator object     An estimator object implementing `fit` and one of `decision_function`     or `predict_proba`.  n_jobs : int, optional, default: 1     The number of jobs to use for the computation. If -1 all CPUs are used.     If 1 is given, no parallel computing code is used at all, which is     useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are     used. Thus for n_jobs = -2, all CPUs but one are used.  Attributes ---------- estimators_ : list of `n_classes` estimators     Estimators used for predictions.  classes_ : array, shape = [`n_classes`]     Class labels. label_binarizer_ : LabelBinarizer object     Object used to transform multiclass labels to binary labels and     vice-versa. multilabel_ : boolean     Whether a OneVsRestClassifier is a multilabel classifier.", [
d("__init__(self, estimator, n_jobs)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("decision_function(self, X)"),
d("multilabel_(self)"),
d("classes_(self)"),
d("coef_(self)"),
d("intercept_(self)"),]),
c("OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin)", "/multiclass.py; One-vs-one multiclass strategy  This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don't scale well with `n_samples`. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used `n_classes` times.  Read more in the :ref:`User Guide <ovo_classification>`.  Parameters ---------- estimator : estimator object     An estimator object implementing `fit` and one of `decision_function`     or `predict_proba`.  n_jobs : int, optional, default: 1     The number of jobs to use for the computation. If -1 all CPUs are used.     If 1 is given, no parallel computing code is used at all, which is     useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are     used. Thus for n_jobs = -2, all CPUs but one are used.  Attributes ---------- estimators_ : list of `n_classes * (n_classes - 1) / 2` estimators     Estimators used for predictions.  classes_ : numpy array of shape [n_classes]     Array containing labels.", [
d("__init__(self, estimator, n_jobs)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("decision_function(self, X)"),]),
c("OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin)", "/multiclass.py; (Error-Correcting) Output-Code multiclass strategy  Output-code based strategies consist in representing each class with a binary code (an array of 0s and 1s). At fitting time, one binary classifier per bit in the code book is fitted.  At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. The main advantage of these strategies is that the number of classifiers used can be controlled by the user, either for compressing the model (0 < code_size < 1) or for making the model more robust to errors (code_size > 1). See the documentation for more details.  Read more in the :ref:`User Guide <ecoc>`.  Parameters ---------- estimator : estimator object     An estimator object implementing `fit` and one of `decision_function`     or `predict_proba`.  code_size : float     Percentage of the number of classes to be used to create the code book.     A number between 0 and 1 will require fewer classifiers than     one-vs-the-rest. A number greater than 1 will require more classifiers     than one-vs-the-rest.  random_state : numpy.RandomState, optional     The generator used to initialize the codebook. Defaults to     numpy.random.  n_jobs : int, optional, default: 1     The number of jobs to use for the computation. If -1 all CPUs are used.     If 1 is given, no parallel computing code is used at all, which is     useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are     used. Thus for n_jobs = -2, all CPUs but one are used.  Attributes ---------- estimators_ : list of `int(n_classes * code_size)` estimators     Estimators used for predictions.  classes_ : numpy array of shape [n_classes]     Array containing labels.  code_book_ : numpy array of shape [n_classes, code_size]     Binary array containing the code of each class.  References ----------  .. [1] 'Solving multiclass learning problems via error-correcting output    codes',    Dietterich T., Bakiri G.,    Journal of Artificial Intelligence Research 2,    1995.  .. [2] 'The error coding method and PICTs',    James G., Hastie T.,    Journal of Computational and Graphical statistics 7,    1998.  .. [3] 'The Elements of Statistical Learning',    Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)    2008.", [
d("__init__(self, estimator, code_size, random_state, n_jobs)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("ParameterGrid(object)", "/grid_search.py; Grid of parameters with a discrete number of values for each.  Can be used to iterate over parameter value combinations with the Python built-in function iter.  Read more in the :ref:`User Guide <grid_search>`.  Parameters ---------- param_grid : dict of string to sequence, or sequence of such     The parameter grid to explore, as a dictionary mapping estimator     parameters to sequences of allowed values.      An empty dict signifies default parameters.      A sequence of dicts signifies a sequence of grids to search, and is     useful to avoid exploring parameter combinations that make no sense     or have no effect. See the examples below.  Examples -------- >>> from sklearn.grid_search import ParameterGrid >>> param_grid = {'a': [1, 2], 'b': [True, False]} >>> list(ParameterGrid(param_grid)) == ( ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False}, ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}]) True  >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}] >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'}, ...                               {'kernel': 'rbf', 'gamma': 1}, ...                               {'kernel': 'rbf', 'gamma': 10}] True >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1} True  See also -------- :class:`GridSearchCV`:     uses ``ParameterGrid`` to perform a full parallelized parameter search.", [
d("__init__(self, param_grid)"),
d("__iter__(self)"),
d("__len__(self)"),
d("__getitem__(self, ind)"),]),
c("ParameterSampler(object)", "/grid_search.py; Generator on parameters sampled from given distributions.  Non-deterministic iterable over random candidate combinations for hyper- parameter search. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.  Note that as of SciPy 0.12, the ``scipy.stats.distributions`` do not accept a custom RNG instance and always use the singleton RNG from ``numpy.random``. Hence setting ``random_state`` will not guarantee a deterministic iteration whenever ``scipy.stats`` distributions are used to define the parameter search space.  Read more in the :ref:`User Guide <grid_search>`.  Parameters ---------- param_distributions : dict     Dictionary where the keys are parameters and values     are distributions from which a parameter is to be sampled.     Distributions either have to provide a ``rvs`` function     to sample from them, or can be given as a list of values,     where a uniform distribution is assumed.  n_iter : integer     Number of parameter settings that are produced.  random_state : int or RandomState     Pseudo random number generator state used for random uniform sampling     from lists of possible values instead of scipy.stats distributions.  Returns ------- params : dict of string to any     **Yields** dictionaries mapping each estimator parameter to     as sampled value.  Examples -------- >>> from sklearn.grid_search import ParameterSampler >>> from scipy.stats.distributions import expon >>> import numpy as np >>> np.random.seed(0) >>> param_grid = {'a':[1, 2], 'b': expon()} >>> param_list = list(ParameterSampler(param_grid, n_iter=4)) >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items()) ...                 for d in param_list] >>> rounded_list == [{'b': 0.89856, 'a': 1}, ...                  {'b': 0.923223, 'a': 1}, ...                  {'b': 1.878964, 'a': 2}, ...                  {'b': 1.038159, 'a': 2}] True", [
d("__init__(self, param_distributions, n_iter, random_state)"),
d("__iter__(self)"),
d("__len__(self)"),]),
c("_CVScoreTuple()", "/grid_search.py; ", [
d("__repr__(self)"),]),
c("BaseSearchCV()", "/grid_search.py; Base class for hyper parameter search with cross-validation.", [
d("__init__(self, estimator, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)"),
d("_estimator_type(self)"),
d("score(self, X, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),
d("decision_function(self, X)"),
d("transform(self, X)"),
d("inverse_transform(self, Xt)"),
d("_fit(self, X, y, parameter_iterable)"),]),
c("GridSearchCV(BaseSearchCV)", "/grid_search.py; Exhaustive search over specified parameter values for an estimator.  Important members are fit, predict.  GridSearchCV implements a 'fit' method and a 'predict' method like any classifier except that the parameters of the classifier used to predict is optimized by cross-validation.  Read more in the :ref:`User Guide <grid_search>`.  Parameters ---------- estimator : object type that implements the 'fit' and 'predict' methods     A object of that type is instantiated for each grid point.  param_grid : dict or list of dictionaries     Dictionary with parameters names (string) as keys and lists of     parameter settings to try as values, or a list of such     dictionaries, in which case the grids spanned by each dictionary     in the list are explored. This enables searching over any sequence     of parameter settings.  scoring : string, callable or None, optional, default: None     A string (see model evaluation documentation) or     a scorer callable object / function with signature     ``scorer(estimator, X, y)``.  fit_params : dict, optional     Parameters to pass to the fit method.  n_jobs : int, default 1     Number of jobs to run in parallel.  pre_dispatch : int, or string, optional     Controls the number of jobs that get dispatched during parallel     execution. Reducing this number can be useful to avoid an     explosion of memory consumption when more jobs get dispatched     than CPUs can process. This parameter can be:          - None, in which case all the jobs are immediately           created and spawned. Use this for lightweight and           fast-running jobs, to avoid delays due to on-demand           spawning of the jobs          - An int, giving the exact number of total jobs that are           spawned          - A string, giving an expression as a function of n_jobs,           as in '2*n_jobs'  iid : boolean, default=True     If True, the data is assumed to be identically distributed across     the folds, and the loss minimized is the total loss per sample,     and not the mean loss across the folds.  cv : integer or cross-validation generator, default=3     If an integer is passed, it is the number of folds.     Specific cross-validation objects can be passed, see     sklearn.cross_validation module for the list of possible objects  refit : boolean, default=True     Refit the best estimator with the entire dataset.     If 'False', it is impossible to make predictions using     this GridSearchCV instance after fitting.  verbose : integer     Controls the verbosity: the higher, the more messages.  error_score : 'raise' (default) or numeric     Value to assign to the score if an error occurs in estimator fitting.     If set to 'raise', the error is raised. If a numeric value is given,     FitFailedWarning is raised. This parameter does not affect the refit     step, which will always raise the error.   Examples -------- >>> from sklearn import svm, grid_search, datasets >>> iris = datasets.load_iris() >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} >>> svr = svm.SVC() >>> clf = grid_search.GridSearchCV(svr, parameters) >>> clf.fit(iris.data, iris.target) ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS GridSearchCV(cv=None, error_score=...,        estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,                      decision_function_shape=None, degree=..., gamma=...,                      kernel='rbf', max_iter=-1, probability=False,                      random_state=None, shrinking=True, tol=...,                      verbose=False),        fit_params={}, iid=..., n_jobs=1,        param_grid=..., pre_dispatch=..., refit=...,        scoring=..., verbose=...)   Attributes ---------- grid_scores_ : list of named tuples     Contains scores for all parameter combinations in param_grid.     Each entry corresponds to one parameter setting.     Each named tuple has the attributes:          * ``parameters``, a dict of parameter settings         * ``mean_validation_score``, the mean score over the           cross-validation folds         * ``cv_validation_scores``, the list of scores for each fold  best_estimator_ : estimator     Estimator that was chosen by the search, i.e. estimator     which gave highest score (or smallest loss if specified)     on the left out data. Not available if refit=False.  best_score_ : float     Score of best_estimator on the left out data.  best_params_ : dict     Parameter setting that gave the best results on the hold out data.  scorer_ : function     Scorer function used on the held out data to choose the best     parameters for the model.  Notes ------ The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.  If `n_jobs` was set to a value higher than one, the data is copied for each point in the grid (and not `n_jobs` times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set `pre_dispatch`. Then, the memory is copied only `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 * n_jobs`.  See Also --------- :class:`ParameterGrid`:     generates all the combinations of a an hyperparameter grid.  :func:`sklearn.cross_validation.train_test_split`:     utility function to split the data into a development set usable     for fitting a GridSearchCV instance and an evaluation set for     its final evaluation.  :func:`sklearn.metrics.make_scorer`:     Make a scorer from a performance metric or loss function.", [
d("__init__(self, estimator, param_grid, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)"),
d("fit(self, X, y)"),]),
c("RandomizedSearchCV(BaseSearchCV)", "/grid_search.py; Randomized search on hyper parameters.  RandomizedSearchCV implements a 'fit' method and a 'predict' method like any classifier except that the parameters of the classifier used to predict is optimized by cross-validation.  In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.  If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.  Read more in the :ref:`User Guide <randomized_parameter_search>`.  Parameters ---------- estimator : object type that implements the 'fit' and 'predict' methods     A object of that type is instantiated for each parameter setting.  param_distributions : dict     Dictionary with parameters names (string) as keys and distributions     or lists of parameters to try. Distributions must provide a ``rvs``     method for sampling (such as those from scipy.stats.distributions).     If a list is given, it is sampled uniformly.  n_iter : int, default=10     Number of parameter settings that are sampled. n_iter trades     off runtime vs quality of the solution.  scoring : string, callable or None, optional, default: None     A string (see model evaluation documentation) or     a scorer callable object / function with signature     ``scorer(estimator, X, y)``.  fit_params : dict, optional     Parameters to pass to the fit method.  n_jobs : int, default=1     Number of jobs to run in parallel.  pre_dispatch : int, or string, optional     Controls the number of jobs that get dispatched during parallel     execution. Reducing this number can be useful to avoid an     explosion of memory consumption when more jobs get dispatched     than CPUs can process. This parameter can be:          - None, in which case all the jobs are immediately           created and spawned. Use this for lightweight and           fast-running jobs, to avoid delays due to on-demand           spawning of the jobs          - An int, giving the exact number of total jobs that are           spawned          - A string, giving an expression as a function of n_jobs,           as in '2*n_jobs'  iid : boolean, default=True     If True, the data is assumed to be identically distributed across     the folds, and the loss minimized is the total loss per sample,     and not the mean loss across the folds.  cv : integer or cross-validation generator, optional     If an integer is passed, it is the number of folds (default 3).     Specific cross-validation objects can be passed, see     sklearn.cross_validation module for the list of possible objects  refit : boolean, default=True     Refit the best estimator with the entire dataset.     If 'False', it is impossible to make predictions using     this RandomizedSearchCV instance after fitting.  verbose : integer     Controls the verbosity: the higher, the more messages.  random_state : int or RandomState     Pseudo random number generator state used for random uniform sampling     from lists of possible values instead of scipy.stats distributions.  error_score : 'raise' (default) or numeric     Value to assign to the score if an error occurs in estimator fitting.     If set to 'raise', the error is raised. If a numeric value is given,     FitFailedWarning is raised. This parameter does not affect the refit     step, which will always raise the error.   Attributes ---------- grid_scores_ : list of named tuples     Contains scores for all parameter combinations in param_grid.     Each entry corresponds to one parameter setting.     Each named tuple has the attributes:          * ``parameters``, a dict of parameter settings         * ``mean_validation_score``, the mean score over the           cross-validation folds         * ``cv_validation_scores``, the list of scores for each fold  best_estimator_ : estimator     Estimator that was chosen by the search, i.e. estimator     which gave highest score (or smallest loss if specified)     on the left out data. Not available if refit=False.  best_score_ : float     Score of best_estimator on the left out data.  best_params_ : dict     Parameter setting that gave the best results on the hold out data.  Notes ----- The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.  If `n_jobs` was set to a value higher than one, the data is copied for each parameter setting(and not `n_jobs` times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set `pre_dispatch`. Then, the memory is copied only `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 * n_jobs`.  See Also -------- :class:`GridSearchCV`:     Does exhaustive search over a grid of parameters.  :class:`ParameterSampler`:     A generator over parameter settins, constructed from     param_distributions.", [
d("__init__(self, estimator, param_distributions, n_iter, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, random_state, error_score)"),
d("fit(self, X, y)"),]),
c("DummyClassifier(BaseEstimator, ClassifierMixin)", "/dummy.py; DummyClassifier is a classifier that makes predictions using simple rules.  This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.  Parameters ---------- strategy : str     Strategy to use to generate predictions.      * 'stratified': generates predictions by respecting the training       set's class distribution.     * 'most_frequent': always predicts the most frequent label in the       training set.     * 'prior': always predicts the class that maximizes the class prior       (like 'most_frequent') and ``predict_proba`` returns the class prior.     * 'uniform': generates predictions uniformly at random.     * 'constant': always predicts a constant label that is provided by       the user. This is useful for metrics that evaluate a non-majority       class  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use.  constant : int or str or array of shape = [n_outputs]     The explicit constant as predicted by the 'constant' strategy. This     parameter is useful only for the 'constant' strategy.  Attributes ---------- classes_ : array or list of array of shape = [n_classes]     Class labels for each output.  n_classes_ : array or list of array of shape = [n_classes]     Number of label for each output.  class_prior_ : array or list of array of shape = [n_classes]     Probability of each class for each output.  n_outputs_ : int,     Number of outputs.  outputs_2d_ : bool,     True if the output at fit is 2d, else false.  sparse_output_ : bool,     True if the array returned from predict is to be in sparse CSC format.     Is automatically set to True if the input y is passed in sparse format.", [
d("__init__(self, strategy, random_state, constant)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("DummyRegressor(BaseEstimator, RegressorMixin)", "/dummy.py; DummyRegressor is a regressor that makes predictions using simple rules.  This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.  Parameters ---------- strategy : str     Strategy to use to generate predictions.      * 'mean': always predicts the mean of the training set     * 'median': always predicts the median of the training set     * 'quantile': always predicts a specified quantile of the training set,       provided with the quantile parameter.     * 'constant': always predicts a constant value that is provided by       the user.  constant : int or float or array of shape = [n_outputs]     The explicit constant as predicted by the 'constant' strategy. This     parameter is useful only for the 'constant' strategy.  quantile : float in [0.0, 1.0]     The quantile to predict using the 'quantile' strategy. A quantile of     0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the     maximum.  Attributes ---------- constant_ : float or array of shape [n_outputs]     Mean or median or quantile of the training targets or constant value     given by the user.  n_outputs_ : int,     Number of outputs.  outputs_2d_ : bool,     True if the output at fit is 2d, else false.", [
d("__init__(self, strategy, constant, quantile)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("ChangedBehaviorWarning(UserWarning)", "/base.py; ", []),
c("BaseEstimator(object)", "/base.py; Base class for all estimators in scikit-learn  Notes ----- All estimators should specify all the parameters that can be set at the class level in their ``__init__`` as explicit keyword arguments (no ``*args`` or ``**kwargs``).", [
d("_get_param_names(cls)"),
d("get_params(self, deep)"),
d("set_params(self)"),
d("__repr__(self)"),]),
c("ClassifierMixin(object)", "/base.py; Mixin class for all classifiers in scikit-learn.", [
d("score(self, X, y, sample_weight)"),]),
c("RegressorMixin(object)", "/base.py; Mixin class for all regression estimators in scikit-learn.", [
d("score(self, X, y, sample_weight)"),]),
c("ClusterMixin(object)", "/base.py; Mixin class for all cluster estimators in scikit-learn.", [
d("fit_predict(self, X, y)"),]),
c("BiclusterMixin(object)", "/base.py; Mixin class for all bicluster estimators in scikit-learn", [
d("biclusters_(self)"),
d("get_indices(self, i)"),
d("get_shape(self, i)"),
d("get_submatrix(self, i, data)"),]),
c("TransformerMixin(object)", "/base.py; Mixin class for all transformers in scikit-learn.", [
d("fit_transform(self, X, y)"),]),
c("MetaEstimatorMixin(object)", "/base.py; Mixin class for all meta estimators in scikit-learn.", []),
c("Pipeline(BaseEstimator)", "/pipeline.py; Pipeline of transforms with a final estimator.  Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be 'transforms', that is, they must implement fit and transform methods. The final estimator only needs to implement fit.  The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a '__', as in the example below.  Read more in the :ref:`User Guide <pipeline>`.  Parameters ---------- steps : list     List of (name, transform) tuples (implementing fit/transform) that are     chained, in the order in which they are chained, with the last object     an estimator.  Attributes ---------- named_steps : dict     Read-only attribute to access any step parameter by user given name.     Keys are step names and values are steps parameters.  Examples -------- >>> from sklearn import svm >>> from sklearn.datasets import samples_generator >>> from sklearn.feature_selection import SelectKBest >>> from sklearn.feature_selection import f_regression >>> from sklearn.pipeline import Pipeline >>> # generate some data to play with >>> X, y = samples_generator.make_classification( ...     n_informative=5, n_redundant=0, random_state=42) >>> # ANOVA SVM-C >>> anova_filter = SelectKBest(f_regression, k=5) >>> clf = svm.SVC(kernel='linear') >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)]) >>> # You can set the parameters using the names issued >>> # For instance, fit using a k of 10 in the SelectKBest >>> # and a parameter 'C' of the svm >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y) ...                                              # doctest: +ELLIPSIS Pipeline(steps=[...]) >>> prediction = anova_svm.predict(X) >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS 0.77... >>> # getting the selected features chosen by anova_filter >>> anova_svm.named_steps['anova'].get_support() ... # doctest: +NORMALIZE_WHITESPACE array([ True,  True,  True, False, False,  True, False,  True,  True, True,        False, False,  True, False,  True, False, False, False, False,        True], dtype=bool)", [
d("__init__(self, steps)"),
d("_estimator_type(self)"),
d("get_params(self, deep)"),
d("named_steps(self)"),
d("_final_estimator(self)"),
d("_pre_transform(self, X, y)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("predict(self, X)"),
d("fit_predict(self, X, y)"),
d("predict_proba(self, X)"),
d("decision_function(self, X)"),
d("predict_log_proba(self, X)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),
d("score(self, X, y)"),
d("classes_(self)"),
d("_pairwise(self)"),]),
c("FeatureUnion(BaseEstimator, TransformerMixin)", "/pipeline.py; Concatenates results of multiple transformer objects.  This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.  Read more in the :ref:`User Guide <feature_union>`.  Parameters ---------- transformer_list: list of (string, transformer) tuples     List of transformer objects to be applied to the data. The first     half of each tuple is the name of the transformer.  n_jobs: int, optional     Number of jobs to run in parallel (default 1).  transformer_weights: dict, optional     Multiplicative weights for features per transformer.     Keys are transformer names, values the weights.", [
d("__init__(self, transformer_list, n_jobs, transformer_weights)"),
d("get_feature_names(self)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),
d("get_params(self, deep)"),
d("_update_transformer_list(self, transformers)"),]),
c("LDA(BaseEstimator, LinearClassifierMixin, TransformerMixin)", "/lda.py; Linear Discriminant Analysis (LDA).  A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.  The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.  Read more in the :ref:`User Guide <lda_qda>`.  Parameters ---------- solver : string, optional     Solver to use, possible values:       - 'svd': Singular value decomposition (default). Does not compute the             covariance matrix, therefore this solver is recommended for             data with a large number of features.       - 'lsqr': Least squares solution, can be combined with shrinkage.       - 'eigen': Eigenvalue decomposition, can be combined with shrinkage.  shrinkage : string or float, optional     Shrinkage parameter, possible values:       - None: no shrinkage (default).       - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.       - float between 0 and 1: fixed shrinkage parameter.      Note that shrinkage works only with 'lsqr' and 'eigen' solvers.  priors : array, optional, shape (n_classes,)     Class priors.  n_components : int, optional     Number of components (< n_classes - 1) for dimensionality reduction.  store_covariance : bool, optional     Additionally compute class covariance matrix (default False).  tol : float, optional     Threshold used for rank estimation in SVD solver.  Attributes ---------- coef_ : array, shape (n_features,) or (n_classes, n_features)     Weight vector(s).  intercept_ : array, shape (n_features,)     Intercept term.  covariance_ : array-like, shape (n_features, n_features)     Covariance matrix (shared by all classes).  means_ : array-like, shape (n_classes, n_features)     Class means.  priors_ : array-like, shape (n_classes,)     Class priors (sum to 1).  scalings_ : array-like, shape (rank, n_classes - 1)     Scaling of the features in the space spanned by the class centroids.  xbar_ : array-like, shape (n_features,)     Overall mean.  classes_ : array-like, shape (n_classes,)     Unique class labels.  See also -------- sklearn.qda.QDA: Quadratic discriminant analysis  Notes ----- The default solver is 'svd'. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the 'svd' solver cannot be used with shrinkage.  The 'lsqr' solver is an efficient algorithm that only works for classification. It supports shrinkage.  The 'eigen' solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the 'eigen' solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.  Examples -------- >>> import numpy as np >>> from sklearn.lda import LDA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> clf = LDA() >>> clf.fit(X, y) LDA(n_components=None, priors=None, shrinkage=None, solver='svd',   store_covariance=False, tol=0.0001) >>> print(clf.predict([[-0.8, -1]])) [1]", [
d("__init__(self, solver, shrinkage, priors, n_components, store_covariance, tol)"),
d("_solve_lsqr(self, X, y, shrinkage)"),
d("_solve_eigen(self, X, y, shrinkage)"),
d("_solve_svd(self, X, y, store_covariance, tol)"),
d("fit(self, X, y, store_covariance, tol)"),
d("transform(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("RBFSampler(BaseEstimator, TransformerMixin)", "/kernel_approximation.py; Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.  It implements a variant of Random Kitchen Sinks.[1]  Read more in the :ref:`User Guide <rbf_kernel_approx>`.  Parameters ---------- gamma : float     Parameter of RBF kernel: exp(-gamma * x^2)  n_components : int     Number of Monte Carlo samples per original feature.     Equals the dimensionality of the computed feature space.  random_state : {int, RandomState}, optional     If int, random_state is the seed used by the random number generator;     if RandomState instance, random_state is the random number generator.  Notes ----- See 'Random Features for Large-Scale Kernel Machines' by A. Rahimi and Benjamin Recht.  [1] 'Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning' by A. Rahimi and Benjamin Recht. (http://www.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)", [
d("__init__(self, gamma, n_components, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("SkewedChi2Sampler(BaseEstimator, TransformerMixin)", "/kernel_approximation.py; Approximates feature map of the 'skewed chi-squared' kernel by Monte Carlo approximation of its Fourier transform.  Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.  Parameters ---------- skewedness : float     'skewedness' parameter of the kernel. Needs to be cross-validated.  n_components : int     number of Monte Carlo samples per original feature.     Equals the dimensionality of the computed feature space.  random_state : {int, RandomState}, optional     If int, random_state is the seed used by the random number generator;     if RandomState instance, random_state is the random number generator.  References ---------- See 'Random Fourier Approximations for Skewed Multiplicative Histogram Kernels' by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.  See also -------- AdditiveChi2Sampler : A different approach for approximating an additive     variant of the chi squared kernel.  sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.", [
d("__init__(self, skewedness, n_components, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("AdditiveChi2Sampler(BaseEstimator, TransformerMixin)", "/kernel_approximation.py; Approximate feature map for additive chi2 kernel.  Uses sampling the fourier transform of the kernel characteristic at regular intervals.  Since the kernel that is to be approximated is additive, the components of the input vectors can be treated separately.  Each entry in the original space is transformed into 2*sample_steps+1 features, where sample_steps is a parameter of the method. Typical values of sample_steps include 1, 2 and 3.  Optimal choices for the sampling interval for certain data ranges can be computed (see the reference). The default values should be reasonable.  Read more in the :ref:`User Guide <additive_chi_kernel_approx>`.  Parameters ---------- sample_steps : int, optional     Gives the number of (complex) sampling points. sample_interval : float, optional     Sampling interval. Must be specified when sample_steps not in {1,2,3}.  Notes ----- This estimator approximates a slightly different version of the additive chi squared kernel then ``metric.additive_chi2`` computes.  See also -------- SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of     the chi squared kernel.  sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.  sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi     squared kernel.  References ---------- See `'Efficient additive kernels via explicit feature maps' <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_ A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, 2011", [
d("__init__(self, sample_steps, sample_interval)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),
d("_transform_dense(self, X)"),
d("_transform_sparse(self, X)"),]),
c("Nystroem(BaseEstimator, TransformerMixin)", "/kernel_approximation.py; Approximate a kernel map using a subset of the training data.  Constructs an approximate feature map for an arbitrary kernel using a subset of the data as basis.  Read more in the :ref:`User Guide <nystroem_kernel_approx>`.  Parameters ---------- kernel : string or callable, default='rbf'     Kernel map to be approximated. A callable should accept two arguments     and the keyword arguments passed to this object as kernel_params, and     should return a floating point number.  n_components : int     Number of features to construct.     How many data points will be used to construct the mapping.  gamma : float, default=None     Gamma parameter for the RBF, polynomial, exponential chi2 and     sigmoid kernels. Interpretation of the default value is left to     the kernel; see the documentation for sklearn.metrics.pairwise.     Ignored by other kernels.  degree : float, default=3     Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1     Zero coefficient for polynomial and sigmoid kernels.     Ignored by other kernels.  kernel_params : mapping of string to any, optional     Additional parameters (keyword arguments) for kernel function passed     as callable object.  random_state : {int, RandomState}, optional     If int, random_state is the seed used by the random number generator;     if RandomState instance, random_state is the random number generator.   Attributes ---------- components_ : array, shape (n_components, n_features)     Subset of training points used to construct the feature map.  component_indices_ : array, shape (n_components)     Indices of ``components_`` in the training set.  normalization_ : array, shape (n_components, n_components)     Normalization matrix needed for embedding.     Square root of the kernel matrix on ``components_``.   References ---------- * Williams, C.K.I. and Seeger, M.   'Using the Nystroem method to speed up kernel machines',   Advances in neural information processing systems 2001  * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou   'Nystroem Method vs Random Fourier Features: A Theoretical and Empirical   Comparison',   Advances in Neural Information Processing Systems 2012   See also -------- RBFSampler : An approximation to the RBF kernel using random Fourier              features.  sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.", [
d("__init__(self, kernel, gamma, coef0, degree, kernel_params, n_components, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X)"),
d("_get_kernel_params(self)"),]),
c("CalibratedClassifierCV(BaseEstimator, ClassifierMixin)", "/calibration.py; Probability calibration with isotonic regression or sigmoid.  With this class, the base_estimator is fit on the train set of the cross-validation generator and the test set is used for calibration. The probabilities for each of the folds are then averaged for prediction. In case that cv='prefit' is passed to __init__, it is it is assumed that base_estimator has been fitted already and all data is used for calibration. Note that data for fitting the classifier and for calibrating it must be disjpint.  Read more in the :ref:`User Guide <calibration>`.  Parameters ---------- base_estimator : instance BaseEstimator     The classifier whose output decision function needs to be calibrated     to offer more accurate predict_proba outputs. If cv=prefit, the     classifier must have been fit already on data.  method : 'sigmoid' | 'isotonic'     The method to use for calibration. Can be 'sigmoid' which     corresponds to Platt's method or 'isotonic' which is a     non-parameteric approach. It is not advised to use isotonic calibration     with too few calibration samples (<<1000) since it tends to overfit.     Use sigmoids (Platt's calibration) in this case.  cv : integer or cross-validation generator or 'prefit', optional     If an integer is passed, it is the number of folds (default 3).     Specific cross-validation objects can be passed, see     sklearn.cross_validation module for the list of possible objects.     If 'prefit' is passed, it is assumed that base_estimator has been     fitted already and all data is used for calibration.  Attributes ---------- classes_ : array, shape (n_classes)     The class labels.  calibrated_classifiers_: list (len() equal to cv or 1 if cv == 'prefit')     The list of calibrated classifiers, one for each crossvalidation fold,     which has been fitted on all but the validation fold and calibrated     on the validation fold.  References ---------- .. [1] Obtaining calibrated probability estimates from decision trees        and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001  .. [2] Transforming Classifier Scores into Accurate Multiclass        Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)  .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to        Regularized Likelihood Methods, J. Platt, (1999)  .. [4] Predicting Good Probabilities with Supervised Learning,        A. Niculescu-Mizil & R. Caruana, ICML 2005", [
d("__init__(self, base_estimator, method, cv)"),
d("fit(self, X, y, sample_weight)"),
d("predict_proba(self, X)"),
d("predict(self, X)"),]),
c("_CalibratedClassifier(object)", "/calibration.py; Probability calibration with isotonic regression or sigmoid.  It assumes that base_estimator has already been fit, and trains the calibration on the input set of the fit function. Note that this class should not be used as an estimator directly. Use CalibratedClassifierCV with cv='prefit' instead.  Parameters ---------- base_estimator : instance BaseEstimator     The classifier whose output decision function needs to be calibrated     to offer more accurate predict_proba outputs. No default value since     it has to be an already fitted estimator.  method : 'sigmoid' | 'isotonic'     The method to use for calibration. Can be 'sigmoid' which     corresponds to Platt's method or 'isotonic' which is a     non-parameteric approach based on isotonic regression.  References ---------- .. [1] Obtaining calibrated probability estimates from decision trees        and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001  .. [2] Transforming Classifier Scores into Accurate Multiclass        Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)  .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to        Regularized Likelihood Methods, J. Platt, (1999)  .. [4] Predicting Good Probabilities with Supervised Learning,        A. Niculescu-Mizil & R. Caruana, ICML 2005", [
d("__init__(self, base_estimator, method)"),
d("_preproc(self, X)"),
d("fit(self, X, y, sample_weight)"),
d("predict_proba(self, X)"),]),
c("_SigmoidCalibration(BaseEstimator, RegressorMixin)", "/calibration.py; Sigmoid regression model.  Attributes ---------- a_ : float     The slope.  b_ : float     The intercept.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, T)"),]),
c("BaseRandomProjection()", "/random_projection.py; Base class for random projections.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, n_components, eps, dense_output, random_state)"),
d("_make_random_matrix(n_components, n_features)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("GaussianRandomProjection(BaseRandomProjection)", "/random_projection.py; Reduce dimensionality through Gaussian random projection  The components of the random matrix are drawn from N(0, 1 / n_components).  Read more in the :ref:`User Guide <gaussian_random_matrix>`.  Parameters ---------- n_components : int or 'auto', optional (default = 'auto')     Dimensionality of the target projection space.      n_components can be automatically adjusted according to the     number of samples in the dataset and the bound given by the     Johnson-Lindenstrauss lemma. In that case the quality of the     embedding is controlled by the ``eps`` parameter.      It should be noted that Johnson-Lindenstrauss lemma can yield     very conservative estimated of the required number of components     as it makes no assumption on the structure of the dataset.  eps : strictly positive float, optional (default=0.1)     Parameter to control the quality of the embedding according to     the Johnson-Lindenstrauss lemma when n_components is set to     'auto'.      Smaller values lead to better embedding and higher number of     dimensions (n_components) in the target projection space.  random_state : integer, RandomState instance or None (default=None)     Control the pseudo random number generator used to generate the     matrix at fit time.  Attributes ---------- n_component_ : int     Concrete number of components computed when n_components='auto'.  components_ : numpy array of shape [n_components, n_features]     Random matrix used for the projection.  See Also -------- SparseRandomProjection", [
d("__init__(self, n_components, eps, random_state)"),
d("_make_random_matrix(self, n_components, n_features)"),]),
c("SparseRandomProjection(BaseRandomProjection)", "/random_projection.py; Reduce dimensionality through sparse random projection  Sparse random matrix is an alternative to dense random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.  If we note `s = 1 / density` the components of the random matrix are drawn from:    - -sqrt(s) / sqrt(n_components)   with probability 1 / 2s   -  0                              with probability 1 - 1 / s   - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s  Read more in the :ref:`User Guide <sparse_random_matrix>`.  Parameters ---------- n_components : int or 'auto', optional (default = 'auto')     Dimensionality of the target projection space.      n_components can be automatically adjusted according to the     number of samples in the dataset and the bound given by the     Johnson-Lindenstrauss lemma. In that case the quality of the     embedding is controlled by the ``eps`` parameter.      It should be noted that Johnson-Lindenstrauss lemma can yield     very conservative estimated of the required number of components     as it makes no assumption on the structure of the dataset.  density : float in range ]0, 1], optional (default='auto')     Ratio of non-zero component in the random projection matrix.      If density = 'auto', the value is set to the minimum density     as recommended by Ping Li et al.: 1 / sqrt(n_features).      Use density = 1 / 3.0 if you want to reproduce the results from     Achlioptas, 2001.  eps : strictly positive float, optional, (default=0.1)     Parameter to control the quality of the embedding according to     the Johnson-Lindenstrauss lemma when n_components is set to     'auto'.      Smaller values lead to better embedding and higher number of     dimensions (n_components) in the target projection space.  dense_output : boolean, optional (default=False)     If True, ensure that the output of the random projection is a     dense numpy array even if the input and random projection matrix     are both sparse. In practice, if the number of components is     small the number of zero components in the projected data will     be very small and it will be more CPU and memory efficient to     use a dense representation.      If False, the projected data uses a sparse representation if     the input is sparse.  random_state : integer, RandomState instance or None (default=None)     Control the pseudo random number generator used to generate the     matrix at fit time.  Attributes ---------- n_component_ : int     Concrete number of components computed when n_components='auto'.  components_ : CSR matrix with shape [n_components, n_features]     Random matrix used for the projection.  density_ : float in range 0.0 - 1.0     Concrete density computed from when density = 'auto'.  See Also -------- GaussianRandomProjection  References ----------  .. [1] Ping Li, T. Hastie and K. W. Church, 2006,        'Very Sparse Random Projections'.        http://www.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf  .. [2] D. Achlioptas, 2001, 'Database-friendly random projections',        http://www.cs.ucsc.edu/~optas/papers/jl.pdf", [
d("__init__(self, n_components, density, eps, dense_output, random_state)"),
d("_make_random_matrix(self, n_components, n_features)"),]),
c("GaussianProcess(BaseEstimator, RegressorMixin)", "/gaussian_process/gaussian_process.py; The Gaussian Process model class.  Read more in the :ref:`User Guide <gaussian_process>`.  Parameters ---------- regr : string or callable, optional     A regression function returning an array of outputs of the linear     regression functional basis. The number of observations n_samples     should be greater than the size p of this basis.     Default assumes a simple constant regression trend.     Available built-in regression models are::          'constant', 'linear', 'quadratic'  corr : string or callable, optional     A stationary autocorrelation function returning the autocorrelation     between two points x and x'.     Default assumes a squared-exponential autocorrelation model.     Built-in correlation models are::          'absolute_exponential', 'squared_exponential',         'generalized_exponential', 'cubic', 'linear'  beta0 : double array_like, optional     The regression weight vector to perform Ordinary Kriging (OK).     Default assumes Universal Kriging (UK) so that the vector beta of     regression weights is estimated using the maximum likelihood     principle.  storage_mode : string, optional     A string specifying whether the Cholesky decomposition of the     correlation matrix should be stored in the class (storage_mode =     'full') or not (storage_mode = 'light').     Default assumes storage_mode = 'full', so that the     Cholesky decomposition of the correlation matrix is stored.     This might be a useful parameter when one is not interested in the     MSE and only plan to estimate the BLUP, for which the correlation     matrix is not required.  verbose : boolean, optional     A boolean specifying the verbose level.     Default is verbose = False.  theta0 : double array_like, optional     An array with shape (n_features, ) or (1, ).     The parameters in the autocorrelation model.     If thetaL and thetaU are also specified, theta0 is considered as     the starting point for the maximum likelihood estimation of the     best set of parameters.     Default assumes isotropic autocorrelation model with theta0 = 1e-1.  thetaL : double array_like, optional     An array with shape matching theta0's.     Lower bound on the autocorrelation parameters for maximum     likelihood estimation.     Default is None, so that it skips maximum likelihood estimation and     it uses theta0.  thetaU : double array_like, optional     An array with shape matching theta0's.     Upper bound on the autocorrelation parameters for maximum     likelihood estimation.     Default is None, so that it skips maximum likelihood estimation and     it uses theta0.  normalize : boolean, optional     Input X and observations y are centered and reduced wrt     means and standard deviations estimated from the n_samples     observations provided.     Default is normalize = True so that data is normalized to ease     maximum likelihood estimation.  nugget : double or ndarray, optional     Introduce a nugget effect to allow smooth predictions from noisy     data.  If nugget is an ndarray, it must be the same length as the     number of data points used for the fit.     The nugget is added to the diagonal of the assumed training covariance;     in this way it acts as a Tikhonov regularization in the problem.  In     the special case of the squared exponential correlation function, the     nugget mathematically represents the variance of the input values.     Default assumes a nugget close to machine precision for the sake of     robustness (nugget = 10. * MACHINE_EPSILON).  optimizer : string, optional     A string specifying the optimization algorithm to be used.     Default uses 'fmin_cobyla' algorithm from scipy.optimize.     Available optimizers are::          'fmin_cobyla', 'Welch'      'Welch' optimizer is dued to Welch et al., see reference [WBSWM1992]_.     It consists in iterating over several one-dimensional optimizations     instead of running one single multi-dimensional optimization.  random_start : int, optional     The number of times the Maximum Likelihood Estimation should be     performed from a random starting point.     The first MLE always uses the specified starting point (theta0),     the next starting points are picked at random according to an     exponential distribution (log-uniform on [thetaL, thetaU]).     Default does not use random starting point (random_start = 1).  random_state: integer or numpy.RandomState, optional     The generator used to shuffle the sequence of coordinates of theta in     the Welch optimizer. If an integer is given, it fixes the seed.     Defaults to the global numpy random number generator.   Attributes ---------- theta_ : array     Specified theta OR the best set of autocorrelation parameters (the         sought maximizer of the reduced likelihood function).  reduced_likelihood_function_value_ : array     The optimal reduced likelihood function value.  Examples -------- >>> import numpy as np >>> from sklearn.gaussian_process import GaussianProcess >>> X = np.array([[1., 3., 5., 6., 7., 8.]]).T >>> y = (X * np.sin(X)).ravel() >>> gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.) >>> gp.fit(X, y)                                      # doctest: +ELLIPSIS GaussianProcess(beta0=None...         ...  Notes ----- The presentation implementation is based on a translation of the DACE Matlab toolbox, see reference [NLNS2002]_.  References ----------  .. [NLNS2002] `H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.     Sondergaard.  DACE - A MATLAB Kriging Toolbox.` (2002)     http://www2.imm.dtu.dk/~hbn/dace/dace.pdf  .. [WBSWM1992] `W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,     and M.D.  Morris (1992). Screening, predicting, and computer     experiments.  Technometrics, 34(1) 15--25.`     http://www.jstor.org/pss/1269548", [
d("__init__(self, regr, corr, beta0, storage_mode, verbose, theta0, thetaL, thetaU, optimizer, random_start, normalize, nugget, random_state)"),
d("fit(self, X, y)"),
d("predict(self, X, eval_MSE, batch_size)"),
d("reduced_likelihood_function(self, theta)"),
d("_arg_max_reduced_likelihood_function(self)"),
d("_check_params(self, n_samples)"),]),
c("Imputer(BaseEstimator, TransformerMixin)", "/preprocessing/imputation.py; Imputation transformer for completing missing values.  Read more in the :ref:`User Guide <imputation>`.  Parameters ---------- missing_values : integer or 'NaN', optional (default='NaN')     The placeholder for the missing values. All occurrences of     `missing_values` will be imputed. For missing values encoded as np.nan,     use the string value 'NaN'.  strategy : string, optional (default='mean')     The imputation strategy.      - If 'mean', then replace missing values using the mean along       the axis.     - If 'median', then replace missing values using the median along       the axis.     - If 'most_frequent', then replace missing using the most frequent       value along the axis.  axis : integer, optional (default=0)     The axis along which to impute.      - If `axis=0`, then impute along columns.     - If `axis=1`, then impute along rows.  verbose : integer, optional (default=0)     Controls the verbosity of the imputer.  copy : boolean, optional (default=True)     If True, a copy of X will be created. If False, imputation will     be done in-place whenever possible. Note that, in the following cases,     a new copy will always be made, even if `copy=False`:      - If X is not an array of floating values;     - If X is sparse and `missing_values=0`;     - If `axis=0` and X is encoded as a CSR matrix;     - If `axis=1` and X is encoded as a CSC matrix.  Attributes ---------- statistics_ : array of shape (n_features,)     The imputation fill value for each feature if axis == 0.  Notes ----- - When ``axis=0``, columns which only contained missing values at `fit`   are discarded upon `transform`. - When ``axis=1``, an exception is raised if there are rows for which it is   not possible to fill in the missing values (e.g., because they only   contain missing values).", [
d("__init__(self, missing_values, strategy, axis, verbose, copy)"),
d("fit(self, X, y)"),
d("_sparse_fit(self, X, strategy, missing_values, axis)"),
d("_dense_fit(self, X, strategy, missing_values, axis)"),
d("transform(self, X)"),]),
c("MinMaxScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Transforms features by scaling each feature to a given range.  This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.  The transformation is given by::      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))     X_scaled = X_std * (max - min) + min  where min, max = feature_range.  This transformation is often used as an alternative to zero mean, unit variance scaling.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- feature_range: tuple (min, max), default=(0, 1)     Desired range of transformed data.  copy : boolean, optional, default True     Set to False to perform inplace row normalization and avoid a     copy (if the input is already a numpy array).  Attributes ---------- min_ : ndarray, shape (n_features,)     Per feature adjustment for minimum.  scale_ : ndarray, shape (n_features,)     Per feature relative scaling of the data.", [
d("__init__(self, feature_range, copy)"),
d("fit(self, X, y)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),]),
c("StandardScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Standardize features by removing the mean and scaling to unit variance  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using the `transform` method.  Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).  For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- with_mean : boolean, True by default     If True, center the data before scaling.     This does not work (and will raise an exception) when attempted on     sparse matrices, because centering them entails building a dense     matrix which in common use cases is likely to be too large to fit in     memory.  with_std : boolean, True by default     If True, scale the data to unit variance (or equivalently,     unit standard deviation).  copy : boolean, optional, default True     If False, try to avoid a copy and do inplace scaling instead.     This is not guaranteed to always work inplace; e.g. if the data is     not a NumPy array or scipy.sparse CSR matrix, a copy may still be     returned.  Attributes ---------- mean_ : array of floats with shape [n_features]     The mean value for each feature in the training set.  std_ : array of floats with shape [n_features]     The standard deviation for each feature in the training set.     Set to one if the standard deviation is zero for a given feature.  See also -------- :func:`sklearn.preprocessing.scale` to perform centering and scaling without using the ``Transformer`` object oriented API  :class:`sklearn.decomposition.RandomizedPCA` with `whiten=True` to further remove the linear correlation across features.", [
d("__init__(self, copy, with_mean, with_std)"),
d("fit(self, X, y)"),
d("transform(self, X, y, copy)"),
d("inverse_transform(self, X, copy)"),]),
c("MaxAbsScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Scale each feature by its maximum absolute value.  This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.  This scaler can also be applied to sparse CSR or CSC matrices.  Parameters ---------- copy : boolean, optional, default is True     Set to False to perform inplace scaling and avoid a copy (if the input     is already a numpy array).  Attributes ---------- scale_ : ndarray, shape (n_features,)     Per feature relative scaling of the data.", [
d("__init__(self, copy)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),
d("inverse_transform(self, X)"),]),
c("RobustScaler(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Scale features using statistics that are robust to outliers.  This Scaler removes the median and scales the data according to the Interquartile Range (IQR). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).  Centering and scaling happen independently on each feature (or each sample, depending on the `axis` argument) by computing the relevant statistics on the samples in the training set. Median and  interquartile range are then stored to be used on later data using the `transform` method.  Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- with_centering : boolean, True by default     If True, center the data before scaling.     This does not work (and will raise an exception) when attempted on     sparse matrices, because centering them entails building a dense     matrix which in common use cases is likely to be too large to fit in     memory.  with_scaling : boolean, True by default     If True, scale the data to interquartile range.  copy : boolean, optional, default is True     If False, try to avoid a copy and do inplace scaling instead.     This is not guaranteed to always work inplace; e.g. if the data is     not a NumPy array or scipy.sparse CSR matrix, a copy may still be     returned.  Attributes ---------- center_ : array of floats     The median value for each feature in the training set.  scale_ : array of floats     The (scaled) interquartile range for each feature in the training set.  See also -------- :class:`sklearn.preprocessing.StandardScaler` to perform centering and scaling using mean and variance.  :class:`sklearn.decomposition.RandomizedPCA` with `whiten=True` to further remove the linear correlation across features.  Notes ----- See examples/preprocessing/plot_robust_scaling.py for an example.  http://en.wikipedia.org/wiki/Median_(statistics) http://en.wikipedia.org/wiki/Interquartile_range", [
d("__init__(self, with_centering, with_scaling, copy)"),
d("_check_array(self, X, copy)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),
d("inverse_transform(self, X)"),]),
c("PolynomialFeatures(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Generate polynomial and interaction features.  Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].  Parameters ---------- degree : integer     The degree of the polynomial features. Default = 2.  interaction_only : boolean, default = False     If true, only interaction features are produced: features that are     products of at most ``degree`` *distinct* input features (so not     ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).  include_bias : boolean     If True (default), then include a bias column, the feature in which     all polynomial powers are zero (i.e. a column of ones - acts as an     intercept term in a linear model).  Examples -------- >>> X = np.arange(6).reshape(3, 2) >>> X array([[0, 1],        [2, 3],        [4, 5]]) >>> poly = PolynomialFeatures(2) >>> poly.fit_transform(X) array([[ 1,  0,  1,  0,  0,  1],        [ 1,  2,  3,  4,  6,  9],        [ 1,  4,  5, 16, 20, 25]]) >>> poly = PolynomialFeatures(interaction_only=True) >>> poly.fit_transform(X) array([[ 1,  0,  1,  0],        [ 1,  2,  3,  6],        [ 1,  4,  5, 20]])  Attributes ---------- powers_ : array, shape (n_input_features, n_output_features)     powers_[i, j] is the exponent of the jth input in the ith output.  n_input_features_ : int     The total number of input features.  n_output_features_ : int     The total number of polynomial output features. The number of output     features is computed by iterating over all suitably sized combinations     of input features.  Notes ----- Be aware that the number of features in the output array scales polynomially in the number of features of the input array, and exponentially in the degree. High degrees can cause overfitting.  See :ref:`examples/linear_model/plot_polynomial_interpolation.py <example_linear_model_plot_polynomial_interpolation.py>`", [
d("__init__(self, degree, interaction_only, include_bias)"),
d("_combinations(n_features, degree, interaction_only, include_bias)"),
d("powers_(self)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("Normalizer(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Normalize samples individually to unit norm.  Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one.  This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion).  Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.  Read more in the :ref:`User Guide <preprocessing_normalization>`.  Parameters ---------- norm : 'l1', 'l2', or 'max', optional ('l2' by default)     The norm to use to normalize each non zero sample.  copy : boolean, optional, default True     set to False to perform inplace row normalization and avoid a     copy (if the input is already a numpy array or a scipy.sparse     CSR matrix).  Notes ----- This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.  See also -------- :func:`sklearn.preprocessing.normalize` equivalent function without the object oriented API", [
d("__init__(self, norm, copy)"),
d("fit(self, X, y)"),
d("transform(self, X, y, copy)"),]),
c("Binarizer(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Binarize data (set feature values to 0 or 1) according to a threshold  Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.  Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.  It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).  Read more in the :ref:`User Guide <preprocessing_binarization>`.  Parameters ---------- threshold : float, optional (0.0 by default)     Feature values below or equal to this are replaced by 0, above it by 1.     Threshold may not be less than 0 for operations on sparse matrices.  copy : boolean, optional, default True     set to False to perform inplace binarization and avoid a copy (if     the input is already a numpy array or a scipy.sparse CSR matrix).  Notes ----- If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.  This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.", [
d("__init__(self, threshold, copy)"),
d("fit(self, X, y)"),
d("transform(self, X, y, copy)"),]),
c("KernelCenterer(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Center a kernel matrix  Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).  Read more in the :ref:`User Guide <kernel_centering>`.", [
d("fit(self, K, y)"),
d("transform(self, K, y, copy)"),]),
c("OneHotEncoder(BaseEstimator, TransformerMixin)", "/preprocessing/data.py; Encode categorical integer features using a one-hot aka one-of-K scheme.  The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix where each column corresponds to one possible value of one feature. It is assumed that input features take on values in the range [0, n_values).  This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.  Parameters ---------- n_values : 'auto', int or array of ints     Number of values per feature.      - 'auto' : determine value range from training data.     - int : maximum value for all features.     - array : maximum value per feature.  categorical_features: 'all' or array of indices or mask     Specify what features are treated as categorical.      - 'all' (default): All features are treated as categorical.     - array of indices: Array of categorical feature indices.     - mask: Array of length n_features and with dtype=bool.      Non-categorical features are always stacked to the right of the matrix.  dtype : number type, default=np.float     Desired dtype of output.  sparse : boolean, default=True     Will return sparse matrix if set True else will return an array.  handle_unknown : str, 'error' or 'ignore'     Whether to raise an error or ignore if a unknown categorical feature is     present during transform.  Attributes ---------- active_features_ : array     Indices for active features, meaning values that actually occur     in the training set. Only available when n_values is ``'auto'``.  feature_indices_ : array of shape (n_features,)     Indices to feature ranges.     Feature ``i`` in the original data is mapped to features     from ``feature_indices_[i]`` to ``feature_indices_[i+1]``     (and then potentially masked by `active_features_` afterwards)  n_values_ : array of shape (n_features,)     Maximum number of values per feature.  Examples -------- Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding.  >>> from sklearn.preprocessing import OneHotEncoder >>> enc = OneHotEncoder() >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  # doctest: +ELLIPSIS OneHotEncoder(categorical_features='all', dtype=<... 'float'>,        handle_unknown='error', n_values='auto', sparse=True) >>> enc.n_values_ array([2, 3, 4]) >>> enc.feature_indices_ array([0, 2, 5, 9]) >>> enc.transform([[0, 1, 1]]).toarray() array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])  See also -------- sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of   dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot   encoding of dictionary items or strings.", [
d("__init__(self, n_values, categorical_features, dtype, sparse, handle_unknown)"),
d("fit(self, X, y)"),
d("_fit_transform(self, X)"),
d("fit_transform(self, X, y)"),
d("_transform(self, X)"),
d("transform(self, X)"),]),
c("FunctionTransformer(BaseEstimator, TransformerMixin)", "/preprocessing/_function_transformer.py; Constructs a transformer from an arbitrary callable.  A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. This is useful for stateless transformations such as taking the log of frequencies, doing custom scaling, etc.  A FunctionTransformer will not do any checks on its function's output.  Note: If a lambda is used as the function, then the resulting transformer will not be pickleable.  Parameters ---------- func : callable, optional default=None     The callable to use for the transformation. This will be passed     the same arguments as transform, with args and kwargs forwarded.     If func is None, then func will be the identity function.  validate : bool, optional default=True     Indicate that the input X array should be checked before calling     func. If validate is false, there will be no input validation.     If it is true, then X will be converted to a 2-dimensional NumPy     array or sparse matrix. If this conversion is not possible or X     contains NaN or infinity, an exception is raised.  accept_sparse : boolean, optional     Indicate that func accepts a sparse matrix as input. If validate is     False, this has no effect. Otherwise, if accept_sparse is false,     sparse matrix inputs will cause an exception to be raised.  pass_y: bool, optional default=False     Indicate that transform should forward the y argument to the     inner callable.", [
d("__init__(self, func, validate, accept_sparse, pass_y)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),]),
c("LabelEncoder(BaseEstimator, TransformerMixin)", "/preprocessing/label.py; Encode labels with value between 0 and n_classes-1.  Read more in the :ref:`User Guide <preprocessing_targets>`.  Attributes ---------- classes_ : array of shape (n_class,)     Holds the label for each class.  Examples -------- `LabelEncoder` can be used to normalize labels.  >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) #doctest: +ELLIPSIS array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6])  It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.  >>> le = preprocessing.LabelEncoder() >>> le.fit(['paris', 'paris', 'tokyo', 'amsterdam']) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform(['tokyo', 'tokyo', 'paris']) #doctest: +ELLIPSIS array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris']", [
d("fit(self, y)"),
d("fit_transform(self, y)"),
d("transform(self, y)"),
d("inverse_transform(self, y)"),]),
c("LabelBinarizer(BaseEstimator, TransformerMixin)", "/preprocessing/label.py; Binarize labels in a one-vs-all fashion  Several regression and binary classification algorithms are available in the scikit. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.  At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.  Read more in the :ref:`User Guide <preprocessing_targets>`.  Parameters ----------  neg_label : int (default: 0)     Value with which negative labels must be encoded.  pos_label : int (default: 1)     Value with which positive labels must be encoded.  sparse_output : boolean (default: False)     True if the returned array from transform is desired to be in sparse     CSR format.  Attributes ----------  classes_ : array of shape [n_class]     Holds the label for each class.  y_type_ : str,     Represents the type of the target data as evaluated by     utils.multiclass.type_of_target. Possible type are 'continuous',     'continuous-multioutput', 'binary', 'multiclass',     'mutliclass-multioutput', 'multilabel-indicator', and 'unknown'.  multilabel_ : boolean     True if the transformer was fitted on a multilabel rather than a     multiclass set of labels. The ``multilabel_`` attribute is deprecated     and will be removed in 0.18  sparse_input_ : boolean,     True if the input data to transform is given as a sparse matrix, False     otherwise.  indicator_matrix_ : str     'sparse' when the input data to tansform is a multilable-indicator and     is sparse, None otherwise. The ``indicator_matrix_`` attribute is     deprecated as of version 0.16 and will be removed in 0.18   Examples -------- >>> from sklearn import preprocessing >>> lb = preprocessing.LabelBinarizer() >>> lb.fit([1, 2, 6, 4, 2]) LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) >>> lb.classes_ array([1, 2, 4, 6]) >>> lb.transform([1, 6]) array([[1, 0, 0, 0],        [0, 0, 0, 1]])  Binary targets transform to a column vector  >>> lb = preprocessing.LabelBinarizer() >>> lb.fit_transform(['yes', 'no', 'no', 'yes']) array([[1],        [0],        [0],        [1]])  Passing a 2D matrix for multilabel classification  >>> import numpy as np >>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]])) LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) >>> lb.classes_ array([0, 1, 2]) >>> lb.transform([0, 1, 2, 1]) array([[1, 0, 0],        [0, 1, 0],        [0, 0, 1],        [0, 1, 0]])  See also -------- label_binarize : function to perform the transform operation of     LabelBinarizer with fixed classes.", [
d("__init__(self, neg_label, pos_label, sparse_output)"),
d("fit(self, y)"),
d("transform(self, y)"),
d("inverse_transform(self, Y, threshold)"),]),
c("MultiLabelBinarizer(BaseEstimator, TransformerMixin)", "/preprocessing/label.py; Transform between iterable of iterables and a multilabel format  Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class label.  Parameters ---------- classes : array-like of shape [n_classes] (optional)     Indicates an ordering for the class labels  sparse_output : boolean (default: False),     Set to true if output binary array is desired in CSR sparse format  Attributes ---------- classes_ : array of labels     A copy of the `classes` parameter where provided,     or otherwise, the sorted set of classes found when fitting.  Examples -------- >>> mlb = MultiLabelBinarizer() >>> mlb.fit_transform([(1, 2), (3,)]) array([[1, 1, 0],        [0, 0, 1]]) >>> mlb.classes_ array([1, 2, 3])  >>> mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])]) array([[0, 1, 1],        [1, 0, 0]]) >>> list(mlb.classes_) ['comedy', 'sci-fi', 'thriller']", [
d("__init__(self, classes, sparse_output)"),
d("fit(self, y)"),
d("fit_transform(self, y)"),
d("transform(self, y)"),
d("_transform(self, y, class_mapping)"),
d("inverse_transform(self, yt)"),]),
c("MyEstimator(BaseEstimator)", "/tests/test_base.py; ", [
d("__init__(self, l1, empty)"),]),
c("K(BaseEstimator)", "/tests/test_base.py; ", [
d("__init__(self, c, d)"),]),
c("T(BaseEstimator)", "/tests/test_base.py; ", [
d("__init__(self, a, b)"),]),
c("DeprecatedAttributeEstimator(BaseEstimator)", "/tests/test_base.py; ", [
d("__init__(self, a, b)"),
d("b(self)"),]),
c("Buggy(BaseEstimator)", "/tests/test_base.py; A buggy estimator that does not set its parameters right. ", [
d("__init__(self, a)"),]),
c("NoEstimator(object)", "/tests/test_base.py; ", [
d("__init__(self)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("VargEstimator(BaseEstimator)", "/tests/test_base.py; Sklearn estimators shouldn't have vargs.", [
d("__init__(self)"),]),
c("MockClassifier(object)", "/tests/test_grid_search.py; Dummy classifier to test the cross-validation", [
d("__init__(self, foo_param)"),
d("fit(self, X, Y)"),
d("predict(self, T)"),
d("score(self, X, Y)"),
d("get_params(self, deep)"),
d("set_params(self)"),]),
c("LinearSVCNoScore(LinearSVC)", "/tests/test_grid_search.py; An LinearSVC classifier that has no score method.", [
d("score(self)"),]),
c("BrokenClassifier(BaseEstimator)", "/tests/test_grid_search.py; Broken classifier that cannot be fit twice", [
d("__init__(self, parameter)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("FailingClassifier(BaseEstimator)", "/tests/test_grid_search.py; Classifier that raises a ValueError on fit()", [
d("__init__(self, parameter)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("MockClassifier(object)", "/tests/test_cross_validation.py; Dummy classifier to test the cross-validation", [
d("__init__(self, a, allow_nd)"),
d("fit(self, X, Y, sample_weight, class_prior, sparse_sample_weight, sparse_param, dummy_int, dummy_str, dummy_obj, callback)"),
d("predict(self, T)"),
d("score(self, X, Y)"),
d("get_params(self, deep)"),]),
c("IncorrectT(object)", "/tests/test_pipeline.py; Small class to test parameter dispatching.     ", [
d("__init__(self, a, b)"),]),
c("T(IncorrectT)", "/tests/test_pipeline.py; ", [
d("fit(self, X, y)"),
d("get_params(self, deep)"),
d("set_params(self)"),]),
c("TransfT(T)", "/tests/test_pipeline.py; ", [
d("transform(self, X, y)"),]),
c("FitParamT(object)", "/tests/test_pipeline.py; Mock classifier     ", [
d("__init__(self)"),
d("fit(self, X, y, should_succeed)"),
d("predict(self, X)"),]),
c("MockImprovingEstimator(BaseEstimator)", "/tests/test_learning_curve.py; Dummy classifier to test the learning curve", [
d("__init__(self, n_max_train_sizes)"),
d("fit(self, X_subset, y_subset)"),
d("predict(self, X)"),
d("score(self, X, Y)"),
d("_is_training_data(self, X)"),]),
c("MockIncrementalImprovingEstimator(MockImprovingEstimator)", "/tests/test_learning_curve.py; Dummy classifier that provides partial_fit", [
d("__init__(self, n_max_train_sizes)"),
d("_is_training_data(self, X)"),
d("partial_fit(self, X, y)"),]),
c("MockEstimatorWithParameter(BaseEstimator)", "/tests/test_learning_curve.py; Dummy classifier to test the validation curve", [
d("__init__(self, param)"),
d("fit(self, X_subset, y_subset)"),
d("predict(self, X)"),
d("score(self, X, y)"),
d("_is_training_data(self, X)"),]),
c("DelegatorData(object)", "/tests/test_metaestimators.py; ", [
d("__init__(self, name, construct, skip_methods, fit_args)"),]),
c("MDS(BaseEstimator)", "/manifold/mds.py; Multidimensional scaling  Read more in the :ref:`User Guide <multidimensional_scaling>`.  Parameters ---------- metric : boolean, optional, default: True     compute metric or nonmetric SMACOF (Scaling by Majorizing a     Complicated Function) algorithm  n_components : int, optional, default: 2     number of dimension in which to immerse the similarities     overridden if initial array is provided.  n_init : int, optional, default: 4     Number of time the smacof algorithm will be run with different     initialisation. The final results will be the best output of the     n_init consecutive runs in terms of stress.  max_iter : int, optional, default: 300     Maximum number of iterations of the SMACOF algorithm for a single run  verbose : int, optional, default: 0     level of verbosity  eps : float, optional, default: 1e-6     relative tolerance w.r.t stress to declare converge  n_jobs : int, optional, default: 1     The number of jobs to use for the computation. This works by breaking     down the pairwise matrix into n_jobs even slices and computing them in     parallel.      If -1 all CPUs are used. If 1 is given, no parallel computing code is     used at all, which is useful for debugging. For n_jobs below -1,     (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one     are used.  random_state : integer or numpy.RandomState, optional     The generator used to initialize the centers. If an integer is     given, it fixes the seed. Defaults to the global numpy random     number generator.  dissimilarity : string     Which dissimilarity measure to use.     Supported are 'euclidean' and 'precomputed'.   Attributes ---------- embedding_ : array-like, shape [n_components, n_samples]     Stores the position of the dataset in the embedding space  stress_ : float     The final value of the stress (sum of squared distance of the     disparities and the distances for all constrained points)   References ---------- 'Modern Multidimensional Scaling - Theory and Applications' Borg, I.; Groenen P. Springer Series in Statistics (1997)  'Nonmetric multidimensional scaling: a numerical method' Kruskal, J. Psychometrika, 29 (1964)  'Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis' Kruskal, J. Psychometrika, 29, (1964)", [
d("__init__(self, n_components, metric, n_init, max_iter, verbose, eps, n_jobs, random_state, dissimilarity)"),
d("_pairwise(self)"),
d("fit(self, X, y, init)"),
d("fit_transform(self, X, y, init)"),]),
c("TSNE(BaseEstimator)", "/manifold/t_sne.py; t-distributed Stochastic Neighbor Embedding.  t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.  It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten's FAQ [2].  Read more in the :ref:`User Guide <t_sne>`.  Parameters ---------- n_components : int, optional (default: 2)     Dimension of the embedded space.  perplexity : float, optional (default: 30)     The perplexity is related to the number of nearest neighbors that     is used in other manifold learning algorithms. Larger datasets     usually require a larger perplexity. Consider selcting a value     between 5 and 50. The choice is not extremely critical since t-SNE     is quite insensitive to this parameter.  early_exaggeration : float, optional (default: 4.0)     Controls how tight natural clusters in the original space are in     the embedded space and how much space will be between them. For     larger values, the space between natural clusters will be larger     in the embedded space. Again, the choice of this parameter is not     very critical. If the cost function increases during initial     optimization, the early exaggeration factor or the learning rate     might be too high.  learning_rate : float, optional (default: 1000)     The learning rate can be a critical parameter. It should be     between 100 and 1000. If the cost function increases during initial     optimization, the early exaggeration factor or the learning rate     might be too high. If the cost function gets stuck in a bad local     minimum increasing the learning rate helps sometimes.  n_iter : int, optional (default: 1000)     Maximum number of iterations for the optimization. Should be at     least 200.  metric : string or callable, optional     The metric to use when calculating distance between instances in a     feature array. If metric is a string, it must be one of the options     allowed by scipy.spatial.distance.pdist for its metric parameter, or     a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.     If metric is 'precomputed', X is assumed to be a distance matrix.     Alternatively, if metric is a callable function, it is called on each     pair of instances (rows) and the resulting value recorded. The callable     should take two arrays from X as input and return a value indicating     the distance between them. The default is 'euclidean' which is     interpreted as squared euclidean distance.  init : string, optional (default: 'random')     Initialization of embedding. Possible options are 'random' and 'pca'.     PCA initialization cannot be used with precomputed distances and is     usually more globally stable than random initialization.  verbose : int, optional (default: 0)     Verbosity level.  random_state : int or RandomState instance or None (default)     Pseudo Random Number generator seed control. If None, use the     numpy.random singleton. Note that different initializations     might result in different local minima of the cost function.  Attributes ---------- embedding_ : array-like, shape (n_samples, n_components)     Stores the embedding vectors.  training_data_ : array-like, shape (n_samples, n_features)     Stores the training data.  Examples --------  >>> import numpy as np >>> from sklearn.manifold import TSNE >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]) >>> model = TSNE(n_components=2, random_state=0) >>> model.fit_transform(X) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array([[  887.28...,   238.61...],        [ -714.79...,  3243.34...],        [  957.30..., -2505.78...],        [-1130.28...,  -974.78...])  References ----------  [1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data     Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.  [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding     http://homepage.tudelft.nl/19j49/t-SNE.html", [
d("__init__(self, n_components, perplexity, early_exaggeration, learning_rate, n_iter, metric, init, verbose, random_state)"),
d("fit(self, X, y)"),
d("_tsne(self, P, alpha, n_samples, random_state, X_embedded)"),
d("fit_transform(self, X, y)"),]),
c("LocallyLinearEmbedding(BaseEstimator, TransformerMixin)", "/manifold/locally_linear.py; Locally Linear Embedding  Read more in the :ref:`User Guide <locally_linear_embedding>`.  Parameters ---------- n_neighbors : integer     number of neighbors to consider for each point.  n_components : integer     number of coordinates for the manifold  reg : float     regularization constant, multiplies the trace of the local covariance     matrix of the distances.  eigen_solver : string, {'auto', 'arpack', 'dense'}     auto : algorithm will attempt to choose the best method for input data      arpack : use arnoldi iteration in shift-invert mode.                 For this method, M may be a dense matrix, sparse matrix,                 or general linear operator.                 Warning: ARPACK can be unstable for some problems.  It is                 best to try several random seeds in order to check results.      dense  : use standard dense matrix operations for the eigenvalue                 decomposition.  For this method, M must be an array                 or matrix type.  This method should be avoided for                 large problems.  tol : float, optional     Tolerance for 'arpack' method     Not used if eigen_solver=='dense'.  max_iter : integer     maximum number of iterations for the arpack solver.     Not used if eigen_solver=='dense'.  method : string ('standard', 'hessian', 'modified' or 'ltsa')     standard : use the standard locally linear embedding algorithm.  see                reference [1]     hessian  : use the Hessian eigenmap method. This method requires                ``n_neighbors > n_components * (1 + (n_components + 1) / 2``                see reference [2]     modified : use the modified locally linear embedding algorithm.                see reference [3]     ltsa     : use local tangent space alignment algorithm                see reference [4]  hessian_tol : float, optional     Tolerance for Hessian eigenmapping method.     Only used if ``method == 'hessian'``  modified_tol : float, optional     Tolerance for modified LLE method.     Only used if ``method == 'modified'``  neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']     algorithm to use for nearest neighbors search,     passed to neighbors.NearestNeighbors instance  random_state: numpy.RandomState or int, optional     The generator or seed used to determine the starting vector for arpack     iterations.  Defaults to numpy.random.  Attributes ---------- embedding_vectors_ : array-like, shape [n_components, n_samples]     Stores the embedding vectors  reconstruction_error_ : float     Reconstruction error associated with `embedding_vectors_`  nbrs_ : NearestNeighbors object     Stores nearest neighbors instance, including BallTree or KDtree     if applicable.  References ----------  .. [1] `Roweis, S. & Saul, L. Nonlinear dimensionality reduction     by locally linear embedding.  Science 290:2323 (2000).` .. [2] `Donoho, D. & Grimes, C. Hessian eigenmaps: Locally     linear embedding techniques for high-dimensional data.     Proc Natl Acad Sci U S A.  100:5591 (2003).` .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear     Embedding Using Multiple Weights.`     http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382 .. [4] `Zhang, Z. & Zha, H. Principal manifolds and nonlinear     dimensionality reduction via tangent space alignment.     Journal of Shanghai Univ.  8:406 (2004)`", [
d("__init__(self, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, neighbors_algorithm, random_state)"),
d("_fit_transform(self, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),]),
c("SpectralEmbedding(BaseEstimator)", "/manifold/spectral_embedding_.py; Spectral embedding for non-linear dimensionality reduction.  Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.  Read more in the :ref:`User Guide <spectral_embedding>`.  Parameters ----------- n_components : integer, default: 2     The dimension of the projected subspace.  eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}     The eigenvalue decomposition strategy to use. AMG requires pyamg     to be installed. It can be faster on very large, sparse problems,     but may also lead to instabilities.  random_state : int seed, RandomState instance, or None, default : None     A pseudo random number generator used for the initialization of the     lobpcg eigenvectors decomposition when eigen_solver == 'amg'.  affinity : string or callable, default : 'nearest_neighbors'     How to construct the affinity matrix.      - 'nearest_neighbors' : construct affinity matrix by knn graph      - 'rbf' : construct affinity matrix by rbf kernel      - 'precomputed' : interpret X as precomputed affinity matrix      - callable : use passed in function as affinity        the function takes in data matrix (n_samples, n_features)        and return affinity matrix (n_samples, n_samples).  gamma : float, optional, default : 1/n_features     Kernel coefficient for rbf kernel.  n_neighbors : int, default : max(n_samples/10 , 1)     Number of nearest neighbors for nearest_neighbors graph building.  Attributes ----------  embedding_ : array, shape = (n_samples, n_components)     Spectral embedding of the training matrix.  affinity_matrix_ : array, shape = (n_samples, n_samples)     Affinity_matrix constructed from samples or precomputed.  References ----------  - A Tutorial on Spectral Clustering, 2007   Ulrike von Luxburg   http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323  - On Spectral Clustering: Analysis and an algorithm, 2011   Andrew Y. Ng, Michael I. Jordan, Yair Weiss   http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100  - Normalized cuts and image segmentation, 2000   Jianbo Shi, Jitendra Malik   http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324", [
d("__init__(self, n_components, affinity, gamma, random_state, eigen_solver, n_neighbors)"),
d("_pairwise(self)"),
d("_get_affinity_matrix(self, X, Y)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),]),
c("Isomap(BaseEstimator, TransformerMixin)", "/manifold/isomap.py; Isomap Embedding  Non-linear dimensionality reduction through Isometric Mapping  Read more in the :ref:`User Guide <isomap>`.  Parameters ---------- n_neighbors : integer     number of neighbors to consider for each point.  n_components : integer     number of coordinates for the manifold  eigen_solver : ['auto'|'arpack'|'dense']     'auto' : Attempt to choose the most efficient solver     for the given problem.      'arpack' : Use Arnoldi decomposition to find the eigenvalues     and eigenvectors.      'dense' : Use a direct solver (i.e. LAPACK)     for the eigenvalue decomposition.  tol : float     Convergence tolerance passed to arpack or lobpcg.     not used if eigen_solver == 'dense'.  max_iter : integer     Maximum number of iterations for the arpack solver.     not used if eigen_solver == 'dense'.  path_method : string ['auto'|'FW'|'D']     Method to use in finding shortest path.      'auto' : attempt to choose the best algorithm automatically.      'FW' : Floyd-Warshall algorithm.      'D' : Dijkstra's algorithm.  neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']     Algorithm to use for nearest neighbors search,     passed to neighbors.NearestNeighbors instance.  Attributes ---------- embedding_ : array-like, shape (n_samples, n_components)     Stores the embedding vectors.  kernel_pca_ : object     `KernelPCA` object used to implement the embedding.  training_data_ : array-like, shape (n_samples, n_features)     Stores the training data.  nbrs_ : sklearn.neighbors.NearestNeighbors instance     Stores nearest neighbors instance, including BallTree or KDtree     if applicable.  dist_matrix_ : array-like, shape (n_samples, n_samples)     Stores the geodesic distance matrix of training data.  References ----------  .. [1] Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. A global geometric        framework for nonlinear dimensionality reduction. Science 290 (5500)", [
d("__init__(self, n_neighbors, n_components, eigen_solver, tol, max_iter, path_method, neighbors_algorithm)"),
d("_fit_transform(self, X)"),
d("reconstruction_error(self)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),]),
c("DPGMM(GMM)", "/mixture/dpgmm.py; Variational Inference for the Infinite Gaussian Mixture Model.  DPGMM stands for Dirichlet Process Gaussian Mixture Model, and it is an infinite mixture model with the Dirichlet Process as a prior distribution on the number of clusters. In practice the approximate inference algorithm uses a truncated distribution with a fixed maximum number of components, but almost always the number of components actually used depends on the data.  Stick-breaking Representation of a Gaussian mixture model probability distribution. This class allows for easy and efficient inference of an approximate posterior distribution over the parameters of a Gaussian mixture model with a variable number of components (smaller than the truncation parameter n_components).  Initialization is with normally-distributed means and identity covariance, for proper convergence.  Read more in the :ref:`User Guide <dpgmm>`.  Parameters ---------- n_components: int, default 1     Number of mixture components.  covariance_type: string, default 'diag'     String describing the type of covariance parameters to     use.  Must be one of 'spherical', 'tied', 'diag', 'full'.  alpha: float, default 1     Real number representing the concentration parameter of     the dirichlet process. Intuitively, the Dirichlet Process     is as likely to start a new cluster for a point as it is     to add that point to a cluster with alpha elements. A     higher alpha means more clusters, as the expected number     of clusters is ``alpha*log(N)``.  tol : float, default 1e-3     Convergence threshold.  n_iter : int, default 10     Maximum number of iterations to perform before convergence.  params : string, default 'wmc'     Controls which parameters are updated in the training     process.  Can contain any combination of 'w' for weights,     'm' for means, and 'c' for covars.  init_params : string, default 'wmc'     Controls which parameters are updated in the initialization     process.  Can contain any combination of 'w' for weights,     'm' for means, and 'c' for covars.  Defaults to 'wmc'.  verbose : int, default 0     Controls output verbosity.  Attributes ---------- covariance_type : string     String describing the type of covariance parameters used by     the DP-GMM.  Must be one of 'spherical', 'tied', 'diag', 'full'.  n_components : int     Number of mixture components.  weights_ : array, shape (`n_components`,)     Mixing weights for each mixture component.  means_ : array, shape (`n_components`, `n_features`)     Mean parameters for each mixture component.  precs_ : array     Precision (inverse covariance) parameters for each mixture     component.  The shape depends on `covariance_type`::          (`n_components`, 'n_features')                if 'spherical',         (`n_features`, `n_features`)                  if 'tied',         (`n_components`, `n_features`)                if 'diag',         (`n_components`, `n_features`, `n_features`)  if 'full'  converged_ : bool     True when convergence was reached in fit(), False otherwise.  See Also -------- GMM : Finite Gaussian mixture model fit with EM  VBGMM : Finite Gaussian mixture model fit with a variational     algorithm, better for situations where there might be too little     data to get a good estimate of the covariance matrix.", [
d("__init__(self, n_components, covariance_type, alpha, random_state, thresh, tol, verbose, min_covar, n_iter, params, init_params)"),
d("_get_precisions(self)"),
d("_get_covars(self)"),
d("_set_covars(self, covars)"),
d("score_samples(self, X)"),
d("_update_concentration(self, z)"),
d("_update_means(self, X, z)"),
d("_update_precisions(self, X, z)"),
d("_monitor(self, X, z, n, end)"),
d("_do_mstep(self, X, z, params)"),
d("_initialize_gamma(self)"),
d("_bound_concentration(self)"),
d("_bound_means(self)"),
d("_bound_precisions(self)"),
d("_bound_proportions(self, z)"),
d("_logprior(self, z)"),
d("lower_bound(self, X, z)"),
d("_set_weights(self)"),
d("_fit(self, X, y)"),]),
c("VBGMM(DPGMM)", "/mixture/dpgmm.py; Variational Inference for the Gaussian Mixture Model  Variational inference for a Gaussian mixture model probability distribution. This class allows for easy and efficient inference of an approximate posterior distribution over the parameters of a Gaussian mixture model with a fixed number of components.  Initialization is with normally-distributed means and identity covariance, for proper convergence.  Read more in the :ref:`User Guide <vbgmm>`.  Parameters ---------- n_components: int, default 1     Number of mixture components.  covariance_type: string, default 'diag'     String describing the type of covariance parameters to     use.  Must be one of 'spherical', 'tied', 'diag', 'full'.  alpha: float, default 1     Real number representing the concentration parameter of     the dirichlet distribution. Intuitively, the higher the     value of alpha the more likely the variational mixture of     Gaussians model will use all components it can.  tol : float, default 1e-3     Convergence threshold.  n_iter : int, default 10     Maximum number of iterations to perform before convergence.  params : string, default 'wmc'     Controls which parameters are updated in the training     process.  Can contain any combination of 'w' for weights,     'm' for means, and 'c' for covars.  init_params : string, default 'wmc'     Controls which parameters are updated in the initialization     process.  Can contain any combination of 'w' for weights,     'm' for means, and 'c' for covars.  Defaults to 'wmc'.  verbose : int, default 0     Controls output verbosity.  Attributes ---------- covariance_type : string     String describing the type of covariance parameters used by     the DP-GMM.  Must be one of 'spherical', 'tied', 'diag', 'full'.  n_features : int     Dimensionality of the Gaussians.  n_components : int (read-only)     Number of mixture components.  weights_ : array, shape (`n_components`,)     Mixing weights for each mixture component.  means_ : array, shape (`n_components`, `n_features`)     Mean parameters for each mixture component.  precs_ : array     Precision (inverse covariance) parameters for each mixture     component.  The shape depends on `covariance_type`::          (`n_components`, 'n_features')                if 'spherical',         (`n_features`, `n_features`)                  if 'tied',         (`n_components`, `n_features`)                if 'diag',         (`n_components`, `n_features`, `n_features`)  if 'full'  converged_ : bool     True when convergence was reached in fit(), False     otherwise.  See Also -------- GMM : Finite Gaussian mixture model fit with EM DPGMM : Infinite Gaussian mixture model, using the dirichlet     process, fit with a variational algorithm", [
d("__init__(self, n_components, covariance_type, alpha, random_state, thresh, tol, verbose, min_covar, n_iter, params, init_params)"),
d("score_samples(self, X)"),
d("_update_concentration(self, z)"),
d("_initialize_gamma(self)"),
d("_bound_proportions(self, z)"),
d("_bound_concentration(self)"),
d("_monitor(self, X, z, n, end)"),
d("_set_weights(self)"),]),
c("GMM(BaseEstimator)", "/mixture/gmm.py; Gaussian Mixture Model  Representation of a Gaussian mixture model probability distribution. This class allows for easy evaluation of, sampling from, and maximum-likelihood estimation of the parameters of a GMM distribution.  Initializes parameters such that every mixture component has zero mean and identity covariance.  Read more in the :ref:`User Guide <gmm>`.  Parameters ---------- n_components : int, optional     Number of mixture components. Defaults to 1.  covariance_type : string, optional     String describing the type of covariance parameters to     use.  Must be one of 'spherical', 'tied', 'diag', 'full'.     Defaults to 'diag'.  random_state: RandomState or an int seed (None by default)     A random number generator instance  min_covar : float, optional     Floor on the diagonal of the covariance matrix to prevent     overfitting.  Defaults to 1e-3.  tol : float, optional     Convergence threshold. EM iterations will stop when average     gain in log-likelihood is below this threshold.  Defaults to 1e-3.  n_iter : int, optional     Number of EM iterations to perform.  n_init : int, optional     Number of initializations to perform. the best results is kept  params : string, optional     Controls which parameters are updated in the training     process.  Can contain any combination of 'w' for weights,     'm' for means, and 'c' for covars.  Defaults to 'wmc'.  init_params : string, optional     Controls which parameters are updated in the initialization     process.  Can contain any combination of 'w' for weights,     'm' for means, and 'c' for covars.  Defaults to 'wmc'.  verbose : int, default: 0     Enable verbose output. If 1 then it always prints the current     initialization and iteration step. If greater than 1 then     it prints additionally the change and time needed for each step.  Attributes ---------- weights_ : array, shape (`n_components`,)     This attribute stores the mixing weights for each mixture component.  means_ : array, shape (`n_components`, `n_features`)     Mean parameters for each mixture component.  covars_ : array     Covariance parameters for each mixture component.  The shape     depends on `covariance_type`::          (n_components, n_features)             if 'spherical',         (n_features, n_features)               if 'tied',         (n_components, n_features)             if 'diag',         (n_components, n_features, n_features) if 'full'  converged_ : bool     True when convergence was reached in fit(), False otherwise.  See Also --------  DPGMM : Infinite gaussian mixture model, using the dirichlet     process, fit with a variational algorithm   VBGMM : Finite gaussian mixture model fit with a variational     algorithm, better for situations where there might be too little     data to get a good estimate of the covariance matrix.  Examples --------  >>> import numpy as np >>> from sklearn import mixture >>> np.random.seed(1) >>> g = mixture.GMM(n_components=2) >>> # Generate random observations with two modes centered on 0 >>> # and 10 to use for training. >>> obs = np.concatenate((np.random.randn(100, 1), ...                       10 + np.random.randn(300, 1))) >>> g.fit(obs) # doctest: +NORMALIZE_WHITESPACE GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,         n_components=2, n_init=1, n_iter=100, params='wmc',         random_state=None, thresh=None, tol=0.001, verbose=0) >>> np.round(g.weights_, 2) array([ 0.75,  0.25]) >>> np.round(g.means_, 2) array([[ 10.05],        [  0.06]]) >>> np.round(g.covars_, 2) #doctest: +SKIP array([[[ 1.02]],        [[ 0.96]]]) >>> g.predict([[0], [2], [9], [10]]) #doctest: +ELLIPSIS array([1, 1, 0, 0]...) >>> np.round(g.score([[0], [2], [9], [10]]), 2) array([-2.19, -4.58, -1.75, -1.21]) >>> # Refit the model on new data (initial parameters remain the >>> # same), this time with an even split between the two modes. >>> g.fit(20 * [[0]] +  20 * [[10]]) # doctest: +NORMALIZE_WHITESPACE GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,         n_components=2, n_init=1, n_iter=100, params='wmc',         random_state=None, thresh=None, tol=0.001, verbose=0) >>> np.round(g.weights_, 2) array([ 0.5,  0.5])", [
d("__init__(self, n_components, covariance_type, random_state, thresh, tol, min_covar, n_iter, n_init, params, init_params, verbose)"),
d("_get_covars(self)"),
d("_set_covars(self, covars)"),
d("score_samples(self, X)"),
d("score(self, X, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("sample(self, n_samples, random_state)"),
d("fit_predict(self, X, y)"),
d("_fit(self, X, y, do_prediction)"),
d("fit(self, X, y)"),
d("_do_mstep(self, X, responsibilities, params, min_covar)"),
d("_n_parameters(self)"),
d("bic(self, X)"),
d("aic(self, X)"),]),
c("GMMTester()", "/mixture/tests/test_gmm.py; ", [
d("_setUp(self)"),
d("test_eval(self)"),
d("test_sample(self, n)"),
d("test_train(self, params)"),
d("test_train_degenerate(self, params)"),
d("test_train_1d(self, params)"),
d("score(self, g, X)"),]),
c("TestGMMWithSphericalCovars(GMMTester)", "/mixture/tests/test_gmm.py; ", []),
c("TestGMMWithDiagonalCovars(GMMTester)", "/mixture/tests/test_gmm.py; ", []),
c("TestGMMWithTiedCovars(GMMTester)", "/mixture/tests/test_gmm.py; ", []),
c("TestGMMWithFullCovars(GMMTester)", "/mixture/tests/test_gmm.py; ", []),
c("DPGMMTester(GMMTester)", "/mixture/tests/test_dpgmm.py; ", [
d("score(self, g, train_obs)"),]),
c("TestDPGMMWithSphericalCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("TestDPGMMWithDiagCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("TestDPGMMWithTiedCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("TestDPGMMWithFullCovars(DPGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("VBGMMTester(GMMTester)", "/mixture/tests/test_dpgmm.py; ", [
d("score(self, g, train_obs)"),]),
c("TestVBGMMWithSphericalCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("TestVBGMMWithDiagCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("TestVBGMMWithTiedCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("TestVBGMMWithFullCovars(VBGMMTester)", "/mixture/tests/test_dpgmm.py; ", []),
c("SparseCodingMixin(TransformerMixin)", "/decomposition/dict_learning.py; Sparse coding mixin", [
d("_set_sparse_coding_params(self, n_components, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs)"),
d("transform(self, X, y)"),]),
c("SparseCoder(BaseEstimator, SparseCodingMixin)", "/decomposition/dict_learning.py; Sparse coding  Finds a sparse representation of data against a fixed, precomputed dictionary.  Each row of the result is the solution to a sparse coding problem. The goal is to find a sparse array `code` such that::      X ~= code * dictionary  Read more in the :ref:`User Guide <SparseCoder>`.  Parameters ---------- dictionary : array, [n_components, n_features]     The dictionary atoms used for sparse coding. Lines are assumed to be     normalized to unit norm.  transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'}     Algorithm used to transform the data:     lars: uses the least angle regression method (linear_model.lars_path)     lasso_lars: uses Lars to compute the Lasso solution     lasso_cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). lasso_lars will be faster if     the estimated components are sparse.     omp: uses orthogonal matching pursuit to estimate the sparse solution     threshold: squashes to zero all coefficients less than alpha from     the projection ``dictionary * X'``  transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default     Number of nonzero coefficients to target in each column of the     solution. This is only used by `algorithm='lars'` and `algorithm='omp'`     and is overridden by `alpha` in the `omp` case.  transform_alpha : float, 1. by default     If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the     penalty applied to the L1 norm.     If `algorithm='threshold'`, `alpha` is the absolute value of the     threshold below which coefficients will be squashed to zero.     If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of     the reconstruction error targeted. In this case, it overrides     `n_nonzero_coefs`.  split_sign : bool, False by default     Whether to split the sparse feature vector into the concatenation of     its negative part and its positive part. This can improve the     performance of downstream classifiers.  n_jobs : int,     number of parallel jobs to run  Attributes ---------- components_ : array, [n_components, n_features]     The unchanged dictionary atoms  See also -------- DictionaryLearning MiniBatchDictionaryLearning SparsePCA MiniBatchSparsePCA sparse_encode", [
d("__init__(self, dictionary, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, split_sign, n_jobs)"),
d("fit(self, X, y)"),]),
c("DictionaryLearning(BaseEstimator, SparseCodingMixin)", "/decomposition/dict_learning.py; Dictionary learning  Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.  Solves the optimization problem::      (U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1                 (U,V)                 with || V_k ||_2 = 1 for all  0 <= k < n_components  Read more in the :ref:`User Guide <DictionaryLearning>`.  Parameters ---------- n_components : int,     number of dictionary elements to extract  alpha : float,     sparsity controlling parameter  max_iter : int,     maximum number of iterations to perform  tol : float,     tolerance for numerical error  fit_algorithm : {'lars', 'cd'}     lars: uses the least angle regression method to solve the lasso problem     (linear_model.lars_path)     cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). Lars will be faster if     the estimated components are sparse.  transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'}     Algorithm used to transform the data     lars: uses the least angle regression method (linear_model.lars_path)     lasso_lars: uses Lars to compute the Lasso solution     lasso_cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). lasso_lars will be faster if     the estimated components are sparse.     omp: uses orthogonal matching pursuit to estimate the sparse solution     threshold: squashes to zero all coefficients less than alpha from     the projection ``dictionary * X'``  transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default     Number of nonzero coefficients to target in each column of the     solution. This is only used by `algorithm='lars'` and `algorithm='omp'`     and is overridden by `alpha` in the `omp` case.  transform_alpha : float, 1. by default     If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the     penalty applied to the L1 norm.     If `algorithm='threshold'`, `alpha` is the absolute value of the     threshold below which coefficients will be squashed to zero.     If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of     the reconstruction error targeted. In this case, it overrides     `n_nonzero_coefs`.  split_sign : bool, False by default     Whether to split the sparse feature vector into the concatenation of     its negative part and its positive part. This can improve the     performance of downstream classifiers.  n_jobs : int,     number of parallel jobs to run  code_init : array of shape (n_samples, n_components),     initial value for the code, for warm restart  dict_init : array of shape (n_components, n_features),     initial values for the dictionary, for warm restart  verbose :     degree of verbosity of the printed output  random_state : int or RandomState     Pseudo number generator state used for random sampling.  Attributes ---------- components_ : array, [n_components, n_features]     dictionary atoms extracted from the data  error_ : array     vector of errors at each iteration  n_iter_ : int     Number of iterations run.  Notes ----- **References:**  J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (http://www.di.ens.fr/sierra/pdfs/icml09.pdf)  See also -------- SparseCoder MiniBatchDictionaryLearning SparsePCA MiniBatchSparsePCA", [
d("__init__(self, n_components, alpha, max_iter, tol, fit_algorithm, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, n_jobs, code_init, dict_init, verbose, split_sign, random_state)"),
d("fit(self, X, y)"),]),
c("MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin)", "/decomposition/dict_learning.py; Mini-batch dictionary learning  Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.  Solves the optimization problem::     (U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1                 (U,V)                 with || V_k ||_2 = 1 for all  0 <= k < n_components  Read more in the :ref:`User Guide <DictionaryLearning>`.  Parameters ---------- n_components : int,     number of dictionary elements to extract  alpha : float,     sparsity controlling parameter  n_iter : int,     total number of iterations to perform  fit_algorithm : {'lars', 'cd'}     lars: uses the least angle regression method to solve the lasso problem     (linear_model.lars_path)     cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). Lars will be faster if     the estimated components are sparse.  transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'}     Algorithm used to transform the data.     lars: uses the least angle regression method (linear_model.lars_path)     lasso_lars: uses Lars to compute the Lasso solution     lasso_cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). lasso_lars will be faster if     the estimated components are sparse.     omp: uses orthogonal matching pursuit to estimate the sparse solution     threshold: squashes to zero all coefficients less than alpha from     the projection dictionary * X'  transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default     Number of nonzero coefficients to target in each column of the     solution. This is only used by `algorithm='lars'` and `algorithm='omp'`     and is overridden by `alpha` in the `omp` case.  transform_alpha : float, 1. by default     If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the     penalty applied to the L1 norm.     If `algorithm='threshold'`, `alpha` is the absolute value of the     threshold below which coefficients will be squashed to zero.     If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of     the reconstruction error targeted. In this case, it overrides     `n_nonzero_coefs`.  split_sign : bool, False by default     Whether to split the sparse feature vector into the concatenation of     its negative part and its positive part. This can improve the     performance of downstream classifiers.  n_jobs : int,     number of parallel jobs to run  dict_init : array of shape (n_components, n_features),     initial value of the dictionary for warm restart scenarios  verbose :     degree of verbosity of the printed output  batch_size : int,     number of samples in each mini-batch  shuffle : bool,     whether to shuffle the samples before forming batches  random_state : int or RandomState     Pseudo number generator state used for random sampling.  Attributes ---------- components_ : array, [n_components, n_features]     components extracted from the data  inner_stats_ : tuple of (A, B) ndarrays     Internal sufficient statistics that are kept by the algorithm.     Keeping them is useful in online settings, to avoid loosing the     history of the evolution, but they shouldn't have any use for the     end user.     A (n_components, n_components) is the dictionary covariance matrix.     B (n_features, n_components) is the data approximation matrix  n_iter_ : int     Number of iterations run.  Notes ----- **References:**  J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (http://www.di.ens.fr/sierra/pdfs/icml09.pdf)  See also -------- SparseCoder DictionaryLearning SparsePCA MiniBatchSparsePCA", [
d("__init__(self, n_components, alpha, n_iter, fit_algorithm, n_jobs, batch_size, shuffle, dict_init, transform_algorithm, transform_n_nonzero_coefs, transform_alpha, verbose, split_sign, random_state)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y, iter_offset)"),]),
c("TruncatedSVD(BaseEstimator, TransformerMixin)", "/decomposition/truncated_svd.py; Dimensionality reduction using truncated SVD (aka LSA).  This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). It is very similar to PCA, but operates on sample vectors directly, instead of on a covariance matrix. This means it can work with scipy.sparse matrices efficiently.  In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).  This estimator supports two algorithm: a fast randomized SVD solver, and a 'naive' algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.  Read more in the :ref:`User Guide <LSA>`.  Parameters ---------- n_components : int, default = 2     Desired dimensionality of output data.     Must be strictly less than the number of features.     The default value is useful for visualisation. For LSA, a value of     100 is recommended.  algorithm : string, default = 'randomized'     SVD solver to use. Either 'arpack' for the ARPACK wrapper in SciPy     (scipy.sparse.linalg.svds), or 'randomized' for the randomized     algorithm due to Halko (2009).  n_iter : int, optional     Number of iterations for randomized SVD solver. Not used by ARPACK.  random_state : int or RandomState, optional     (Seed for) pseudo-random number generator. If not given, the     numpy.random singleton is used.  tol : float, optional     Tolerance for ARPACK. 0 means machine precision. Ignored by randomized     SVD solver.  Attributes ---------- components_ : array, shape (n_components, n_features)  explained_variance_ratio_ : array, [n_components]     Percentage of variance explained by each of the selected components.  explained_variance_ : array, [n_components]     The variance of the training samples transformed by a projection to     each component.  Examples -------- >>> from sklearn.decomposition import TruncatedSVD >>> from sklearn.random_projection import sparse_random_matrix >>> X = sparse_random_matrix(100, 100, density=0.01, random_state=42) >>> svd = TruncatedSVD(n_components=5, random_state=42) >>> svd.fit(X) # doctest: +NORMALIZE_WHITESPACE TruncatedSVD(algorithm='randomized', n_components=5, n_iter=5,         random_state=42, tol=0.0) >>> print(svd.explained_variance_ratio_) # doctest: +ELLIPSIS [ 0.07825... 0.05528... 0.05445... 0.04997... 0.04134...] >>> print(svd.explained_variance_ratio_.sum()) # doctest: +ELLIPSIS 0.27930...  See also -------- PCA RandomizedPCA  References ---------- Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061  Notes ----- SVD suffers from a problem called 'sign indeterminancy', which means the sign of the ``components_`` and the output from transform depend on the algorithm and random state. To work around this, fit instances of this class to data once, then keep the instance around to do transformations.", [
d("__init__(self, n_components, algorithm, n_iter, random_state, tol)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),]),
c("ProjectedGradientNMF(BaseEstimator, TransformerMixin)", "/decomposition/nmf.py; Non-Negative matrix factorization by Projected Gradient (NMF)  Read more in the :ref:`User Guide <NMF>`.  Parameters ---------- n_components : int or None     Number of components, if n_components is not set all components     are kept  init :  'nndsvd' |  'nndsvda' | 'nndsvdar' | 'random'     Method used to initialize the procedure.     Default: 'nndsvd' if n_components < n_features, otherwise random.     Valid options::          'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)             initialization (better for sparseness)         'nndsvda': NNDSVD with zeros filled with the average of X             (better when sparsity is not desired)         'nndsvdar': NNDSVD with zeros filled with small random values             (generally faster, less accurate alternative to NNDSVDa             for when sparsity is not desired)         'random': non-negative random matrices  sparseness : 'data' | 'components' | None, default: None     Where to enforce sparsity in the model.  beta : double, default: 1     Degree of sparseness, if sparseness is not None. Larger values mean     more sparseness.  eta : double, default: 0.1     Degree of correctness to maintain, if sparsity is not None. Smaller     values mean larger error.  tol : double, default: 1e-4     Tolerance value used in stopping conditions.  max_iter : int, default: 200     Number of iterations to compute.  nls_max_iter : int, default: 2000     Number of iterations in NLS subproblem.  random_state : int or RandomState     Random number generator seed control.  Attributes ---------- components_ : array, [n_components, n_features]     Non-negative components of the data.  reconstruction_err_ : number     Frobenius norm of the matrix difference between     the training data and the reconstructed data from     the fit produced by the model. ``|| X - WH ||_2``  n_iter_ : int     Number of iterations run.  Examples --------  >>> import numpy as np >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]) >>> from sklearn.decomposition import ProjectedGradientNMF >>> model = ProjectedGradientNMF(n_components=2, init='random', ...                              random_state=0) >>> model.fit(X) #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE ProjectedGradientNMF(beta=1, eta=0.1, init='random', max_iter=200,         n_components=2, nls_max_iter=2000, random_state=0, sparseness=None,         tol=0.0001) >>> model.components_ array([[ 0.77032744,  0.11118662],        [ 0.38526873,  0.38228063]]) >>> model.reconstruction_err_ #doctest: +ELLIPSIS 0.00746... >>> model = ProjectedGradientNMF(n_components=2, ...              sparseness='components', init='random', random_state=0) >>> model.fit(X) #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE ProjectedGradientNMF(beta=1, eta=0.1, init='random', max_iter=200,             n_components=2, nls_max_iter=2000, random_state=0,             sparseness='components', tol=0.0001) >>> model.components_ array([[ 1.67481991,  0.29614922],        [ 0.        ,  0.4681982 ]]) >>> model.reconstruction_err_ #doctest: +ELLIPSIS 0.513...  References ---------- This implements  C.-J. Lin. Projected gradient methods for non-negative matrix factorization. Neural Computation, 19(2007), 2756-2779. http://www.csie.ntu.edu.tw/~cjlin/nmf/  P. Hoyer. Non-negative Matrix Factorization with Sparseness Constraints. Journal of Machine Learning Research 2004.  NNDSVD is introduced in  C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for nonnegative matrix factorization - Pattern Recognition, 2008 http://tinyurl.com/nndsvd", [
d("__init__(self, n_components, init, sparseness, beta, eta, tol, max_iter, nls_max_iter, random_state)"),
d("_init(self, X)"),
d("_update_W(self, X, H, W, tolW)"),
d("_update_H(self, X, H, W, tolH)"),
d("fit_transform(self, X, y)"),
d("fit(self, X, y)"),
d("transform(self, X)"),]),
c("NMF(ProjectedGradientNMF)", "/decomposition/nmf.py; ", []),
c("KernelPCA(BaseEstimator, TransformerMixin)", "/decomposition/kernel_pca.py; Kernel Principal component analysis (KPCA)  Non-linear dimensionality reduction through the use of kernels (see :ref:`metrics`).  Read more in the :ref:`User Guide <kernel_PCA>`.  Parameters ---------- n_components: int or None     Number of components. If None, all non-zero components are kept.  kernel: 'linear' | 'poly' | 'rbf' | 'sigmoid' | 'cosine' | 'precomputed'     Kernel.     Default: 'linear'  degree : int, default=3     Degree for poly kernels. Ignored by other kernels.  gamma : float, optional     Kernel coefficient for rbf and poly kernels. Default: 1/n_features.     Ignored by other kernels.  coef0 : float, optional     Independent term in poly and sigmoid kernels.     Ignored by other kernels.  kernel_params : mapping of string to any, optional     Parameters (keyword arguments) and values for kernel passed as     callable object. Ignored by other kernels.  alpha: int     Hyperparameter of the ridge regression that learns the     inverse transform (when fit_inverse_transform=True).     Default: 1.0  fit_inverse_transform: bool     Learn the inverse transform for non-precomputed kernels.     (i.e. learn to find the pre-image of a point)     Default: False  eigen_solver: string ['auto'|'dense'|'arpack']     Select eigensolver to use.  If n_components is much less than     the number of training samples, arpack may be more efficient     than the dense eigensolver.  tol: float     convergence tolerance for arpack.     Default: 0 (optimal value will be chosen by arpack)  max_iter : int     maximum number of iterations for arpack     Default: None (optimal value will be chosen by arpack)  remove_zero_eig : boolean, default=True     If True, then all components with zero eigenvalues are removed, so     that the number of components in the output may be < n_components     (and sometimes even zero due to numerical instability).     When n_components is None, this parameter is ignored and components     with zero eigenvalues are removed regardless.  Attributes ----------  lambdas_ :     Eigenvalues of the centered kernel matrix  alphas_ :     Eigenvectors of the centered kernel matrix  dual_coef_ :     Inverse transform matrix  X_transformed_fit_ :     Projection of the fitted data on the kernel principal components  References ---------- Kernel PCA was introduced in:     Bernhard Schoelkopf, Alexander J. Smola,     and Klaus-Robert Mueller. 1999. Kernel principal     component analysis. In Advances in kernel methods,     MIT Press, Cambridge, MA, USA 327-352.", [
d("__init__(self, n_components, kernel, gamma, degree, coef0, kernel_params, alpha, fit_inverse_transform, eigen_solver, tol, max_iter, remove_zero_eig)"),
d("_pairwise(self)"),
d("_get_kernel(self, X, Y)"),
d("_fit_transform(self, K)"),
d("_fit_inverse_transform(self, X_transformed, X)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),]),
c("PCA(BaseEstimator, TransformerMixin)", "/decomposition/pca.py; Principal component analysis (PCA)  Linear dimensionality reduction using Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.  This implementation uses the scipy.linalg implementation of the singular value decomposition. It only works for dense arrays and is not scalable to large dimensional data.  The time complexity of this implementation is ``O(n ** 3)`` assuming n ~ n_samples ~ n_features.  Read more in the :ref:`User Guide <PCA>`.  Parameters ---------- n_components : int, None or string     Number of components to keep.     if n_components is not set all components are kept::          n_components == min(n_samples, n_features)      if n_components == 'mle', Minka's MLE is used to guess the dimension     if ``0 < n_components < 1``, select the number of components such that     the amount of variance that needs to be explained is greater than the     percentage specified by n_components  copy : bool     If False, data passed to fit are overwritten and running     fit(X).transform(X) will not yield the expected results,     use fit_transform(X) instead.  whiten : bool, optional     When True (False by default) the `components_` vectors are divided     by n_samples times singular values to ensure uncorrelated outputs     with unit component-wise variances.      Whitening will remove some information from the transformed signal     (the relative variance scales of the components) but can sometime     improve the predictive accuracy of the downstream estimators by     making there data respect some hard-wired assumptions.  Attributes ---------- components_ : array, [n_components, n_features]     Principal axes in feature space, representing the directions of     maximum variance in the data.  explained_variance_ratio_ : array, [n_components]     Percentage of variance explained by each of the selected components.     If ``n_components`` is not set then all components are stored and the     sum of explained variances is equal to 1.0  mean_ : array, [n_features]     Per-feature empirical mean, estimated from the training set.  n_components_ : int     The estimated number of components. Relevant when n_components is set     to 'mle' or a number between 0 and 1 to select using explained     variance.  noise_variance_ : float     The estimated noise covariance following the Probabilistic PCA model     from Tipping and Bishop 1999. See 'Pattern Recognition and     Machine Learning' by C. Bishop, 12.2.1 p. 574 or     http://www.miketipping.com/papers/met-mppca.pdf. It is required to     computed the estimated data covariance and score samples.  Notes ----- For n_components='mle', this class uses the method of `Thomas P. Minka: Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`  Implements the probabilistic PCA model from: M. Tipping and C. Bishop, Probabilistic Principal Component Analysis, Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622 via the score and score_samples methods. See http://www.miketipping.com/papers/met-mppca.pdf  Due to implementation subtleties of the Singular Value Decomposition (SVD), which is used in this implementation, running fit twice on the same matrix can lead to principal components with signs flipped (change in direction). For this reason, it is important to always use the same estimator object to transform data in a consistent fashion.  Examples --------  >>> import numpy as np >>> from sklearn.decomposition import PCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> pca = PCA(n_components=2) >>> pca.fit(X) PCA(copy=True, n_components=2, whiten=False) >>> print(pca.explained_variance_ratio_) # doctest: +ELLIPSIS [ 0.99244...  0.00755...]  See also -------- RandomizedPCA KernelPCA SparsePCA TruncatedSVD", [
d("__init__(self, n_components, copy, whiten)"),
d("fit(self, X, y)"),
d("fit_transform(self, X, y)"),
d("_fit(self, X)"),
d("get_covariance(self)"),
d("get_precision(self)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),
d("score_samples(self, X)"),
d("score(self, X, y)"),]),
c("RandomizedPCA(BaseEstimator, TransformerMixin)", "/decomposition/pca.py; Principal component analysis (PCA) using randomized SVD  Linear dimensionality reduction using approximated Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.  Read more in the :ref:`User Guide <RandomizedPCA>`.  Parameters ---------- n_components : int, optional     Maximum number of components to keep. When not given or None, this     is set to n_features (the second dimension of the training data).  copy : bool     If False, data passed to fit are overwritten and running     fit(X).transform(X) will not yield the expected results,     use fit_transform(X) instead.  iterated_power : int, optional     Number of iterations for the power method. 3 by default.  whiten : bool, optional     When True (False by default) the `components_` vectors are divided     by the singular values to ensure uncorrelated outputs with unit     component-wise variances.      Whitening will remove some information from the transformed signal     (the relative variance scales of the components) but can sometime     improve the predictive accuracy of the downstream estimators by     making their data respect some hard-wired assumptions.  random_state : int or RandomState instance or None (default)     Pseudo Random Number generator seed control. If None, use the     numpy.random singleton.  Attributes ---------- components_ : array, [n_components, n_features]     Components with maximum variance.  explained_variance_ratio_ : array, [n_components]     Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0  mean_ : array, [n_features]     Per-feature empirical mean, estimated from the training set.  Examples -------- >>> import numpy as np >>> from sklearn.decomposition import RandomizedPCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> pca = RandomizedPCA(n_components=2) >>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE RandomizedPCA(copy=True, iterated_power=3, n_components=2,        random_state=None, whiten=False) >>> print(pca.explained_variance_ratio_) # doctest: +ELLIPSIS [ 0.99244...  0.00755...]  See also -------- PCA TruncatedSVD  References ----------  .. [Halko2009] `Finding structure with randomness: Stochastic algorithms   for constructing approximate matrix decompositions Halko, et al., 2009   (arXiv:909)`  .. [MRT] `A randomized algorithm for the decomposition of matrices   Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert`", [
d("__init__(self, n_components, copy, iterated_power, whiten, random_state)"),
d("fit(self, X, y)"),
d("_fit(self, X)"),
d("transform(self, X, y)"),
d("fit_transform(self, X, y)"),
d("inverse_transform(self, X, y)"),]),
c("FactorAnalysis(BaseEstimator, TransformerMixin)", "/decomposition/factor_analysis.py; Factor Analysis (FA)  A simple linear generative model with Gaussian latent variables.  The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.  If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain :class:`PPCA`.  FactorAnalysis performs a maximum likelihood estimate of the so-called `loading` matrix, the transformation of the latent variables to the observed ones, using expectation-maximization (EM).  Read more in the :ref:`User Guide <FA>`.  Parameters ---------- n_components : int | None     Dimensionality of latent space, the number of components     of ``X`` that are obtained after ``transform``.     If None, n_components is set to the number of features.  tol : float     Stopping tolerance for EM algorithm.  copy : bool     Whether to make a copy of X. If ``False``, the input X gets overwritten     during fitting.  max_iter : int     Maximum number of iterations.  noise_variance_init : None | array, shape=(n_features,)     The initial guess of the noise variance for each feature.     If None, it defaults to np.ones(n_features)  svd_method : {'lapack', 'randomized'}     Which SVD method to use. If 'lapack' use standard SVD from     scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.     Defaults to 'randomized'. For most applications 'randomized' will     be sufficiently precise while providing significant speed gains.     Accuracy can also be improved by setting higher values for     `iterated_power`. If this is not sufficient, for maximum precision     you should choose 'lapack'.  iterated_power : int, optional     Number of iterations for the power method. 3 by default. Only used     if ``svd_method`` equals 'randomized'  random_state : int or RandomState     Pseudo number generator state used for random sampling. Only used     if ``svd_method`` equals 'randomized'  Attributes ---------- components_ : array, [n_components, n_features]     Components with maximum variance.  loglike_ : list, [n_iterations]     The log likelihood at each iteration.  noise_variance_ : array, shape=(n_features,)     The estimated noise variance for each feature.  n_iter_ : int     Number of iterations run.  References ---------- .. David Barber, Bayesian Reasoning and Machine Learning,     Algorithm 21.1  .. Christopher M. Bishop: Pattern Recognition and Machine Learning,     Chapter 12.2.4  See also -------- PCA: Principal component analysis is also a latent linear variable model     which however assumes equal noise variance for each feature.     This extra assumption makes probabilistic PCA faster as it can be     computed in closed form. FastICA: Independent component analysis, a latent variable model with     non-Gaussian latent variables.", [
d("__init__(self, n_components, tol, copy, max_iter, noise_variance_init, svd_method, iterated_power, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X)"),
d("get_covariance(self)"),
d("get_precision(self)"),
d("score_samples(self, X)"),
d("score(self, X, y)"),]),
c("LatentDirichletAllocation(BaseEstimator, TransformerMixin)", "/decomposition/online_lda.py; Latent Dirichlet Allocation with online variational Bayes algorithm  Parameters ---------- n_topics : int, optional (default=10)     Number of topics.  doc_topic_prior : float, optional (default=None)     Prior of document topic distribution `theta`. If the value is None,     defaults to `1 / n_topics`.     In the literature, this is called `alpha`.  topic_word_prior : float, optional (default=None)     Prior of topic word distribution `beta`. If the value is None, defaults     to `1 / n_topics`.     In the literature, this is called `eta`.  learning_method : 'batch' | 'online', default='online'     Method used to update `_component`. Only used in `fit` method.     In general, if the data size is large, the online update will be much     faster than the batch update.     Valid options::          'batch': Batch variational Bayes method. Use all training data in             each EM update.             Old `components_` will be overwritten in each iteration.         'online': Online variational Bayes method. In each EM update, use             mini-batch of training data to update the ``components_``             variable incrementally. The learning rate is controlled by the             ``learning_decay`` and the ``learning_offset`` parameters.  learning_decay : float, optional (default=0.7)     It is a parameter that control learning rate in the online learning     method. The value should be set between (0.5, 1.0] to guarantee     asymptotic convergence. When the value is 0.0 and batch_size is     ``n_samples``, the update method is same as batch learning. In the     literature, this is called kappa.  learning_offset : float, optional (default=10.)     A (positive) parameter that downweights early iterations in online     learning.  It should be greater than 1.0. In the literature, this is     called tau_0.  max_iter : integer, optional (default=10)     The maximum number of iterations.  total_samples : int, optional (default=1e6)     Total number of documents. Only used in the `partial_fit` method.  batch_size : int, optional (default=128)     Number of documents to use in each EM iteration. Only used in online     learning.  evaluate_every : int optional (default=0)     How often to evaluate perplexity. Only used in `fit` method.     set it to 0 or and negative number to not evalute perplexity in     training at all. Evaluating perplexity can help you check convergence     in training process, but it will also increase total training time.     Evaluating perplexity in every iteration might increase training time     up to two-fold.  perp_tol : float, optional (default=1e-1)     Perplexity tolerance in batch learning. Only used when     ``evaluate_every`` is greater than 0.  mean_change_tol : float, optional (default=1e-3)     Stopping tolerance for updating document topic distribution in E-step.  max_doc_update_iter : int (default=100)     Max number of iterations for updating document topic distribution in     the E-step.  n_jobs : int, optional (default=1)     The number of jobs to use in the E-step. If -1, all CPUs are used. For     ``n_jobs`` below -1, (n_cpus + 1 + n_jobs) are used.  verbose : int, optional (default=0)     Verbosity level.  random_state : int or RandomState instance or None, optional (default=None)     Pseudo-random number generator seed control.  Attributes ---------- components_ : array, [n_topics, n_features]     Topic word distribution. ``components_[i, j]`` represents word j in     topic `i`. In the literature, this is called lambda.  n_batch_iter_ : int     Number of iterations of the EM step.  n_iter_ : int     Number of passes over the dataset.   References ---------- [1] 'Online Learning for Latent Dirichlet Allocation', Matthew D. Hoffman,     David M. Blei, Francis Bach, 2010  [2] 'Stochastic Variational Inference', Matthew D. Hoffman, David M. Blei,     Chong Wang, John Paisley, 2013  [3] Matthew D. Hoffman's onlineldavb code. Link:     http://www.cs.princeton.edu/~mdhoffma/code/onlineldavb.tar", [
d("__init__(self, n_topics, doc_topic_prior, topic_word_prior, learning_method, learning_decay, learning_offset, max_iter, batch_size, evaluate_every, total_samples, perp_tol, mean_change_tol, max_doc_update_iter, n_jobs, verbose, random_state)"),
d("_check_params(self)"),
d("_init_latent_vars(self, n_features)"),
d("_e_step(self, X, cal_sstats, random_init)"),
d("_em_step(self, X, total_samples, batch_update)"),
d("_check_non_neg_array(self, X, whom)"),
d("partial_fit(self, X, y)"),
d("fit(self, X, y)"),
d("transform(self, X)"),
d("_approx_bound(self, X, doc_topic_distr, sub_sampling)"),
d("score(self, X, y)"),
d("perplexity(self, X, doc_topic_distr, sub_sampling)"),]),
c("IncrementalPCA(_BasePCA)", "/decomposition/incremental_pca.py; Incremental principal components analysis (IPCA).  Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.  Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA.  This algorithm has constant memory complexity, on the order of ``batch_size``, enabling use of np.memmap files without loading the entire file into memory.  The computational overhead of each SVD is ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples remain in memory at a time. There will be ``n_samples / batch_size`` SVD computations to get the principal components, versus 1 large SVD of complexity ``O(n_samples * n_features ** 2)`` for PCA.  Read more in the :ref:`User Guide <IncrementalPCA>`.  Parameters ---------- n_components : int or None, (default=None)     Number of components to keep. If ``n_components `` is ``None``,     then ``n_components`` is set to ``min(n_samples, n_features)``.  batch_size : int or None, (default=None)     The number of samples to use for each batch. Only used when calling     ``fit``. If ``batch_size`` is ``None``, then ``batch_size``     is inferred from the data and set to ``5 * n_features``, to provide a     balance between approximation accuracy and memory consumption.  copy : bool, (default=True)     If False, X will be overwritten. ``copy=False`` can be used to     save memory but is unsafe for general use.  whiten : bool, optional     When True (False by default) the ``components_`` vectors are divided     by ``n_samples`` times ``components_`` to ensure uncorrelated outputs     with unit component-wise variances.      Whitening will remove some information from the transformed signal     (the relative variance scales of the components) but can sometimes     improve the predictive accuracy of the downstream estimators by     making data respect some hard-wired assumptions.  Attributes ---------- components_ : array, shape (n_components, n_features)     Components with maximum variance.  explained_variance_ : array, shape (n_components,)     Variance explained by each of the selected components.  explained_variance_ratio_ : array, shape (n_components,)     Percentage of variance explained by each of the selected components.     If all components are stored, the sum of explained variances is equal     to 1.0  mean_ : array, shape (n_features,)     Per-feature empirical mean, aggregate over calls to ``partial_fit``.  var_ : array, shape (n_features,)     Per-feature empirical variance, aggregate over calls to ``partial_fit``.  noise_variance_ : float     The estimated noise covariance following the Probabilistic PCA model     from Tipping and Bishop 1999. See 'Pattern Recognition and     Machine Learning' by C. Bishop, 12.2.1 p. 574 or     http://www.miketipping.com/papers/met-mppca.pdf.  n_components_ : int     The estimated number of components. Relevant when ``n_components=None``.  n_samples_seen_ : int     The number of samples processed by the estimator. Will be reset on     new calls to fit, but increments across ``partial_fit`` calls.  Notes ----- Implements the incremental PCA model from: `D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.` See http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf  This model is an extension of the Sequential Karhunen-Loeve Transform from: `A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.` See http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf  We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is `Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.`. This technique has been omitted because it is advantageous only when decomposing a matrix with ``n_samples`` (rows) >= 5/3 * ``n_features`` (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary.  References ---------- D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual     Tracking, International Journal of Computer Vision, Volume 77,     Issue 1-3, pp. 125-141, May 2008.  G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,     Section 5.4.4, pp. 252-253.  See also -------- PCA RandomizedPCA KernelPCA SparsePCA TruncatedSVD", [
d("__init__(self, n_components, whiten, copy, batch_size)"),
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),]),
c("SparsePCA(BaseEstimator, TransformerMixin)", "/decomposition/sparse_pca.py; Sparse Principal Components Analysis (SparsePCA)  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.  Parameters ---------- n_components : int,     Number of sparse atoms to extract.  alpha : float,     Sparsity controlling parameter. Higher values lead to sparser     components.  ridge_alpha : float,     Amount of ridge shrinkage to apply in order to improve     conditioning when calling the transform method.  max_iter : int,     Maximum number of iterations to perform.  tol : float,     Tolerance for the stopping condition.  method : {'lars', 'cd'}     lars: uses the least angle regression method to solve the lasso problem     (linear_model.lars_path)     cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). Lars will be faster if     the estimated components are sparse.  n_jobs : int,     Number of parallel jobs to run.  U_init : array of shape (n_samples, n_components),     Initial values for the loadings for warm restart scenarios.  V_init : array of shape (n_components, n_features),     Initial values for the components for warm restart scenarios.  verbose :     Degree of verbosity of the printed output.  random_state : int or RandomState     Pseudo number generator state used for random sampling.  Attributes ---------- components_ : array, [n_components, n_features]     Sparse components extracted from the data.  error_ : array     Vector of errors at each iteration.  n_iter_ : int     Number of iterations run.  See also -------- PCA MiniBatchSparsePCA DictionaryLearning", [
d("__init__(self, n_components, alpha, ridge_alpha, max_iter, tol, method, n_jobs, U_init, V_init, verbose, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X, ridge_alpha)"),]),
c("MiniBatchSparsePCA(SparsePCA)", "/decomposition/sparse_pca.py; Mini-batch Sparse Principal Components Analysis  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.  Parameters ---------- n_components : int,     number of sparse atoms to extract  alpha : int,     Sparsity controlling parameter. Higher values lead to sparser     components.  ridge_alpha : float,     Amount of ridge shrinkage to apply in order to improve     conditioning when calling the transform method.  n_iter : int,     number of iterations to perform for each mini batch  callback : callable,     callable that gets invoked every five iterations  batch_size : int,     the number of features to take in each mini batch  verbose :     degree of output the procedure will print  shuffle : boolean,     whether to shuffle the data before splitting it in batches  n_jobs : int,     number of parallel jobs to run, or -1 to autodetect.  method : {'lars', 'cd'}     lars: uses the least angle regression method to solve the lasso problem     (linear_model.lars_path)     cd: uses the coordinate descent method to compute the     Lasso solution (linear_model.Lasso). Lars will be faster if     the estimated components are sparse.  random_state : int or RandomState     Pseudo number generator state used for random sampling.  Attributes ---------- components_ : array, [n_components, n_features]     Sparse components extracted from the data.  error_ : array     Vector of errors at each iteration.  n_iter_ : int     Number of iterations run.  See also -------- PCA SparsePCA DictionaryLearning", [
d("__init__(self, n_components, alpha, ridge_alpha, n_iter, callback, batch_size, verbose, shuffle, n_jobs, method, random_state)"),
d("fit(self, X, y)"),]),
c("_BasePCA()", "/decomposition/base.py; Base class for PCA methods.  Warning: This class should not be used directly. Use derived classes instead.", [
d("get_covariance(self)"),
d("get_precision(self)"),
d("fit(X, y)"),
d("transform(self, X, y)"),
d("inverse_transform(self, X, y)"),]),
c("FastICA(BaseEstimator, TransformerMixin)", "/decomposition/fastica_.py; FastICA: a fast algorithm for Independent Component Analysis.  Read more in the :ref:`User Guide <ICA>`.  Parameters ---------- n_components : int, optional     Number of components to use. If none is passed, all are used.  algorithm : {'parallel', 'deflation'}     Apply parallel or deflational algorithm for FastICA.  whiten : boolean, optional     If whiten is false, the data is already considered to be     whitened, and no whitening is performed.  fun : string or function, optional. Default: 'logcosh'     The functional form of the G function used in the     approximation to neg-entropy. Could be either 'logcosh', 'exp',     or 'cube'.     You can also provide your own function. It should return a tuple     containing the value of the function, and of its derivative, in the     point. Example:      def my_g(x):         return x ** 3, 3 * x ** 2  fun_args : dictionary, optional     Arguments to send to the functional form.     If empty and if fun='logcosh', fun_args will take value     {'alpha' : 1.0}.  max_iter : int, optional     Maximum number of iterations during fit.  tol : float, optional     Tolerance on update at each iteration.  w_init : None of an (n_components, n_components) ndarray     The mixing matrix to be used to initialize the algorithm.  random_state : int or RandomState     Pseudo number generator state used for random sampling.  Attributes ---------- components_ : 2D array, shape (n_components, n_features)     The unmixing matrix.  mixing_ : array, shape (n_features, n_components)     The mixing matrix.  n_iter_ : int     If the algorithm is 'deflation', n_iter is the     maximum number of iterations run across all components. Else     they are just the number of iterations taken to converge.  Notes ----- Implementation based on `A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430`", [
d("__init__(self, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, random_state)"),
d("_fit(self, X, compute_sources)"),
d("fit_transform(self, X, y)"),
d("fit(self, X, y)"),
d("transform(self, X, y, copy)"),
d("inverse_transform(self, X, copy)"),]),
c("ShrunkCovariance(EmpiricalCovariance)", "/covariance/shrunk_covariance_.py; Covariance estimator with shrinkage  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- store_precision : boolean, default True     Specify if the estimated precision is stored  shrinkage : float, 0 <= shrinkage <= 1, default 0.1     Coefficient in the convex combination used for the computation     of the shrunk estimate.  assume_centered : boolean, default False     If True, data are not centered before computation.     Useful when working with data whose mean is almost, but not exactly     zero.     If False, data are centered before computation.  Attributes ---------- covariance_ : array-like, shape (n_features, n_features)     Estimated covariance matrix  precision_ : array-like, shape (n_features, n_features)     Estimated pseudo inverse matrix.     (stored only if store_precision is True)  `shrinkage` : float, 0 <= shrinkage <= 1     Coefficient in the convex combination used for the computation     of the shrunk estimate.  Notes ----- The regularized covariance is given by  (1 - shrinkage)*cov   + shrinkage*mu*np.identity(n_features)  where mu = trace(cov) / n_features", [
d("__init__(self, store_precision, assume_centered, shrinkage)"),
d("fit(self, X, y)"),]),
c("LedoitWolf(EmpiricalCovariance)", "/covariance/shrunk_covariance_.py; LedoitWolf Estimator  Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf's formula as described in 'A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices', Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- store_precision : bool, default=True     Specify if the estimated precision is stored.  assume_centered : bool, default=False     If True, data are not centered before computation.     Useful when working with data whose mean is almost, but not exactly     zero.     If False (default), data are centered before computation.  block_size : int, default=1000     Size of the blocks into which the covariance matrix will be split     during its Ledoit-Wolf estimation. This is purely a memory     optimization and does not affect results.  Attributes ---------- covariance_ : array-like, shape (n_features, n_features)     Estimated covariance matrix  precision_ : array-like, shape (n_features, n_features)     Estimated pseudo inverse matrix.     (stored only if store_precision is True)  shrinkage_ : float, 0 <= shrinkage <= 1     Coefficient in the convex combination used for the computation     of the shrunk estimate.  Notes ----- The regularised covariance is::      (1 - shrinkage)*cov             + shrinkage*mu*np.identity(n_features)  where mu = trace(cov) / n_features and shrinkage is given by the Ledoit and Wolf formula (see References)  References ---------- 'A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices', Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.", [
d("__init__(self, store_precision, assume_centered, block_size)"),
d("fit(self, X, y)"),]),
c("OAS(EmpiricalCovariance)", "/covariance/shrunk_covariance_.py; Oracle Approximating Shrinkage Estimator  Read more in the :ref:`User Guide <shrunk_covariance>`.  OAS is a particular form of shrinkage described in 'Shrinkage Algorithms for MMSE Covariance Estimation' Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.  The formula used here does not correspond to the one given in the article. It has been taken from the Matlab program available from the authors' webpage (https://tbayes.eecs.umich.edu/yilun/covestimation).  Parameters ---------- store_precision : bool, default=True     Specify if the estimated precision is stored.  assume_centered: bool, default=False     If True, data are not centered before computation.     Useful when working with data whose mean is almost, but not exactly     zero.     If False (default), data are centered before computation.  Attributes ---------- covariance_ : array-like, shape (n_features, n_features)     Estimated covariance matrix.  precision_ : array-like, shape (n_features, n_features)     Estimated pseudo inverse matrix.     (stored only if store_precision is True)  shrinkage_ : float, 0 <= shrinkage <= 1   coefficient in the convex combination used for the computation   of the shrunk estimate.  Notes ----- The regularised covariance is::      (1 - shrinkage)*cov             + shrinkage*mu*np.identity(n_features)  where mu = trace(cov) / n_features and shrinkage is given by the OAS formula (see References)  References ---------- 'Shrinkage Algorithms for MMSE Covariance Estimation' Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.", [
d("fit(self, X, y)"),]),
c("OutlierDetectionMixin(object)", "/covariance/outlier_detection.py; Set of methods for outliers detection with covariance estimators.  Parameters ---------- contamination : float, 0. < contamination < 0.5     The amount of contamination of the data set, i.e. the proportion     of outliers in the data set.  Notes ----- Outlier detection from covariance estimation may break or not perform well in high-dimensional settings. In particular, one will always take care to work with ``n_samples > n_features ** 2``.", [
d("__init__(self, contamination)"),
d("decision_function(self, X, raw_values)"),
d("predict(self, X)"),
d("threshold(self)"),]),
c("EllipticEnvelope(ClassifierMixin, OutlierDetectionMixin, MinCovDet)", "/covariance/outlier_detection.py; An object for detecting outliers in a Gaussian distributed dataset.  Read more in the :ref:`User Guide <outlier_detection>`.  Attributes ---------- `contamination` : float, 0. < contamination < 0.5   The amount of contamination of the data set, i.e. the proportion of       outliers in the data set.  location_ : array-like, shape (n_features,)     Estimated robust location  covariance_ : array-like, shape (n_features, n_features)     Estimated robust covariance matrix  precision_ : array-like, shape (n_features, n_features)     Estimated pseudo inverse matrix.     (stored only if store_precision is True)  support_ : array-like, shape (n_samples,)     A mask of the observations that have been used to compute the     robust estimates of location and shape.  Parameters ---------- store_precision : bool     Specify if the estimated precision is stored.  assume_centered : Boolean     If True, the support of robust location and covariance estimates     is computed, and a covariance estimate is recomputed from it,     without centering the data.     Useful to work with data whose mean is significantly equal to     zero but is not exactly zero.     If False, the robust location and covariance are directly computed     with the FastMCD algorithm without additional treatment.  support_fraction : float, 0 < support_fraction < 1     The proportion of points to be included in the support of the raw     MCD estimate. Default is ``None``, which implies that the minimum     value of support_fraction will be used within the algorithm:     `[n_sample + n_features + 1] / 2`.  contamination : float, 0. < contamination < 0.5     The amount of contamination of the data set, i.e. the proportion     of outliers in the data set.  See Also -------- EmpiricalCovariance, MinCovDet  Notes ----- Outlier detection from covariance estimation may break or not perform well in high-dimensional settings. In particular, one will always take care to work with ``n_samples > n_features ** 2``.  References ---------- ..  [1] Rousseeuw, P.J., Van Driessen, K. 'A fast algorithm for the minimum     covariance determinant estimator' Technometrics 41(3), 212 (1999)", [
d("__init__(self, store_precision, assume_centered, support_fraction, contamination, random_state)"),
d("fit(self, X, y)"),]),
c("EmpiricalCovariance(BaseEstimator)", "/covariance/empirical_covariance_.py; Maximum likelihood covariance estimator  Read more in the :ref:`User Guide <covariance>`.  Parameters ---------- store_precision : bool     Specifies if the estimated precision is stored.  assume_centered : bool     If True, data are not centered before computation.     Useful when working with data whose mean is almost, but not exactly     zero.     If False (default), data are centered before computation.  Attributes ---------- covariance_ : 2D ndarray, shape (n_features, n_features)     Estimated covariance matrix  precision_ : 2D ndarray, shape (n_features, n_features)     Estimated pseudo-inverse matrix.     (stored only if store_precision is True)", [
d("__init__(self, store_precision, assume_centered)"),
d("_set_covariance(self, covariance)"),
d("get_precision(self)"),
d("fit(self, X, y)"),
d("score(self, X_test, y)"),
d("error_norm(self, comp_cov, norm, scaling, squared)"),
d("mahalanobis(self, observations)"),]),
c("GraphLasso(EmpiricalCovariance)", "/covariance/graph_lasso_.py; Sparse inverse covariance estimation with an l1-penalized estimator.  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  Parameters ---------- alpha : positive float, default 0.01     The regularization parameter: the higher alpha, the more     regularization, the sparser the inverse covariance.  mode : {'cd', 'lars'}, default 'cd'     The Lasso solver to use: coordinate descent or LARS. Use LARS for     very sparse underlying graphs, where p > n. Elsewhere prefer cd     which is more numerically stable.  tol : positive float, default 1e-4     The tolerance to declare convergence: if the dual gap goes below     this value, iterations are stopped.  enet_tol : positive float, optional     The tolerance for the elastic net solver used to calculate the descent     direction. This parameter controls the accuracy of the search direction     for a given column update, not of the overall parameter estimate. Only     used for mode='cd'.  max_iter : integer, default 100     The maximum number of iterations.  verbose : boolean, default False     If verbose is True, the objective function and dual gap are     plotted at each iteration.  assume_centered : boolean, default False     If True, data are not centered before computation.     Useful when working with data whose mean is almost, but not exactly     zero.     If False, data are centered before computation.  Attributes ---------- covariance_ : array-like, shape (n_features, n_features)     Estimated covariance matrix  precision_ : array-like, shape (n_features, n_features)     Estimated pseudo inverse matrix.  n_iter_ : int     Number of iterations run.  See Also -------- graph_lasso, GraphLassoCV", [
d("__init__(self, alpha, mode, tol, enet_tol, max_iter, verbose, assume_centered)"),
d("fit(self, X, y)"),]),
c("GraphLassoCV(GraphLasso)", "/covariance/graph_lasso_.py; Sparse inverse covariance w/ cross-validated choice of the l1 penalty  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  Parameters ---------- alphas : integer, or list positive float, optional     If an integer is given, it fixes the number of points on the     grids of alpha to be used. If a list is given, it gives the     grid to be used. See the notes in the class docstring for     more details.  n_refinements: strictly positive integer     The number of times the grid is refined. Not used if explicit     values of alphas are passed.  cv : cross-validation generator, optional     see sklearn.cross_validation module. If None is passed, defaults to     a 3-fold strategy  tol: positive float, optional     The tolerance to declare convergence: if the dual gap goes below     this value, iterations are stopped.  enet_tol : positive float, optional     The tolerance for the elastic net solver used to calculate the descent     direction. This parameter controls the accuracy of the search direction     for a given column update, not of the overall parameter estimate. Only     used for mode='cd'.  max_iter: integer, optional     Maximum number of iterations.  mode: {'cd', 'lars'}     The Lasso solver to use: coordinate descent or LARS. Use LARS for     very sparse underlying graphs, where number of features is greater     than number of samples. Elsewhere prefer cd which is more numerically     stable.  n_jobs: int, optional     number of jobs to run in parallel (default 1).  verbose: boolean, optional     If verbose is True, the objective function and duality gap are     printed at each iteration.  assume_centered : Boolean     If True, data are not centered before computation.     Useful when working with data whose mean is almost, but not exactly     zero.     If False, data are centered before computation.  Attributes ---------- covariance_ : numpy.ndarray, shape (n_features, n_features)     Estimated covariance matrix.  precision_ : numpy.ndarray, shape (n_features, n_features)     Estimated precision matrix (inverse covariance).  alpha_ : float     Penalization parameter selected.  cv_alphas_ : list of float     All penalization parameters explored.  `grid_scores`: 2D numpy.ndarray (n_alphas, n_folds)     Log-likelihood score on left-out data across folds.  n_iter_ : int     Number of iterations run for the optimal alpha.  See Also -------- graph_lasso, GraphLasso  Notes ----- The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.  One of the challenges which is faced here is that the solvers can fail to converge to a well-conditioned estimate. The corresponding values of alpha then come out as missing values, but the optimum may be close to these missing values.", [
d("__init__(self, alphas, n_refinements, cv, tol, enet_tol, max_iter, mode, n_jobs, verbose, assume_centered)"),
d("fit(self, X, y)"),]),
c("MinCovDet(EmpiricalCovariance)", "/covariance/robust_covariance.py; Minimum Covariance Determinant (MCD): robust estimator of covariance.  The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.  Read more in the :ref:`User Guide <robust_covariance>`.  Parameters ---------- store_precision : bool     Specify if the estimated precision is stored.  assume_centered : Boolean     If True, the support of the robust location and the covariance     estimates is computed, and a covariance estimate is recomputed from     it, without centering the data.     Useful to work with data whose mean is significantly equal to     zero but is not exactly zero.     If False, the robust location and covariance are directly computed     with the FastMCD algorithm without additional treatment.  support_fraction : float, 0 < support_fraction < 1     The proportion of points to be included in the support of the raw     MCD estimate. Default is None, which implies that the minimum     value of support_fraction will be used within the algorithm:     [n_sample + n_features + 1] / 2  random_state : integer or numpy.RandomState, optional     The random generator used. If an integer is given, it fixes the     seed. Defaults to the global numpy random number generator.  Attributes ---------- raw_location_ : array-like, shape (n_features,)     The raw robust estimated location before correction and re-weighting.  raw_covariance_ : array-like, shape (n_features, n_features)     The raw robust estimated covariance before correction and re-weighting.  raw_support_ : array-like, shape (n_samples,)     A mask of the observations that have been used to compute     the raw robust estimates of location and shape, before correction     and re-weighting.  location_ : array-like, shape (n_features,)     Estimated robust location  covariance_ : array-like, shape (n_features, n_features)     Estimated robust covariance matrix  precision_ : array-like, shape (n_features, n_features)     Estimated pseudo inverse matrix.     (stored only if store_precision is True)  support_ : array-like, shape (n_samples,)     A mask of the observations that have been used to compute     the robust estimates of location and shape.  dist_ : array-like, shape (n_samples,)     Mahalanobis distances of the training set (on which `fit` is called)     observations.  References ----------  .. [Rouseeuw1984] `P. J. Rousseeuw. Least median of squares regression.     J. Am Stat Ass, 79:871, 1984.` .. [Rouseeuw1999] `A Fast Algorithm for the Minimum Covariance Determinant     Estimator, 1999, American Statistical Association and the American     Society for Quality, TECHNOMETRICS` .. [Butler1993] `R. W. Butler, P. L. Davies and M. Jhun,     Asymptotics For The Minimum Covariance Determinant Estimator,     The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400`", [
d("__init__(self, store_precision, assume_centered, support_fraction, random_state)"),
d("fit(self, X, y)"),
d("correct_covariance(self, data)"),
d("reweight_covariance(self, data)"),]),
c("BaseLabelPropagation()", "/semi_supervised/label_propagation.py; Base class for label propagation module.  Parameters ---------- kernel : {'knn', 'rbf'}     String identifier for kernel function to use.     Only 'rbf' and 'knn' kernels are currently supported..  gamma : float     Parameter for rbf kernel  alpha : float     Clamping factor  max_iter : float     Change maximum number of iterations allowed  tol : float     Convergence tolerance: threshold to consider the system at steady     state  n_neighbors : integer > 0     Parameter for knn kernel", [
d("__init__(self, kernel, gamma, n_neighbors, alpha, max_iter, tol)"),
d("_get_kernel(self, X, y)"),
d("_build_graph(self)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("fit(self, X, y)"),]),
c("LabelPropagation(BaseLabelPropagation)", "/semi_supervised/label_propagation.py; Label Propagation classifier  Read more in the :ref:`User Guide <label_propagation>`.  Parameters ---------- kernel : {'knn', 'rbf'}     String identifier for kernel function to use.     Only 'rbf' and 'knn' kernels are currently supported..  gamma : float     Parameter for rbf kernel  n_neighbors : integer > 0     Parameter for knn kernel  alpha : float     Clamping factor  max_iter : float     Change maximum number of iterations allowed  tol : float     Convergence tolerance: threshold to consider the system at steady     state  Attributes ---------- X_ : array, shape = [n_samples, n_features]     Input array.  classes_ : array, shape = [n_classes]     The distinct labels used in classifying instances.  label_distributions_ : array, shape = [n_samples, n_classes]     Categorical distribution for each item.  transduction_ : array, shape = [n_samples]     Label assigned to each item via the transduction.  n_iter_ : int     Number of iterations run.  Examples -------- >>> from sklearn import datasets >>> from sklearn.semi_supervised import LabelPropagation >>> label_prop_model = LabelPropagation() >>> iris = datasets.load_iris() >>> random_unlabeled_points = np.where(np.random.random_integers(0, 1, ...    size=len(iris.target))) >>> labels = np.copy(iris.target) >>> labels[random_unlabeled_points] = -1 >>> label_prop_model.fit(iris.data, labels) ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS LabelPropagation(...)  References ---------- Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf  See Also -------- LabelSpreading : Alternate label propagation strategy more robust to noise", [
d("_build_graph(self)"),]),
c("LabelSpreading(BaseLabelPropagation)", "/semi_supervised/label_propagation.py; LabelSpreading model for semi-supervised learning  This model is similar to the basic Label Propgation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.  Read more in the :ref:`User Guide <label_propagation>`.  Parameters ---------- kernel : {'knn', 'rbf'}     String identifier for kernel function to use.     Only 'rbf' and 'knn' kernels are currently supported.  gamma : float   parameter for rbf kernel  n_neighbors : integer > 0   parameter for knn kernel  alpha : float   clamping factor  max_iter : float   maximum number of iterations allowed  tol : float   Convergence tolerance: threshold to consider the system at steady   state  Attributes ---------- X_ : array, shape = [n_samples, n_features]     Input array.  classes_ : array, shape = [n_classes]     The distinct labels used in classifying instances.  label_distributions_ : array, shape = [n_samples, n_classes]     Categorical distribution for each item.  transduction_ : array, shape = [n_samples]     Label assigned to each item via the transduction.  n_iter_ : int     Number of iterations run.  Examples -------- >>> from sklearn import datasets >>> from sklearn.semi_supervised import LabelSpreading >>> label_prop_model = LabelSpreading() >>> iris = datasets.load_iris() >>> random_unlabeled_points = np.where(np.random.random_integers(0, 1, ...    size=len(iris.target))) >>> labels = np.copy(iris.target) >>> labels[random_unlabeled_points] = -1 >>> label_prop_model.fit(iris.data, labels) ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS LabelSpreading(...)  References ---------- Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Schoelkopf. Learning with local and global consistency (2004) http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219  See Also -------- LabelPropagation : Unregularized graph based semi-supervised learning", [
d("__init__(self, kernel, gamma, n_neighbors, alpha, max_iter, tol)"),
d("_build_graph(self)"),]),
c("_PLS(BaseEstimator, TransformerMixin, RegressorMixin)", "/cross_decomposition/pls_.py; Partial Least Squares (PLS)  This class implements the generic PLS algorithm, constructors' parameters allow to obtain a specific implementation such as:  - PLS2 regression, i.e., PLS 2 blocks, mode A, with asymmetric deflation   and unnormalized y weights such as defined by [Tenenhaus 1998] p. 132.   With univariate response it implements PLS1.  - PLS canonical, i.e., PLS 2 blocks, mode A, with symmetric deflation and   normalized y weights such as defined by [Tenenhaus 1998] (p. 132) and   [Wegelin et al. 2000]. This parametrization implements the original Wold   algorithm.  We use the terminology defined by [Wegelin et al. 2000]. This implementation uses the PLS Wold 2 blocks algorithm based on two nested loops:     (i) The outer loop iterate over components.     (ii) The inner loop estimates the weights vectors. This can be done     with two algo. (a) the inner loop of the original NIPALS algo. or (b) a     SVD on residuals cross-covariance matrices.  n_components : int, number of components to keep. (default 2).  scale : boolean, scale data? (default True)  deflation_mode : str, 'canonical' or 'regression'. See notes.  mode : 'A' classical PLS and 'B' CCA. See notes.  norm_y_weights: boolean, normalize Y weights to one? (default False)  algorithm : string, 'nipals' or 'svd'     The algorithm used to estimate the weights. It will be called     n_components times, i.e. once for each iteration of the outer loop.  max_iter : an integer, the maximum number of iterations (default 500)     of the NIPALS inner loop (used only if algorithm='nipals')  tol : non-negative real, default 1e-06     The tolerance used in the iterative algorithm.  copy : boolean, default True     Whether the deflation should be done on a copy. Let the default     value to True unless you don't care about side effects.  Attributes ---------- x_weights_ : array, [p, n_components]     X block weights vectors.  y_weights_ : array, [q, n_components]     Y block weights vectors.  x_loadings_ : array, [p, n_components]     X block loadings vectors.  y_loadings_ : array, [q, n_components]     Y block loadings vectors.  x_scores_ : array, [n_samples, n_components]     X scores.  y_scores_ : array, [n_samples, n_components]     Y scores.  x_rotations_ : array, [p, n_components]     X block to latents rotations.  y_rotations_ : array, [q, n_components]     Y block to latents rotations.  coef_: array, [p, q]     The coefficients of the linear model: ``Y = X coef_ + Err``  n_iter_ : array-like     Number of iterations of the NIPALS inner loop for each     component. Not useful if the algorithm given is 'svd'.  References ----------  Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.  In French but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.  See also -------- PLSCanonical PLSRegression CCA PLS_SVD", [
d("__init__(self, n_components, scale, deflation_mode, mode, algorithm, norm_y_weights, max_iter, tol, copy)"),
d("fit(self, X, Y)"),
d("transform(self, X, Y, copy)"),
d("predict(self, X, copy)"),
d("fit_transform(self, X, y)"),]),
c("PLSRegression(_PLS)", "/cross_decomposition/pls_.py; PLS regression  PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1 in case of one dimensional response. This class inherits from _PLS with mode='A', deflation_mode='regression', norm_y_weights=False and algorithm='nipals'.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- n_components : int, (default 2)     Number of components to keep.  scale : boolean, (default True)     whether to scale the data  max_iter : an integer, (default 500)     the maximum number of iterations of the NIPALS inner loop (used     only if algorithm='nipals')  tol : non-negative real     Tolerance used in the iterative algorithm default 1e-06.  copy : boolean, default True     Whether the deflation should be done on a copy. Let the default     value to True unless you don't care about side effect  Attributes ---------- x_weights_ : array, [p, n_components]     X block weights vectors.  y_weights_ : array, [q, n_components]     Y block weights vectors.  x_loadings_ : array, [p, n_components]     X block loadings vectors.  y_loadings_ : array, [q, n_components]     Y block loadings vectors.  x_scores_ : array, [n_samples, n_components]     X scores.  y_scores_ : array, [n_samples, n_components]     Y scores.  x_rotations_ : array, [p, n_components]     X block to latents rotations.  y_rotations_ : array, [q, n_components]     Y block to latents rotations.  coef_: array, [p, q]     The coefficients of the linear model: ``Y = X coef_ + Err``  n_iter_ : array-like     Number of iterations of the NIPALS inner loop for each     component.  Notes ----- For each component k, find weights u, v that optimizes: ``max corr(Xk u, Yk v) * var(Xk u) var(Yk u)``, such that ``|u| = 1``  Note that it maximizes both the correlations between the scores and the intra-block variances.  The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.  The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.  This implementation provides the same results that 3 PLS packages provided in the R language (R-project):      - 'mixOmics' with function pls(X, Y, mode = 'regression')     - 'plspm ' with function plsreg2(X, Y)     - 'pls' with function oscorespls.fit(X, Y)  Examples -------- >>> from sklearn.cross_decomposition import PLSRegression >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]] >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]] >>> pls2 = PLSRegression(n_components=2) >>> pls2.fit(X, Y) ... # doctest: +NORMALIZE_WHITESPACE PLSRegression(copy=True, max_iter=500, n_components=2, scale=True,         tol=1e-06) >>> Y_pred = pls2.predict(X)  References ----------  Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.  In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.", [
d("__init__(self, n_components, scale, max_iter, tol, copy)"),]),
c("PLSCanonical(_PLS)", "/cross_decomposition/pls_.py; PLSCanonical implements the 2 blocks canonical PLS of the original Wold algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].  This class inherits from PLS with mode='A' and deflation_mode='canonical', norm_y_weights=True and algorithm='nipals', but svd should provide similar results up to numerical errors.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- scale : boolean, scale data? (default True)  algorithm : string, 'nipals' or 'svd'     The algorithm used to estimate the weights. It will be called     n_components times, i.e. once for each iteration of the outer loop.  max_iter : an integer, (default 500)     the maximum number of iterations of the NIPALS inner loop (used     only if algorithm='nipals')  tol : non-negative real, default 1e-06     the tolerance used in the iterative algorithm  copy : boolean, default True     Whether the deflation should be done on a copy. Let the default     value to True unless you don't care about side effect  n_components : int, number of components to keep. (default 2).  Attributes ---------- x_weights_ : array, shape = [p, n_components]     X block weights vectors.  y_weights_ : array, shape = [q, n_components]     Y block weights vectors.  x_loadings_ : array, shape = [p, n_components]     X block loadings vectors.  y_loadings_ : array, shape = [q, n_components]     Y block loadings vectors.  x_scores_ : array, shape = [n_samples, n_components]     X scores.  y_scores_ : array, shape = [n_samples, n_components]     Y scores.  x_rotations_ : array, shape = [p, n_components]     X block to latents rotations.  y_rotations_ : array, shape = [q, n_components]     Y block to latents rotations.  n_iter_ : array-like     Number of iterations of the NIPALS inner loop for each     component. Not useful if the algorithm provided is 'svd'.  Notes ----- For each component k, find weights u, v that optimize:: max corr(Xk u, Yk v) * var(Xk u) var(Yk u), such that ``|u| = |v| = 1``  Note that it maximizes both the correlations between the scores and the intra-block variances.  The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.  The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.  This implementation provides the same results that the 'plspm' package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function ``pls(..., mode = 'canonical')`` of the 'mixOmics' package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one.  Examples -------- >>> from sklearn.cross_decomposition import PLSCanonical >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]] >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]] >>> plsca = PLSCanonical(n_components=2) >>> plsca.fit(X, Y) ... # doctest: +NORMALIZE_WHITESPACE PLSCanonical(algorithm='nipals', copy=True, max_iter=500, n_components=2,              scale=True, tol=1e-06) >>> X_c, Y_c = plsca.transform(X, Y)  References ----------  Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.  Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.  See also -------- CCA PLSSVD", [
d("__init__(self, n_components, scale, algorithm, max_iter, tol, copy)"),]),
c("PLSSVD(BaseEstimator, TransformerMixin)", "/cross_decomposition/pls_.py; Partial Least Square SVD  Simply perform a svd on the crosscovariance matrix: X'Y There are no iterative deflation here.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- n_components : int, default 2     Number of components to keep.  scale : boolean, default True     Whether to scale X and Y.  copy : boolean, default True     Whether to copy X and Y, or perform in-place computations.  Attributes ---------- x_weights_ : array, [p, n_components]     X block weights vectors.  y_weights_ : array, [q, n_components]     Y block weights vectors.  x_scores_ : array, [n_samples, n_components]     X scores.  y_scores_ : array, [n_samples, n_components]     Y scores.  See also -------- PLSCanonical CCA", [
d("__init__(self, n_components, scale, copy)"),
d("fit(self, X, Y)"),
d("transform(self, X, Y)"),
d("fit_transform(self, X, y)"),]),
c("CCA(_PLS)", "/cross_decomposition/cca_.py; CCA Canonical Correlation Analysis.  CCA inherits from PLS with mode='B' and deflation_mode='canonical'.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- n_components : int, (default 2).     number of components to keep.  scale : boolean, (default True)     whether to scale the data?  max_iter : an integer, (default 500)     the maximum number of iterations of the NIPALS inner loop  tol : non-negative real, default 1e-06.     the tolerance used in the iterative algorithm  copy : boolean     Whether the deflation be done on a copy. Let the default value     to True unless you don't care about side effects  Attributes ---------- x_weights_ : array, [p, n_components]     X block weights vectors.  y_weights_ : array, [q, n_components]     Y block weights vectors.  x_loadings_ : array, [p, n_components]     X block loadings vectors.  y_loadings_ : array, [q, n_components]     Y block loadings vectors.  x_scores_ : array, [n_samples, n_components]     X scores.  y_scores_ : array, [n_samples, n_components]     Y scores.  x_rotations_ : array, [p, n_components]     X block to latents rotations.  y_rotations_ : array, [q, n_components]     Y block to latents rotations.  n_iter_ : array-like     Number of iterations of the NIPALS inner loop for each     component.  Notes ----- For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that ``|u| = |v| = 1``  Note that it maximizes only the correlations between the scores.  The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.  The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.  Examples -------- >>> from sklearn.cross_decomposition import CCA >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]] >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]] >>> cca = CCA(n_components=1) >>> cca.fit(X, Y) ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE CCA(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06) >>> X_c, Y_c = cca.transform(X, Y)  References ----------  Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.  In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.  See also -------- PLSCanonical PLSSVD", [
d("__init__(self, n_components, scale, max_iter, tol, copy)"),]),
c("BernoulliRBM(BaseEstimator, TransformerMixin)", "/neural_network/rbm.py; Bernoulli Restricted Boltzmann Machine (RBM).  A Restricted Boltzmann Machine with binary visible units and binary hiddens. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2].  The time complexity of this implementation is ``O(d ** 2)`` assuming d ~ n_features ~ n_components.  Read more in the :ref:`User Guide <rbm>`.  Parameters ---------- n_components : int, optional     Number of binary hidden units.  learning_rate : float, optional     The learning rate for weight updates. It is *highly* recommended     to tune this hyper-parameter. Reasonable values are in the     10**[0., -3.] range.  batch_size : int, optional     Number of examples per minibatch.  n_iter : int, optional     Number of iterations/sweeps over the training dataset to perform     during training.  verbose : int, optional     The verbosity level. The default, zero, means silent mode.  random_state : integer or numpy.RandomState, optional     A random number generator instance to define the state of the     random permutations generator. If an integer is given, it fixes the     seed. Defaults to the global numpy random number generator.  Attributes ---------- intercept_hidden_ : array-like, shape (n_components,)     Biases of the hidden units.  intercept_visible_ : array-like, shape (n_features,)     Biases of the visible units.  components_ : array-like, shape (n_components, n_features)     Weight matrix, where n_features in the number of     visible units and n_components is the number of hidden units.  Examples --------  >>> import numpy as np >>> from sklearn.neural_network import BernoulliRBM >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]) >>> model = BernoulliRBM(n_components=2) >>> model.fit(X) BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,        random_state=None, verbose=0)  References ----------  [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for     deep belief nets. Neural Computation 18, pp 1527-1554.     http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf  [2] Tieleman, T. Training Restricted Boltzmann Machines using     Approximations to the Likelihood Gradient. International Conference     on Machine Learning (ICML) 2008", [
d("__init__(self, n_components, learning_rate, batch_size, n_iter, verbose, random_state)"),
d("transform(self, X)"),
d("_mean_hiddens(self, v)"),
d("_sample_hiddens(self, v, rng)"),
d("_sample_visibles(self, h, rng)"),
d("_free_energy(self, v)"),
d("gibbs(self, v)"),
d("partial_fit(self, X, y)"),
d("_fit(self, v_pos, rng)"),
d("score_samples(self, X)"),
d("fit(self, X, y)"),]),
c("NotAnArray(object)", "/utils/estimator_checks.py; An object that is convertable to an array", [
d("__init__(self, data)"),
d("__array__(self, dtype)"),]),
c("ArraySlicingWrapper(object)", "/utils/mocking.py; ", [
d("__init__(self, array)"),
d("__getitem__(self, aslice)"),]),
c("MockDataFrame(object)", "/utils/mocking.py; ", [
d("__init__(self, array)"),
d("__len__(self)"),
d("__array__(self)"),]),
c("CheckingClassifier(BaseEstimator, ClassifierMixin)", "/utils/mocking.py; Dummy classifier to test pipelining and meta-estimators.  Checks some property of X and y in fit / predict. This allows testing whether pipelines / cross-validation or metaestimators changed the input.", [
d("__init__(self, check_y, check_X, foo_param)"),
d("fit(self, X, y)"),
d("predict(self, T)"),
d("score(self, X, Y)"),]),
c("_IffHasAttrDescriptor(object)", "/utils/metaestimators.py; Implements a conditional property using the descriptor protocol.  Using this class to create a decorator will raise an ``AttributeError`` if the ``attribute_name`` is not present on the base object.  This allows ducktyping of the decorated method based on ``attribute_name``.  See https://docs.python.org/3/howto/descriptor.html for an explanation of descriptors.", [
d("__init__(self, fn, attribute_name)"),
d("__get__(self, obj, type)"),]),
c("_HungarianState(object)", "/utils/linear_assignment_.py; State of one execution of the Hungarian algorithm.  Parameters ---------- cost_matrix : 2D matrix     The cost matrix. Does not have to be square.", [
d("__init__(self, cost_matrix)"),
d("_find_prime_in_row(self, row)"),
d("_clear_covers(self)"),]),
c("_IgnoreWarnings(object)", "/utils/testing.py; Improved and simplified Python warnings context manager  Copied from Python 2.7.5 and modified as required.", [
d("__init__(self)"),
d("__repr__(self)"),
d("__enter__(self)"),
d("__exit__(self)"),]),
c("mock_mldata_urlopen(object)", "/utils/testing.py; ", [
d("__init__(self, mock_datasets)"),
d("__call__(self, urlname)"),]),
c("TempMemmap(object)", "/utils/testing.py; ", [
d("__init__(self, data, mmap_mode)"),
d("__enter__(self)"),
d("__exit__(self, exc_type, exc_val, exc_tb)"),]),
c("_LineSearchError(RuntimeError)", "/utils/optimize.py; ", []),
c("deprecated(object)", "/utils/__init__.py; Decorator to mark a function or class as deprecated.  Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.  The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:  >>> from sklearn.utils import deprecated >>> deprecated() # doctest: +ELLIPSIS <sklearn.utils.deprecated object at ...>  >>> @deprecated() ... def some_function(): pass", [
d("__init__(self, extra)"),
d("__call__(self, obj)"),
d("_decorate_class(self, cls)"),
d("_decorate_fun(self, fun)"),
d("_update_doc(self, olddoc)"),]),
c("ConvergenceWarning(UserWarning)", "/utils/__init__.py; Custom warning to capture convergence problems", []),
c("DataDimensionalityWarning(UserWarning)", "/utils/__init__.py; Custom warning to notify potential issues with data dimensionality", []),
c("DataConversionWarning(UserWarning)", "/utils/validation.py; A warning on implicit data conversions happening in the code", []),
c("NonBLASDotWarning(UserWarning)", "/utils/validation.py; A warning on implicit dispatch to numpy.dot", []),
c("NotFittedError(ValueError, AttributeError)", "/utils/validation.py; Exception class to raise if estimator is used before fitting  This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.", []),
c("ArpackError(RuntimeError)", "/utils/arpack.py; ARPACK error", [
d("__init__(self, info, infodict)"),]),
c("ArpackNoConvergence(ArpackError)", "/utils/arpack.py; ARPACK iteration did not converge  Attributes ---------- eigenvalues : ndarray     Partial result. Converged eigenvalues. eigenvectors : ndarray     Partial result. Converged eigenvectors.", [
d("__init__(self, msg, eigenvalues, eigenvectors)"),]),
c("_ArpackParams(object)", "/utils/arpack.py; ", [
d("__init__(self, n, k, tp, mode, sigma, ncv, v0, maxiter, which, tol)"),
d("_raise_no_convergence(self)"),]),
c("_SymmetricArpackParams(_ArpackParams)", "/utils/arpack.py; ", [
d("__init__(self, n, k, tp, matvec, mode, M_matvec, Minv_matvec, sigma, ncv, v0, maxiter, which, tol)"),
d("iterate(self)"),
d("extract(self, return_eigenvectors)"),]),
c("_UnsymmetricArpackParams(_ArpackParams)", "/utils/arpack.py; ", [
d("__init__(self, n, k, tp, matvec, mode, M_matvec, Minv_matvec, sigma, ncv, v0, maxiter, which, tol)"),
d("iterate(self)"),
d("extract(self, return_eigenvectors)"),]),
c("SpLuInv(LinearOperator)", "/utils/arpack.py; SpLuInv:    helper class to repeatedly solve M*x=b    using a sparse LU-decopposition of M", [
d("__init__(self, M)"),
d("_matvec(self, x)"),]),
c("LuInv(LinearOperator)", "/utils/arpack.py; LuInv:    helper class to repeatedly solve M*x=b    using an LU-decomposition of M", [
d("__init__(self, M)"),
d("_matvec(self, x)"),]),
c("IterInv(LinearOperator)", "/utils/arpack.py; IterInv:    helper class to repeatedly solve M*x=b    using an iterative method.", [
d("__init__(self, M, ifunc, tol)"),
d("_matvec(self, x)"),]),
c("IterOpInv(LinearOperator)", "/utils/arpack.py; IterOpInv:    helper class to repeatedly solve [A-sigma*M]*x = b    using an iterative method", [
d("__init__(self, A, M, sigma, ifunc, tol)"),
d("mult_func(self, x)"),
d("mult_func_M_None(self, x)"),
d("_matvec(self, x)"),]),
c("CorrectNotFittedError(ValueError)", "/utils/tests/test_estimator_checks.py; Exception class to raise if estimator is used before fitting.  Like NotFittedError, it inherits from ValueError, but not from AttributeError. Used for testing only.", []),
c("BaseBadClassifier(BaseEstimator, ClassifierMixin)", "/utils/tests/test_estimator_checks.py; ", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("NoCheckinPredict(BaseBadClassifier)", "/utils/tests/test_estimator_checks.py; ", [
d("fit(self, X, y)"),]),
c("NoSparseClassifier(BaseBadClassifier)", "/utils/tests/test_estimator_checks.py; ", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("CorrectNotFittedErrorClassifier(BaseBadClassifier)", "/utils/tests/test_estimator_checks.py; ", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("Prefix(object)", "/utils/tests/test_metaestimators.py; ", [
d("func()"),]),
c("MockMetaEstimator(object)", "/utils/tests/test_metaestimators.py; This is a mock meta estimator", [
d("func(self)"),]),
c("NotAnArray(object)", "/utils/tests/test_multiclass.py; An object that is convertable to an array. This is useful to simulate a Pandas timeseries.", [
d("__init__(self, data)"),
d("__array__(self)"),]),
c("TestWarns()", "/utils/tests/test_testing.py; ", [
d("test_warn(self)"),
d("test_warn_wrong_warning(self)"),]),
c("OrthogonalMatchingPursuit(LinearModel, RegressorMixin)", "/linear_model/omp.py; Orthogonal Matching Pursuit model (OMP)  Parameters ---------- n_nonzero_coefs : int, optional     Desired number of non-zero entries in the solution. If None (by     default) this value is set to 10% of n_features.  tol : float, optional     Maximum norm of the residual. If not None, overrides n_nonzero_coefs.  fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional     If False, the regressors X are assumed to be already normalized.  precompute : {True, False, 'auto'}, default 'auto'     Whether to use a precomputed Gram and Xy matrix to speed up     calculations. Improves performance when `n_targets` or `n_samples` is     very large. Note that if you already have such matrices, you can pass     them directly to the fit method.  Read more in the :ref:`User Guide <omp>`.  Attributes ---------- coef_ : array, shape (n_features,) or (n_features, n_targets)     parameter vector (w in the formula)  intercept_ : float or array, shape (n_targets,)     independent term in decision function.  n_iter_ : int or array-like     Number of active features across every target.  Notes ----- Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf  See also -------- orthogonal_mp orthogonal_mp_gram lars_path Lars LassoLars decomposition.sparse_encode", [
d("__init__(self, n_nonzero_coefs, tol, fit_intercept, normalize, precompute)"),
d("fit(self, X, y)"),]),
c("OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin)", "/linear_model/omp.py; Cross-validated Orthogonal Matching Pursuit model (OMP)  Parameters ---------- copy : bool, optional     Whether the design matrix X must be copied by the algorithm. A false     value is only helpful if X is already Fortran-ordered, otherwise a     copy is made anyway.  fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional     If False, the regressors X are assumed to be already normalized.  max_iter : integer, optional     Maximum numbers of iterations to perform, therefore maximum features     to include. 10% of ``n_features`` but at least 5 if available.  cv : cross-validation generator, optional     see :mod:`sklearn.cross_validation`. If ``None`` is passed, default to     a 5-fold strategy  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs  verbose : boolean or integer, optional     Sets the verbosity amount  Read more in the :ref:`User Guide <omp>`.  Attributes ---------- intercept_ : float or array, shape (n_targets,)     Independent term in decision function.  coef_ : array, shape (n_features,) or (n_features, n_targets)     Parameter vector (w in the problem formulation).  n_nonzero_coefs_ : int     Estimated number of non-zero coefficients giving the best mean squared     error over the cross-validation folds.  n_iter_ : int or array-like     Number of active features across every target for the model refit with     the best hyperparameters got by cross-validating across all folds.  See also -------- orthogonal_mp orthogonal_mp_gram lars_path Lars LassoLars OrthogonalMatchingPursuit LarsCV LassoLarsCV decomposition.sparse_encode", [
d("__init__(self, copy, fit_intercept, normalize, max_iter, cv, n_jobs, verbose)"),
d("fit(self, X, y)"),]),
c("TheilSenRegressor(LinearModel, RegressorMixin)", "/linear_model/theil_sen.py; Theil-Sen Estimator: robust multivariate regression model.  The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is 'n_samples choose n_subsamples', it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.  Read more in the :ref:`User Guide <theil_sen_regression>`.  Parameters ---------- fit_intercept : boolean, optional, default True     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  max_subpopulation : int, optional, default 1e4     Instead of computing with a set of cardinality 'n choose k', where n is     the number of samples and k is the number of subsamples (at least     number of features), consider only a stochastic subpopulation of a     given maximal size if 'n choose k' is larger than max_subpopulation.     For other than small problem sizes this parameter will determine     memory usage and runtime if n_subsamples is not changed.  n_subsamples : int, optional, default None     Number of samples to calculate the parameters. This is at least the     number of features (plus 1 if fit_intercept=True) and the number of     samples as a maximum. A lower number leads to a higher breakdown     point and a low efficiency while a high number leads to a low     breakdown point and a high efficiency. If None, take the     minimum number of subsamples leading to maximal robustness.     If n_subsamples is set to n_samples, Theil-Sen is identical to least     squares.  max_iter : int, optional, default 300     Maximum number of iterations for the calculation of spatial median.  tol : float, optional, default 1.e-3     Tolerance when calculating spatial median.  random_state : RandomState or an int seed, optional, default None     A random number generator instance to define the state of the     random permutations generator.  n_jobs : integer, optional, default 1     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs.  verbose : boolean, optional, default False     Verbose mode when fitting the model.  Attributes ---------- coef_ : array, shape = (n_features)     Coefficients of the regression model (median of distribution).  intercept_ : float     Estimated intercept of regression model.  breakdown_ : float     Approximated breakdown point.  n_iter_ : int     Number of iterations needed for the spatial median.  n_subpopulation_ : int     Number of combinations taken into account from 'n choose k', where n is     the number of samples and k is the number of subsamples.  References ---------- - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009   Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang   http://www.math.iupui.edu/~hpeng/MTSE_0908.pdf", [
d("__init__(self, fit_intercept, copy_X, max_subpopulation, n_subsamples, max_iter, tol, random_state, n_jobs, verbose)"),
d("_check_subparams(self, n_samples, n_features)"),
d("fit(self, X, y)"),]),
c("Lars(LinearModel, RegressorMixin)", "/linear_model/least_angle.py; Least Angle Regression model a.k.a. LAR  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- n_nonzero_coefs : int, optional     Target number of non-zero coefficients. Use ``np.inf`` for no limit.  fit_intercept : boolean     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  eps : float, optional     The machine-precision regularization in the computation of the     Cholesky diagonal factors. Increase this for very ill-conditioned     systems. Unlike the ``tol`` parameter in some iterative     optimization-based algorithms, this parameter does not control     the tolerance of the optimization.  fit_path : boolean     If True the full path is stored in the ``coef_path_`` attribute.     If you compute the solution for a large problem or many targets,     setting ``fit_path`` to ``False`` will lead to a speedup, especially     with a small alpha.  Attributes ---------- alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays     Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller.  active_ : list, length = n_alphas | list of n_targets such lists     Indices of active variables at the end of the path.  coef_path_ : array, shape (n_features, n_alphas + 1)         | list of n_targets such arrays     The varying values of the coefficients along the path. It is not     present if the ``fit_path`` parameter is ``False``.  coef_ : array, shape (n_features,) or (n_targets, n_features)     Parameter vector (w in the formulation formula).  intercept_ : float | array, shape (n_targets,)     Independent term in decision function.  n_iter_ : array-like or int     The number of iterations taken by lars_path to find the     grid of alphas for each target.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.Lars(n_nonzero_coefs=1) >>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111]) ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,    n_nonzero_coefs=1, normalize=True, precompute='auto', verbose=False) >>> print(clf.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE [ 0. -1.11...]  See also -------- lars_path, LarsCV sklearn.decomposition.sparse_encode", [
d("__init__(self, fit_intercept, verbose, normalize, precompute, n_nonzero_coefs, eps, copy_X, fit_path)"),
d("_get_gram(self)"),
d("fit(self, X, y, Xy)"),]),
c("LassoLars(Lars)", "/linear_model/least_angle.py; Lasso model fit with Least Angle Regression a.k.a. Lars  It is a Linear Model trained with an L1 prior as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- alpha : float     Constant that multiplies the penalty term. Defaults to 1.0.     ``alpha = 0`` is equivalent to an ordinary least square, solved     by :class:`LinearRegression`. For numerical reasons, using     ``alpha = 0`` with the LassoLars object is not advised and you     should prefer the LinearRegression object.  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  max_iter : integer, optional     Maximum number of iterations to perform.  eps : float, optional     The machine-precision regularization in the computation of the     Cholesky diagonal factors. Increase this for very ill-conditioned     systems. Unlike the ``tol`` parameter in some iterative     optimization-based algorithms, this parameter does not control     the tolerance of the optimization.  fit_path : boolean     If ``True`` the full path is stored in the ``coef_path_`` attribute.     If you compute the solution for a large problem or many targets,     setting ``fit_path`` to ``False`` will lead to a speedup, especially     with a small alpha.  Attributes ---------- alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays     Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.  active_ : list, length = n_alphas | list of n_targets such lists     Indices of active variables at the end of the path.  coef_path_ : array, shape (n_features, n_alphas + 1) or list     If a list is passed it's expected to be one of n_targets such arrays.     The varying values of the coefficients along the path. It is not     present if the ``fit_path`` parameter is ``False``.  coef_ : array, shape (n_features,) or (n_targets, n_features)     Parameter vector (w in the formulation formula).  intercept_ : float | array, shape (n_targets,)     Independent term in decision function.  n_iter_ : array-like or int.     The number of iterations taken by lars_path to find the     grid of alphas for each target.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.LassoLars(alpha=0.01) >>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1]) ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,      fit_path=True, max_iter=500, normalize=True, precompute='auto',      verbose=False) >>> print(clf.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE [ 0.         -0.963257...]  See also -------- lars_path lasso_path Lasso LassoCV LassoLarsCV sklearn.decomposition.sparse_encode", [
d("__init__(self, alpha, fit_intercept, verbose, normalize, precompute, max_iter, eps, copy_X, fit_path)"),]),
c("LarsCV(Lars)", "/linear_model/least_angle.py; Cross-validated Least Angle Regression model  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  max_iter: integer, optional     Maximum number of iterations to perform.  cv : cross-validation generator, optional     see :mod:`sklearn.cross_validation`. If ``None`` is passed, default to     a 5-fold strategy  max_n_alphas : integer, optional     The maximum number of points on the path used to compute the     residuals in the cross-validation  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs  eps : float, optional     The machine-precision regularization in the computation of the     Cholesky diagonal factors. Increase this for very ill-conditioned     systems.   Attributes ---------- coef_ : array, shape (n_features,)     parameter vector (w in the formulation formula)  intercept_ : float     independent term in decision function  coef_path_ : array, shape (n_features, n_alphas)     the varying values of the coefficients along the path  alpha_ : float     the estimated regularization parameter alpha  alphas_ : array, shape (n_alphas,)     the different values of alpha along the path  cv_alphas_ : array, shape (n_cv_alphas,)     all the values of alpha along the path for the different folds  cv_mse_path_ : array, shape (n_folds, n_cv_alphas)     the mean square error on left-out for each fold along the path     (alpha values given by ``cv_alphas``)  n_iter_ : array-like or int     the number of iterations run by Lars with the optimal alpha.  See also -------- lars_path, LassoLars, LassoLarsCV", [
d("__init__(self, fit_intercept, verbose, max_iter, normalize, precompute, cv, max_n_alphas, n_jobs, eps, copy_X)"),
d("fit(self, X, y)"),
d("alpha(self)"),]),
c("LassoLarsCV(LarsCV)", "/linear_model/least_angle.py; Cross-validated Lasso, using the LARS algorithm  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  max_iter : integer, optional     Maximum number of iterations to perform.  cv : cross-validation generator, optional     see sklearn.cross_validation module. If None is passed, default to     a 5-fold strategy  max_n_alphas : integer, optional     The maximum number of points on the path used to compute the     residuals in the cross-validation  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs  eps : float, optional     The machine-precision regularization in the computation of the     Cholesky diagonal factors. Increase this for very ill-conditioned     systems.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  Attributes ---------- coef_ : array, shape (n_features,)     parameter vector (w in the formulation formula)  intercept_ : float     independent term in decision function.  coef_path_ : array, shape (n_features, n_alphas)     the varying values of the coefficients along the path  alpha_ : float     the estimated regularization parameter alpha  alphas_ : array, shape (n_alphas,)     the different values of alpha along the path  cv_alphas_ : array, shape (n_cv_alphas,)     all the values of alpha along the path for the different folds  cv_mse_path_ : array, shape (n_folds, n_cv_alphas)     the mean square error on left-out for each fold along the path     (alpha values given by ``cv_alphas``)  n_iter_ : array-like or int     the number of iterations run by Lars with the optimal alpha.  Notes -----  The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.  It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.  See also -------- lars_path, LassoLars, LarsCV, LassoCV", []),
c("LassoLarsIC(LassoLars)", "/linear_model/least_angle.py; Lasso model fit with Lars using BIC or AIC for model selection  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- criterion : 'bic' | 'aic'     The type of criterion to use.  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  max_iter : integer, optional     Maximum number of iterations to perform. Can be used for     early stopping.  eps : float, optional     The machine-precision regularization in the computation of the     Cholesky diagonal factors. Increase this for very ill-conditioned     systems. Unlike the ``tol`` parameter in some iterative     optimization-based algorithms, this parameter does not control     the tolerance of the optimization.   Attributes ---------- coef_ : array, shape (n_features,)     parameter vector (w in the formulation formula)  intercept_ : float     independent term in decision function.  alpha_ : float     the alpha parameter chosen by the information criterion  n_iter_ : int     number of iterations run by lars_path to find the grid of     alphas.  criterion_ : array, shape (n_alphas,)     The value of the information criteria ('aic', 'bic') across all     alphas. The alpha which has the smallest information criteria     is chosen.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.LassoLarsIC(criterion='bic') >>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111]) ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,       max_iter=500, normalize=True, precompute='auto',       verbose=False) >>> print(clf.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE [ 0.  -1.11...]  Notes ----- The estimation of the number of degrees of freedom is given by:  'On the degrees of freedom of the lasso' Hui Zou, Trevor Hastie, and Robert Tibshirani Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.  http://en.wikipedia.org/wiki/Akaike_information_criterion http://en.wikipedia.org/wiki/Bayesian_information_criterion  See also -------- lars_path, LassoLars, LassoLarsCV", [
d("__init__(self, criterion, fit_intercept, verbose, normalize, precompute, max_iter, eps, copy_X)"),
d("fit(self, X, y, copy_X)"),]),
c("BayesianRidge(LinearModel, RegressorMixin)", "/linear_model/bayes.py; Bayesian ridge regression  Fit a Bayesian ridge model and optimize the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`.  Parameters ---------- n_iter : int, optional     Maximum number of iterations.  Default is 300.  tol : float, optional     Stop the algorithm if w has converged. Default is 1.e-3.  alpha_1 : float, optional     Hyper-parameter : shape parameter for the Gamma distribution prior     over the alpha parameter. Default is 1.e-6  alpha_2 : float, optional     Hyper-parameter : inverse scale parameter (rate parameter) for the     Gamma distribution prior over the alpha parameter.     Default is 1.e-6.  lambda_1 : float, optional     Hyper-parameter : shape parameter for the Gamma distribution prior     over the lambda parameter. Default is 1.e-6.  lambda_2 : float, optional     Hyper-parameter : inverse scale parameter (rate parameter) for the     Gamma distribution prior over the lambda parameter.     Default is 1.e-6  compute_score : boolean, optional     If True, compute the objective function at each step of the model.     Default is False  fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).     Default is True.  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  verbose : boolean, optional, default False     Verbose mode when fitting the model.   Attributes ---------- coef_ : array, shape = (n_features)     Coefficients of the regression model (mean of distribution)  alpha_ : float    estimated precision of the noise.  lambda_ : array, shape = (n_features)    estimated precisions of the weights.  scores_ : float     if computed, value of the objective function (to be maximized)  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.BayesianRidge() >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) ... # doctest: +NORMALIZE_WHITESPACE BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,         copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,         n_iter=300, normalize=False, tol=0.001, verbose=False) >>> clf.predict([[1, 1]]) array([ 1.])  Notes ----- See examples/linear_model/plot_bayesian_ridge.py for an example.", [
d("__init__(self, n_iter, tol, alpha_1, alpha_2, lambda_1, lambda_2, compute_score, fit_intercept, normalize, copy_X, verbose)"),
d("fit(self, X, y)"),]),
c("ARDRegression(LinearModel, RegressorMixin)", "/linear_model/bayes.py; Bayesian ARD regression.  Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)  Read more in the :ref:`User Guide <bayesian_regression>`.  Parameters ---------- n_iter : int, optional     Maximum number of iterations. Default is 300  tol : float, optional     Stop the algorithm if w has converged. Default is 1.e-3.  alpha_1 : float, optional     Hyper-parameter : shape parameter for the Gamma distribution prior     over the alpha parameter. Default is 1.e-6.  alpha_2 : float, optional     Hyper-parameter : inverse scale parameter (rate parameter) for the     Gamma distribution prior over the alpha parameter. Default is 1.e-6.  lambda_1 : float, optional     Hyper-parameter : shape parameter for the Gamma distribution prior     over the lambda parameter. Default is 1.e-6.  lambda_2 : float, optional     Hyper-parameter : inverse scale parameter (rate parameter) for the     Gamma distribution prior over the lambda parameter. Default is 1.e-6.  compute_score : boolean, optional     If True, compute the objective function at each step of the model.     Default is False.  threshold_lambda : float, optional     threshold for removing (pruning) weights with high precision from     the computation. Default is 1.e+4.  fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).     Default is True.  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True.     If True, X will be copied; else, it may be overwritten.  verbose : boolean, optional, default False     Verbose mode when fitting the model.  Attributes ---------- coef_ : array, shape = (n_features)     Coefficients of the regression model (mean of distribution)  alpha_ : float    estimated precision of the noise.  lambda_ : array, shape = (n_features)    estimated precisions of the weights.  sigma_ : array, shape = (n_features, n_features)     estimated variance-covariance matrix of the weights  scores_ : float     if computed, value of the objective function (to be maximized)  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.ARDRegression() >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) ... # doctest: +NORMALIZE_WHITESPACE ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,         copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,         n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,         verbose=False) >>> clf.predict([[1, 1]]) array([ 1.])  Notes -------- See examples/linear_model/plot_ard.py for an example.", [
d("__init__(self, n_iter, tol, alpha_1, alpha_2, lambda_1, lambda_2, compute_score, threshold_lambda, fit_intercept, normalize, copy_X, verbose)"),
d("fit(self, X, y)"),]),
c("PassiveAggressiveClassifier(BaseSGDClassifier)", "/linear_model/passive_aggressive.py; Passive Aggressive Classifier  Read more in the :ref:`User Guide <passive_aggressive>`.  Parameters ----------  C : float     Maximum step size (regularization). Defaults to 1.0.  fit_intercept : bool, default=False     Whether the intercept should be estimated or not. If False, the     data is assumed to be already centered.  n_iter : int, optional     The number of passes over the training data (aka epochs).     Defaults to 5.  shuffle : bool, default=True     Whether or not the training data should be shuffled after each epoch.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data.  verbose : integer, optional     The verbosity level  n_jobs : integer, optional     The number of CPUs to use to do the OVA (One Versus All, for     multi-class problems) computation. -1 means 'all CPUs'. Defaults     to 1.  loss : string, optional     The loss function to be used:     hinge: equivalent to PA-I in the reference paper.     squared_hinge: equivalent to PA-II in the reference paper.  warm_start : bool, optional     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  Attributes ---------- coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]     Weights assigned to the features.  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]     Constants in decision function.  See also --------  SGDClassifier Perceptron  References ---------- Online Passive-Aggressive Algorithms <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)", [
d("__init__(self, C, fit_intercept, n_iter, shuffle, verbose, loss, n_jobs, random_state, warm_start)"),
d("partial_fit(self, X, y, classes)"),
d("fit(self, X, y, coef_init, intercept_init)"),]),
c("PassiveAggressiveRegressor(BaseSGDRegressor)", "/linear_model/passive_aggressive.py; Passive Aggressive Regressor  Read more in the :ref:`User Guide <passive_aggressive>`.  Parameters ----------  C : float     Maximum step size (regularization). Defaults to 1.0.  epsilon : float     If the difference between the current prediction and the correct label     is below this threshold, the model is not updated.  fit_intercept : bool     Whether the intercept should be estimated or not. If False, the     data is assumed to be already centered. Defaults to True.  n_iter : int, optional     The number of passes over the training data (aka epochs).     Defaults to 5.  shuffle : bool, default=True     Whether or not the training data should be shuffled after each epoch.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data.  verbose : integer, optional     The verbosity level  loss : string, optional     The loss function to be used:     epsilon_insensitive: equivalent to PA-I in the reference paper.     squared_epsilon_insensitive: equivalent to PA-II in the reference     paper.  warm_start : bool, optional     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  Attributes ---------- coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]     Weights assigned to the features.  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]     Constants in decision function.  See also --------  SGDRegressor  References ---------- Online Passive-Aggressive Algorithms <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)", [
d("__init__(self, C, fit_intercept, n_iter, shuffle, verbose, loss, epsilon, random_state, class_weight, warm_start)"),
d("partial_fit(self, X, y)"),
d("fit(self, X, y, coef_init, intercept_init)"),]),
c("BaseRandomizedLinearModel()", "/linear_model/randomized_l1.py; Base class to implement randomized linear models for feature selection  This implements the strategy by Meinshausen and Buhlman: stability selection with randomized sampling, and random re-weighting of the penalty.", [
d("__init__(self)"),
d("fit(self, X, y)"),
d("_make_estimator_and_params(self, X, y)"),
d("get_support(self, indices)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),]),
c("RandomizedLasso(BaseRandomizedLinearModel)", "/linear_model/randomized_l1.py; Randomized Lasso.  Randomized Lasso works by resampling the train data and computing a Lasso on each resampling. In short, the features selected more often are good features. It is also known as stability selection.  Read more in the :ref:`User Guide <randomized_l1>`.  Parameters ---------- alpha : float, 'aic', or 'bic', optional     The regularization parameter alpha parameter in the Lasso.     Warning: this is not the alpha parameter in the stability selection     article which is scaling.  scaling : float, optional     The alpha parameter in the stability selection article used to     randomly scale the features. Should be between 0 and 1.  sample_fraction : float, optional     The fraction of samples to be used in each randomized design.     Should be between 0 and 1. If 1, all samples are used.  n_resampling : int, optional     Number of randomized models.  selection_threshold: float, optional     The score above which features should be selected.  fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default True     If True, the regressors X will be normalized before regression.  precompute : True | False | 'auto'     Whether to use a precomputed Gram matrix to speed up     calculations. If set to 'auto' let us decide. The Gram     matrix can also be passed as argument.  max_iter : integer, optional     Maximum number of iterations to perform in the Lars algorithm.  eps : float, optional     The machine-precision regularization in the computation of the     Cholesky diagonal factors. Increase this for very ill-conditioned     systems. Unlike the 'tol' parameter in some iterative     optimization-based algorithms, this parameter does not control     the tolerance of the optimization.  n_jobs : integer, optional     Number of CPUs to use during the resampling. If '-1', use     all the CPUs  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  pre_dispatch : int, or string, optional     Controls the number of jobs that get dispatched during parallel     execution. Reducing this number can be useful to avoid an     explosion of memory consumption when more jobs get dispatched     than CPUs can process. This parameter can be:          - None, in which case all the jobs are immediately           created and spawned. Use this for lightweight and           fast-running jobs, to avoid delays due to on-demand           spawning of the jobs          - An int, giving the exact number of total jobs that are           spawned          - A string, giving an expression as a function of n_jobs,           as in '2*n_jobs'  memory : Instance of joblib.Memory or string     Used for internal caching. By default, no caching is done.     If a string is given, it is the path to the caching directory.  Attributes ---------- scores_ : array, shape = [n_features]     Feature scores between 0 and 1.  all_scores_ : array, shape = [n_features, n_reg_parameter]     Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max of         ``all_scores_``.  Examples -------- >>> from sklearn.linear_model import RandomizedLasso >>> randomized_lasso = RandomizedLasso()  Notes ----- See examples/linear_model/plot_sparse_recovery.py for an example.  References ---------- Stability selection Nicolai Meinshausen, Peter Buhlmann Journal of the Royal Statistical Society: Series B Volume 72, Issue 4, pages 417-473, September 2010 DOI: 10.1111/j.1467-9868.2010.00740.x  See also -------- RandomizedLogisticRegression, LogisticRegression", [
d("__init__(self, alpha, scaling, sample_fraction, n_resampling, selection_threshold, fit_intercept, verbose, normalize, precompute, max_iter, eps, random_state, n_jobs, pre_dispatch, memory)"),
d("_make_estimator_and_params(self, X, y)"),]),
c("RandomizedLogisticRegression(BaseRandomizedLinearModel)", "/linear_model/randomized_l1.py; Randomized Logistic Regression  Randomized Regression works by resampling the train data and computing a LogisticRegression on each resampling. In short, the features selected more often are good features. It is also known as stability selection.  Read more in the :ref:`User Guide <randomized_l1>`.  Parameters ---------- C : float, optional, default=1     The regularization parameter C in the LogisticRegression.  scaling : float, optional, default=0.5     The alpha parameter in the stability selection article used to     randomly scale the features. Should be between 0 and 1.  sample_fraction : float, optional, default=0.75     The fraction of samples to be used in each randomized design.     Should be between 0 and 1. If 1, all samples are used.  n_resampling : int, optional, default=200     Number of randomized models.  selection_threshold : float, optional, default=0.25     The score above which features should be selected.  fit_intercept : boolean, optional, default=True     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  verbose : boolean or integer, optional     Sets the verbosity amount  normalize : boolean, optional, default=True     If True, the regressors X will be normalized before regression.  tol : float, optional, default=1e-3      tolerance for stopping criteria of LogisticRegression  n_jobs : integer, optional     Number of CPUs to use during the resampling. If '-1', use     all the CPUs  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  pre_dispatch : int, or string, optional     Controls the number of jobs that get dispatched during parallel     execution. Reducing this number can be useful to avoid an     explosion of memory consumption when more jobs get dispatched     than CPUs can process. This parameter can be:          - None, in which case all the jobs are immediately           created and spawned. Use this for lightweight and           fast-running jobs, to avoid delays due to on-demand           spawning of the jobs          - An int, giving the exact number of total jobs that are           spawned          - A string, giving an expression as a function of n_jobs,           as in '2*n_jobs'  memory : Instance of joblib.Memory or string     Used for internal caching. By default, no caching is done.     If a string is given, it is the path to the caching directory.  Attributes ---------- scores_ : array, shape = [n_features]     Feature scores between 0 and 1.  all_scores_ : array, shape = [n_features, n_reg_parameter]     Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max         of ``all_scores_``.  Examples -------- >>> from sklearn.linear_model import RandomizedLogisticRegression >>> randomized_logistic = RandomizedLogisticRegression()  Notes ----- See examples/linear_model/plot_sparse_recovery.py for an example.  References ---------- Stability selection Nicolai Meinshausen, Peter Buhlmann Journal of the Royal Statistical Society: Series B Volume 72, Issue 4, pages 417-473, September 2010 DOI: 10.1111/j.1467-9868.2010.00740.x  See also -------- RandomizedLasso, Lasso, ElasticNet", [
d("__init__(self, C, scaling, sample_fraction, n_resampling, selection_threshold, tol, fit_intercept, verbose, normalize, random_state, n_jobs, pre_dispatch, memory)"),
d("_make_estimator_and_params(self, X, y)"),
d("_center_data(self, X, y, fit_intercept, normalize)"),]),
c("RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin)", "/linear_model/ransac.py; RANSAC (RANdom SAmple Consensus) algorithm.  RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. More information can be found in the general documentation of linear models.  A detailed description of the algorithm can be found in the documentation of the ``linear_model`` sub-package.  Read more in the :ref:`User Guide <RansacRegression>`.  Parameters ---------- base_estimator : object, optional     Base estimator object which implements the following methods:       * `fit(X, y)`: Fit model to given training data and target values.      * `score(X, y)`: Returns the mean accuracy on the given test data,        which is used for the stop criterion defined by `stop_score`.        Additionally, the score is used to decide which of two equally        large consensus sets is chosen as the better one.      If `base_estimator` is None, then     ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for     target values of dtype float.      Note that the current implementation only supports regression     estimators.  min_samples : int (>= 1) or float ([0, 1]), optional     Minimum number of samples chosen randomly from original data. Treated     as an absolute number of samples for `min_samples >= 1`, treated as a     relative number `ceil(min_samples * X.shape[0]`) for     `min_samples < 1`. This is typically chosen as the minimal number of     samples necessary to estimate the given `base_estimator`. By default a     ``sklearn.linear_model.LinearRegression()`` estimator is assumed and     `min_samples` is chosen as ``X.shape[1] + 1``.  residual_threshold : float, optional     Maximum residual for a data sample to be classified as an inlier.     By default the threshold is chosen as the MAD (median absolute     deviation) of the target values `y`.  is_data_valid : callable, optional     This function is called with the randomly selected data before the     model is fitted to it: `is_data_valid(X, y)`. If its return value is     False the current randomly chosen sub-sample is skipped.  is_model_valid : callable, optional     This function is called with the estimated model and the randomly     selected data: `is_model_valid(model, X, y)`. If its return value is     False the current randomly chosen sub-sample is skipped.     Rejecting samples with this function is computationally costlier than     with `is_data_valid`. `is_model_valid` should therefore only be used if     the estimated model is needed for making the rejection decision.  max_trials : int, optional     Maximum number of iterations for random sample selection.  stop_n_inliers : int, optional     Stop iteration if at least this number of inliers are found.  stop_score : float, optional     Stop iteration if score is greater equal than this threshold.  stop_probability : float in range [0, 1], optional     RANSAC iteration stops if at least one outlier-free set of the training     data is sampled in RANSAC. This requires to generate at least N     samples (iterations)::          N >= log(1 - probability) / log(1 - e**m)      where the probability (confidence) is typically set to high value such     as 0.99 (the default) and e is the current fraction of inliers w.r.t.     the total number of samples.  residual_metric : callable, optional     Metric to reduce the dimensionality of the residuals to 1 for     multi-dimensional target values ``y.shape[1] > 1``. By default the sum     of absolute differences is used::          lambda dy: np.sum(np.abs(dy), axis=1)  random_state : integer or numpy.RandomState, optional     The generator used to initialize the centers. If an integer is     given, it fixes the seed. Defaults to the global numpy random     number generator.  Attributes ---------- estimator_ : object     Best fitted model (copy of the `base_estimator` object).  n_trials_ : int     Number of random selection trials until one of the stop criteria is     met. It is always ``<= max_trials``.  inlier_mask_ : bool array of shape [n_samples]     Boolean mask of inliers classified as ``True``.  References ---------- .. [1] http://en.wikipedia.org/wiki/RANSAC .. [2] http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf", [
d("__init__(self, base_estimator, min_samples, residual_threshold, is_data_valid, is_model_valid, max_trials, stop_n_inliers, stop_score, stop_probability, residual_metric, random_state)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("score(self, X, y)"),]),
c("LogisticRegression(BaseEstimator, LinearClassifierMixin, _LearntSelectorMixin, SparseCoefMixin)", "/linear_model/logistic.py; Logistic Regression (aka logit, MaxEnt) classifier.  In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr' and uses the cross-entropy loss, if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs' and 'newton-cg' solvers.)  This class implements regularized logistic regression using the `liblinear` library, newton-cg and lbfgs solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The newton-cg and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.  Read more in the :ref:`User Guide <logistic_regression>`.  Parameters ---------- penalty : str, 'l1' or 'l2'     Used to specify the norm used in the penalization. The newton-cg and     lbfgs solvers support only l2 penalties.  dual : bool     Dual or primal formulation. Dual formulation is only implemented for     l2 penalty with liblinear solver. Prefer dual=False when     n_samples > n_features.  C : float, optional (default=1.0)     Inverse of regularization strength; must be a positive float.     Like in support vector machines, smaller values specify stronger     regularization.  fit_intercept : bool, default: True     Specifies if a constant (a.k.a. bias or intercept) should be     added the decision function.  intercept_scaling : float, default: 1     Useful only if solver is liblinear.     when self.fit_intercept is True, instance vector x becomes     [x, self.intercept_scaling],     i.e. a 'synthetic' feature with constant value equals to     intercept_scaling is appended to the instance vector.     The intercept becomes intercept_scaling * synthetic feature weight     Note! the synthetic feature weight is subject to l1/l2 regularization     as all other features.     To lessen the effect of regularization on synthetic feature weight     (and therefore on the intercept) intercept_scaling has to be increased.  class_weight : dict or 'balanced', optional     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  max_iter : int     Useful only for the newton-cg and lbfgs solvers. Maximum number of     iterations taken for the solvers to converge.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data.  solver : {'newton-cg', 'lbfgs', 'liblinear'}     Algorithm to use in the optimization problem.  tol : float, optional     Tolerance for stopping criteria.  multi_class : str, {'ovr', 'multinomial'}     Multiclass option can be either 'ovr' or 'multinomial'. If the option     chosen is 'ovr', then a binary problem is fit for each label. Else     the loss minimised is the multinomial loss fit across     the entire probability distribution. Works only for the 'lbfgs'     solver.  verbose : int     For the liblinear and lbfgs solvers set verbose to any positive     number for verbosity.  Attributes ---------- coef_ : array, shape (n_classes, n_features)     Coefficient of the features in the decision function.  intercept_ : array, shape (n_classes,)     Intercept (a.k.a. bias) added to the decision function.     If `fit_intercept` is set to False, the intercept is set to zero.  n_iter_ : int     Maximum of the actual number of iterations across all classes.     Valid only for the liblinear solver.  See also -------- SGDClassifier : incrementally trained logistic regression (when given     the parameter ``loss='log'``). sklearn.svm.LinearSVC : learns SVM models using the same algorithm.  Notes ----- The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.  Predict output may not match that of standalone liblinear in certain cases. See :ref:`differences from liblinear <liblinear_differences>` in the narrative documentation.  References ----------  LIBLINEAR -- A Library for Large Linear Classification     http://www.csie.ntu.edu.tw/~cjlin/liblinear/  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent     methods for logistic regression and maximum entropy models.     Machine Learning 85(1-2):41-75.     http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf   See also -------- sklearn.linear_model.SGDClassifier", [
d("__init__(self, penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state, solver, max_iter, multi_class, verbose)"),
d("fit(self, X, y)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("LogisticRegressionCV(LogisticRegression, BaseEstimator, LinearClassifierMixin, _LearntSelectorMixin)", "/linear_model/logistic.py; Logistic Regression CV (aka logit, MaxEnt) classifier.  This class implements logistic regression using liblinear, newton-cg or LBFGS optimizer. The newton-cg and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.  For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.  For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.  Read more in the :ref:`User Guide <logistic_regression>`.  Parameters ---------- Cs : list of floats | int     Each of the values in Cs describes the inverse of regularization     strength. If Cs is as an int, then a grid of Cs values are chosen     in a logarithmic scale between 1e-4 and 1e4.     Like in support vector machines, smaller values specify stronger     regularization.  fit_intercept : bool, default: True     Specifies if a constant (a.k.a. bias or intercept) should be     added the decision function.  class_weight : dict or 'balanced', optional     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  cv : integer or cross-validation generator     The default cross-validation generator used is Stratified K-Folds.     If an integer is provided, then it is the number of folds used.     See the module :mod:`sklearn.cross_validation` module for the     list of possible cross-validation objects.  penalty : str, 'l1' or 'l2'     Used to specify the norm used in the penalization. The newton-cg and     lbfgs solvers support only l2 penalties.  dual : bool     Dual or primal formulation. Dual formulation is only implemented for     l2 penalty with liblinear solver. Prefer dual=False when     n_samples > n_features.  scoring : callabale     Scoring function to use as cross-validation criteria. For a list of     scoring functions that can be used, look at :mod:`sklearn.metrics`.     The default scoring option used is accuracy_score.  solver : {'newton-cg', 'lbfgs', 'liblinear'}     Algorithm to use in the optimization problem.  tol : float, optional     Tolerance for stopping criteria.  max_iter : int, optional     Maximum number of iterations of the optimization algorithm.  class_weight : dict or 'balanced', optional     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  n_jobs : int, optional     Number of CPU cores used during the cross-validation loop. If given     a value of -1, all cores are used.  verbose : int     For the liblinear and lbfgs solvers set verbose to any positive     number for verbosity.  refit : bool     If set to True, the scores are averaged across all folds, and the     coefs and the C that corresponds to the best score is taken, and a     final refit is done using these parameters.     Otherwise the coefs, intercepts and C that correspond to the     best scores across folds are averaged.  multi_class : str, {'ovr', 'multinomial'}     Multiclass option can be either 'ovr' or 'multinomial'. If the option     chosen is 'ovr', then a binary problem is fit for each label. Else     the loss minimised is the multinomial loss fit across     the entire probability distribution. Works only for the 'lbfgs'     solver.  intercept_scaling : float, default 1.     Useful only if solver is liblinear.     This parameter is useful only when the solver 'liblinear' is used     and self.fit_intercept is set to True. In this case, x becomes     [x, self.intercept_scaling],     i.e. a 'synthetic' feature with constant value equals to     intercept_scaling is appended to the instance vector.     The intercept becomes intercept_scaling * synthetic feature weight     Note! the synthetic feature weight is subject to l1/l2 regularization     as all other features.     To lessen the effect of regularization on synthetic feature weight     (and therefore on the intercept) intercept_scaling has to be increased.  Attributes ---------- coef_ : array, shape (1, n_features) or (n_classes, n_features)     Coefficient of the features in the decision function.      `coef_` is of shape (1, n_features) when the given problem     is binary.     `coef_` is readonly property derived from `raw_coef_` that     follows the internal memory layout of liblinear.  intercept_ : array, shape (1,) or (n_classes,)     Intercept (a.k.a. bias) added to the decision function.     It is available only when parameter intercept is set to True     and is of shape(1,) when the problem is binary.  Cs_ : array     Array of C i.e. inverse of regularization parameter values used     for cross-validation.  coefs_paths_ : array, shape ``(n_folds, len(Cs_), n_features)`` or                    ``(n_folds, len(Cs_), n_features + 1)``     dict with classes as the keys, and the path of coefficients obtained     during cross-validating across each fold and then across each Cs     after doing an OvR for the corresponding class as values.     If the 'multi_class' option is set to 'multinomial', then     the coefs_paths are the coefficients corresponding to each class.     Each dict value has shape ``(n_folds, len(Cs_), n_features)`` or     ``(n_folds, len(Cs_), n_features + 1)`` depending on whether the     intercept is fit or not.  scores_ : dict     dict with classes as the keys, and the values as the     grid of scores obtained during cross-validating each fold, after doing     an OvR for the corresponding class. If the 'multi_class' option     given is 'multinomial' then the same scores are repeated across     all classes, since this is the multinomial class.     Each dict value has shape (n_folds, len(Cs))  C_ : array, shape (n_classes,) or (n_classes - 1,)     Array of C that maps to the best scores across every class. If refit is     set to False, then for each class, the best C is the average of the     C's that correspond to the best scores for each fold.  See also -------- LogisticRegression", [
d("__init__(self, Cs, fit_intercept, cv, dual, penalty, scoring, solver, tol, max_iter, class_weight, n_jobs, verbose, refit, intercept_scaling, multi_class)"),
d("fit(self, X, y)"),]),
c("BaseSGD()", "/linear_model/stochastic_gradient.py; Base class for SGD classification and regression.", [
d("__init__(self, loss, penalty, alpha, C, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, warm_start, average)"),
d("set_params(self)"),
d("fit(self, X, y)"),
d("_validate_params(self)"),
d("_get_loss_function(self, loss)"),
d("_get_learning_rate_type(self, learning_rate)"),
d("_get_penalty_type(self, penalty)"),
d("_validate_sample_weight(self, sample_weight, n_samples)"),
d("_allocate_parameter_mem(self, n_classes, n_features, coef_init, intercept_init)"),]),
c("BaseSGDClassifier()", "/linear_model/stochastic_gradient.py; ", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, class_weight, warm_start, average)"),
d("_partial_fit(self, X, y, alpha, C, loss, learning_rate, n_iter, classes, sample_weight, coef_init, intercept_init)"),
d("_fit(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)"),
d("_fit_binary(self, X, y, alpha, C, sample_weight, learning_rate, n_iter)"),
d("_fit_multiclass(self, X, y, alpha, C, learning_rate, sample_weight, n_iter)"),
d("partial_fit(self, X, y, classes, sample_weight)"),
d("fit(self, X, y, coef_init, intercept_init, sample_weight)"),]),
c("SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin)", "/linear_model/stochastic_gradient.py; Linear classifiers (SVM, logistic regression, a.o.) with SGD training.  This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.  This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  Read more in the :ref:`User Guide <sgd>`.  Parameters ---------- loss : str, 'hinge', 'log', 'modified_huber', 'squared_hinge',                'perceptron', or a regression loss: 'squared_loss', 'huber',                'epsilon_insensitive', or 'squared_epsilon_insensitive'     The loss function to be used. Defaults to 'hinge', which gives a     linear SVM.     The 'log' loss gives logistic regression, a probabilistic classifier.     'modified_huber' is another smooth loss that brings tolerance to     outliers as well as probability estimates.     'squared_hinge' is like hinge but is quadratically penalized.     'perceptron' is the linear loss used by the perceptron algorithm.     The other losses are designed for regression but can be useful in     classification as well; see SGDRegressor for a description.  penalty : str, 'none', 'l2', 'l1', or 'elasticnet'     The penalty (aka regularization term) to be used. Defaults to 'l2'     which is the standard regularizer for linear SVM models. 'l1' and     'elasticnet' might bring sparsity to the model (feature selection)     not achievable with 'l2'.  alpha : float     Constant that multiplies the regularization term. Defaults to 0.0001  l1_ratio : float     The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.     l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.     Defaults to 0.15.  fit_intercept : bool     Whether the intercept should be estimated or not. If False, the     data is assumed to be already centered. Defaults to True.  n_iter : int, optional     The number of passes over the training data (aka epochs). The number     of iterations is set to 1 if using partial_fit.     Defaults to 5.  shuffle : bool, optional     Whether or not the training data should be shuffled after each epoch.     Defaults to True.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data.  verbose : integer, optional     The verbosity level  epsilon : float     Epsilon in the epsilon-insensitive loss functions; only if `loss` is     'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.     For 'huber', determines the threshold at which it becomes less     important to get the prediction exactly right.     For epsilon-insensitive, any differences between the current prediction     and the correct label are ignored if they are less than this threshold.  n_jobs : integer, optional     The number of CPUs to use to do the OVA (One Versus All, for     multi-class problems) computation. -1 means 'all CPUs'. Defaults     to 1.  learning_rate : string, optional     The learning rate schedule:     constant: eta = eta0     optimal: eta = 1.0 / (t + t0) [default]     invscaling: eta = eta0 / pow(t, power_t)     where t0 is chosen by a heuristic proposed by Leon Bottou.  eta0 : double     The initial learning rate for the 'constant' or 'invscaling'     schedules. The default value is 0.0 as eta0 is not used by the     default schedule 'optimal'.  power_t : double     The exponent for inverse scaling learning rate [default 0.5].  class_weight : dict, {class_label: weight} or 'balanced' or None, optional     Preset for the class_weight fit parameter.      Weights associated with classes. If not given, all classes     are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  warm_start : bool, optional     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  average : bool or int, optional     When set to True, computes the averaged SGD weights and stores the     result in the ``coef_`` attribute. If set to an int greater than 1,     averaging will begin once the total number of samples seen reaches     average. So average=10 will begin averaging after seeing 10 samples.  Attributes ---------- coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)     Weights assigned to the features.  intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)     Constants in decision function.  Examples -------- >>> import numpy as np >>> from sklearn import linear_model >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) >>> Y = np.array([1, 1, 2, 2]) >>> clf = linear_model.SGDClassifier() >>> clf.fit(X, Y) ... #doctest: +NORMALIZE_WHITESPACE SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,         eta0=0.0, fit_intercept=True, l1_ratio=0.15,         learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,         penalty='l2', power_t=0.5, random_state=None, shuffle=True,         verbose=0, warm_start=False) >>> print(clf.predict([[-0.8, -1]])) [1]  See also -------- LinearSVC, LogisticRegression, Perceptron", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, class_weight, warm_start, average)"),
d("_check_proba(self)"),
d("predict_proba(self)"),
d("_predict_proba(self, X)"),
d("predict_log_proba(self)"),
d("_predict_log_proba(self, X)"),]),
c("BaseSGDRegressor(BaseSGD, RegressorMixin)", "/linear_model/stochastic_gradient.py; ", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, warm_start, average)"),
d("_partial_fit(self, X, y, alpha, C, loss, learning_rate, n_iter, sample_weight, coef_init, intercept_init)"),
d("partial_fit(self, X, y, sample_weight)"),
d("_fit(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)"),
d("fit(self, X, y, coef_init, intercept_init, sample_weight)"),
d("decision_function(self, X)"),
d("_decision_function(self, X)"),
d("predict(self, X)"),
d("_fit_regressor(self, X, y, alpha, C, loss, learning_rate, sample_weight, n_iter)"),]),
c("SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin)", "/linear_model/stochastic_gradient.py; Linear model fitted by minimizing a regularized empirical loss with SGD  SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  This implementation works with data represented as dense numpy arrays of floating point values for the features.  Read more in the :ref:`User Guide <sgd>`.  Parameters ---------- loss : str, 'squared_loss', 'huber', 'epsilon_insensitive',                 or 'squared_epsilon_insensitive'     The loss function to be used. Defaults to 'squared_loss' which refers     to the ordinary least squares fit. 'huber' modifies 'squared_loss' to     focus less on getting outliers correct by switching from squared to     linear loss past a distance of epsilon. 'epsilon_insensitive' ignores     errors less than epsilon and is linear past that; this is the loss     function used in SVR. 'squared_epsilon_insensitive' is the same but     becomes squared loss past a tolerance of epsilon.  penalty : str, 'none', 'l2', 'l1', or 'elasticnet'     The penalty (aka regularization term) to be used. Defaults to 'l2'     which is the standard regularizer for linear SVM models. 'l1' and     'elasticnet' might bring sparsity to the model (feature selection)     not achievable with 'l2'.  alpha : float     Constant that multiplies the regularization term. Defaults to 0.0001  l1_ratio : float     The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.     l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.     Defaults to 0.15.  fit_intercept : bool     Whether the intercept should be estimated or not. If False, the     data is assumed to be already centered. Defaults to True.  n_iter : int, optional     The number of passes over the training data (aka epochs). The number     of iterations is set to 1 if using partial_fit.     Defaults to 5.  shuffle : bool, optional     Whether or not the training data should be shuffled after each epoch.     Defaults to True.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data.  verbose : integer, optional     The verbosity level.  epsilon : float     Epsilon in the epsilon-insensitive loss functions; only if `loss` is     'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.     For 'huber', determines the threshold at which it becomes less     important to get the prediction exactly right.     For epsilon-insensitive, any differences between the current prediction     and the correct label are ignored if they are less than this threshold.  learning_rate : string, optional     The learning rate:     constant: eta = eta0     optimal: eta = 1.0/(alpha * t)     invscaling: eta = eta0 / pow(t, power_t) [default]  eta0 : double, optional     The initial learning rate [default 0.01].  power_t : double, optional     The exponent for inverse scaling learning rate [default 0.25].  warm_start : bool, optional     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  average : bool or int, optional     When set to True, computes the averaged SGD weights and stores the     result in the ``coef_`` attribute. If set to an int greater than 1,     averaging will begin once the total number of samples seen reaches     average. So ``average=10 will`` begin averaging after seeing 10 samples.  Attributes ---------- coef_ : array, shape (n_features,)     Weights assigned to the features.  intercept_ : array, shape (1,)     The intercept term.  average_coef_ : array, shape (n_features,)     Averaged weights assigned to the features.  average_intercept_ : array, shape (1,)     The averaged intercept term.  Examples -------- >>> import numpy as np >>> from sklearn import linear_model >>> n_samples, n_features = 10, 5 >>> np.random.seed(0) >>> y = np.random.randn(n_samples) >>> X = np.random.randn(n_samples, n_features) >>> clf = linear_model.SGDRegressor() >>> clf.fit(X, y) ... #doctest: +NORMALIZE_WHITESPACE SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,              fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',              loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,              random_state=None, shuffle=True, verbose=0, warm_start=False)  See also -------- Ridge, ElasticNet, Lasso, SVR", [
d("__init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, n_iter, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, warm_start, average)"),]),
c("_BaseRidge()", "/linear_model/ridge.py; ", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, solver)"),
d("fit(self, X, y, sample_weight)"),]),
c("Ridge(_BaseRidge, RegressorMixin)", "/linear_model/ridge.py; Linear least squares with l2 regularization.  This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alpha : {float, array-like}     shape = [n_targets]     Small positive values of alpha improve the conditioning of the problem     and reduce the variance of the estimates.  Alpha corresponds to     ``(2*C)^-1`` in other linear models such as LogisticRegression or     LinearSVC. If an array is passed, penalties are assumed to be specific     to the targets. Hence they must correspond in number.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  fit_intercept : boolean     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  max_iter : int, optional     Maximum number of iterations for conjugate gradient solver.     The default value is determined by scipy.sparse.linalg.  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'}     Solver to use in the computational routines:      - 'auto' chooses the solver automatically based on the type of data.      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge       coefficients. More stable for singular matrices than       'cholesky'.      - 'cholesky' uses the standard scipy.linalg.solve function to       obtain a closed-form solution.      - 'sparse_cg' uses the conjugate gradient solver as found in       scipy.sparse.linalg.cg. As an iterative algorithm, this solver is       more appropriate than 'cholesky' for large-scale data       (possibility to set `tol` and `max_iter`).      - 'lsqr' uses the dedicated regularized least-squares routine       scipy.sparse.linalg.lsqr. It is the fatest but may not be available       in old scipy versions. It also uses an iterative procedure.      All three solvers support both dense and sparse data.  tol : float     Precision of the solution.  Attributes ---------- coef_ : array, shape = [n_features] or [n_targets, n_features]     Weight vector(s).  See also -------- RidgeClassifier, RidgeCV, KernelRidge  Examples -------- >>> from sklearn.linear_model import Ridge >>> import numpy as np >>> n_samples, n_features = 10, 5 >>> np.random.seed(0) >>> y = np.random.randn(n_samples) >>> X = np.random.randn(n_samples, n_features) >>> clf = Ridge(alpha=1.0) >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,       normalize=False, solver='auto', tol=0.001)", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, solver)"),
d("fit(self, X, y, sample_weight)"),]),
c("RidgeClassifier(LinearClassifierMixin, _BaseRidge)", "/linear_model/ridge.py; Classifier using Ridge regression.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alpha : float     Small positive values of alpha improve the conditioning of the problem     and reduce the variance of the estimates.  Alpha corresponds to     ``(2*C)^-1`` in other linear models such as LogisticRegression or     LinearSVC.  class_weight : dict or 'balanced', optional     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  fit_intercept : boolean     Whether to calculate the intercept for this model. If set to false, no     intercept will be used in calculations (e.g. data is expected to be     already centered).  max_iter : int, optional     Maximum number of iterations for conjugate gradient solver.     The default value is determined by scipy.sparse.linalg.  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'}     Solver to use in the computational     routines. 'svd' will use a Singular value decomposition to obtain     the solution, 'cholesky' will use the standard     scipy.linalg.solve function, 'sparse_cg' will use the     conjugate gradient solver as found in     scipy.sparse.linalg.cg while 'auto' will chose the most     appropriate depending on the matrix X. 'lsqr' uses     a direct regularized least-squares routine provided by scipy.  tol : float     Precision of the solution.  Attributes ---------- coef_ : array, shape = [n_features] or [n_classes, n_features]     Weight vector(s).  See also -------- Ridge, RidgeClassifierCV  Notes ----- For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, class_weight, solver)"),
d("fit(self, X, y, sample_weight)"),
d("classes_(self)"),]),
c("_RidgeGCV(LinearModel)", "/linear_model/ridge.py; Ridge regression with built-in Generalized Cross-Validation  It allows efficient Leave-One-Out cross-validation.  This class is not intended to be used directly. Use RidgeCV instead.  Notes -----  We want to solve (K + alpha*Id)c = y, where K = X X^T is the kernel matrix.  Let G = (K + alpha*Id)^-1.  Dual solution: c = Gy Primal solution: w = X^T c  Compute eigendecomposition K = Q V Q^T. Then G = Q (V + alpha*Id)^-1 Q^T, where (V + alpha*Id) is diagonal. It is thus inexpensive to inverse for many alphas.  Let loov be the vector of prediction values for each example when the model was fitted with all examples but this example.  loov = (KGY - diag(KG)Y) / diag(I-KG)  Let looe be the vector of prediction errors for each example when the model was fitted with all examples but this example.  looe = y - loov = c / diag(G)  References ---------- http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-TR-2007-025.pdf http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf", [
d("__init__(self, alphas, fit_intercept, normalize, scoring, copy_X, gcv_mode, store_cv_values)"),
d("_pre_compute(self, X, y)"),
d("_decomp_diag(self, v_prime, Q)"),
d("_diag_dot(self, D, B)"),
d("_errors(self, alpha, y, v, Q, QT_y)"),
d("_values(self, alpha, y, v, Q, QT_y)"),
d("_pre_compute_svd(self, X, y)"),
d("_errors_svd(self, alpha, y, v, U, UT_y)"),
d("_values_svd(self, alpha, y, v, U, UT_y)"),
d("fit(self, X, y, sample_weight)"),]),
c("_BaseRidgeCV(LinearModel)", "/linear_model/ridge.py; ", [
d("__init__(self, alphas, fit_intercept, normalize, scoring, cv, gcv_mode, store_cv_values)"),
d("fit(self, X, y, sample_weight)"),]),
c("RidgeCV(_BaseRidgeCV, RegressorMixin)", "/linear_model/ridge.py; Ridge regression with built-in cross-validation.  By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alphas : numpy array of shape [n_alphas]     Array of alpha values to try.     Small positive values of alpha improve the conditioning of the     problem and reduce the variance of the estimates.     Alpha corresponds to ``(2*C)^-1`` in other linear models such as     LogisticRegression or LinearSVC.  fit_intercept : boolean     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  scoring : string, callable or None, optional, default: None     A string (see model evaluation documentation) or     a scorer callable object / function with signature     ``scorer(estimator, X, y)``.  cv : integer or cross-validation generator, optional     If None, Generalized Cross-Validation (efficient Leave-One-Out)     will be used.     If an integer is passed, it is the number of folds for KFold cross     validation.  Specific cross-validation objects can be passed, see     sklearn.cross_validation module for the list of possible objects  gcv_mode : {None, 'auto', 'svd', eigen'}, optional     Flag indicating which strategy to use when performing     Generalized Cross-Validation. Options are::          'auto' : use svd if n_samples > n_features or when X is a sparse                  matrix, otherwise use eigen         'svd' : force computation via singular value decomposition of X                 (does not work for sparse matrices)         'eigen' : force computation via eigendecomposition of X^T X      The 'auto' mode is the default and is intended to pick the cheaper     option of the two depending upon the shape and format of the training     data.  store_cv_values : boolean, default=False     Flag indicating if the cross-validation values corresponding to     each alpha should be stored in the `cv_values_` attribute (see     below). This flag is only compatible with `cv=None` (i.e. using     Generalized Cross-Validation).  Attributes ---------- cv_values_ : array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional     Cross-validation values for each alpha (if `store_cv_values=True` and         `cv=None`). After `fit()` has been called, this attribute will         contain the mean squared errors (by default) or the values of the         `{loss,score}_func` function (if provided in the constructor).  coef_ : array, shape = [n_features] or [n_targets, n_features]     Weight vector(s).  alpha_ : float     Estimated regularization parameter.  intercept_ : float | array, shape = (n_targets,)     Independent term in decision function. Set to 0.0 if     ``fit_intercept = False``.  See also -------- Ridge: Ridge regression RidgeClassifier: Ridge classifier RidgeClassifierCV: Ridge classifier with built-in cross validation", []),
c("RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV)", "/linear_model/ridge.py; Ridge classifier with built-in cross-validation.  By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n_features > n_samples case is handled efficiently.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alphas : numpy array of shape [n_alphas]     Array of alpha values to try.     Small positive values of alpha improve the conditioning of the     problem and reduce the variance of the estimates.     Alpha corresponds to ``(2*C)^-1`` in other linear models such as     LogisticRegression or LinearSVC.  fit_intercept : boolean     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  scoring : string, callable or None, optional, default: None     A string (see model evaluation documentation) or     a scorer callable object / function with signature     ``scorer(estimator, X, y)``.  cv : cross-validation generator, optional     If None, Generalized Cross-Validation (efficient Leave-One-Out)     will be used.  class_weight : dict or 'balanced', optional     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  Attributes ---------- cv_values_ : array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional     Cross-validation values for each alpha (if `store_cv_values=True` and `cv=None`). After `fit()` has been called, this attribute will contain     the mean squared errors (by default) or the values of the     `{loss,score}_func` function (if provided in the constructor).  coef_ : array, shape = [n_features] or [n_targets, n_features]     Weight vector(s).  alpha_ : float     Estimated regularization parameter  See also -------- Ridge: Ridge regression RidgeClassifier: Ridge classifier RidgeCV: Ridge regression with built-in cross validation  Notes ----- For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.", [
d("__init__(self, alphas, fit_intercept, normalize, scoring, cv, class_weight)"),
d("fit(self, X, y, sample_weight)"),
d("classes_(self)"),]),
c("LinearModel()", "/linear_model/base.py; Base class for Linear Models", [
d("fit(self, X, y)"),
d("decision_function(self, X)"),
d("_decision_function(self, X)"),
d("predict(self, X)"),
d("_set_intercept(self, X_mean, y_mean, X_std)"),]),
c("LinearClassifierMixin(ClassifierMixin)", "/linear_model/base.py; Mixin for linear classifiers.  Handles prediction for sparse and dense X.", [
d("decision_function(self, X)"),
d("predict(self, X)"),
d("_predict_proba_lr(self, X)"),]),
c("SparseCoefMixin(object)", "/linear_model/base.py; Mixin for converting coef_ to and from CSR format.  L1-regularizing estimators should inherit this.", [
d("densify(self)"),
d("sparsify(self)"),]),
c("LinearRegression(LinearModel, RegressorMixin)", "/linear_model/base.py; Ordinary least squares Linear Regression.  Parameters ---------- fit_intercept : boolean, optional     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If True, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If True, X will be copied; else, it may be overwritten.  n_jobs : int, optional, default 1     The number of jobs to use for the computation.     If -1 all CPUs are used. This will only provide speedup for     n_targets > 1 and sufficient large problems.  Attributes ---------- coef_ : array, shape (n_features, ) or (n_targets, n_features)     Estimated coefficients for the linear regression problem.     If multiple targets are passed during the fit (y 2D), this     is a 2D array of shape (n_targets, n_features), while if only     one target is passed, this is a 1D array of length n_features.  intercept_ : array     Independent term in the linear model.  Notes ----- From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.", [
d("__init__(self, fit_intercept, normalize, copy_X, n_jobs)"),
d("fit(self, X, y)"),]),
c("Perceptron(BaseSGDClassifier, _LearntSelectorMixin)", "/linear_model/perceptron.py; Perceptron  Read more in the :ref:`User Guide <perceptron>`.  Parameters ----------  penalty : None, 'l2' or 'l1' or 'elasticnet'     The penalty (aka regularization term) to be used. Defaults to None.  alpha : float     Constant that multiplies the regularization term if regularization is     used. Defaults to 0.0001  fit_intercept : bool     Whether the intercept should be estimated or not. If False, the     data is assumed to be already centered. Defaults to True.  n_iter : int, optional     The number of passes over the training data (aka epochs).     Defaults to 5.  shuffle : bool, optional, default True     Whether or not the training data should be shuffled after each epoch.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data.  verbose : integer, optional     The verbosity level  n_jobs : integer, optional     The number of CPUs to use to do the OVA (One Versus All, for     multi-class problems) computation. -1 means 'all CPUs'. Defaults     to 1.  eta0 : double     Constant by which the updates are multiplied. Defaults to 1.  class_weight : dict, {class_label: weight} or 'balanced' or None, optional     Preset for the class_weight fit parameter.      Weights associated with classes. If not given, all classes     are supposed to have weight one.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  warm_start : bool, optional     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  Attributes ---------- coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]     Weights assigned to the features.  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]     Constants in decision function.  Notes -----  `Perceptron` and `SGDClassifier` share the same underlying implementation. In fact, `Perceptron()` is equivalent to `SGDClassifier(loss='perceptron', eta0=1, learning_rate='constant', penalty=None)`.  See also --------  SGDClassifier  References ----------  http://en.wikipedia.org/wiki/Perceptron and references therein.", [
d("__init__(self, penalty, alpha, fit_intercept, n_iter, shuffle, verbose, eta0, n_jobs, random_state, class_weight, warm_start)"),]),
c("ElasticNet(LinearModel, RegressorMixin)", "/linear_model/coordinate_descent.py; Linear regression with combined L1 and L2 priors as regularizer.  Minimizes the objective function::          1 / (2 * n_samples) * ||y - Xw||^2_2 +         + alpha * l1_ratio * ||w||_1         + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to::          a * L1 + b * L2  where::          alpha = a + b and l1_ratio = a / (a + b)  The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.  Read more in the :ref:`User Guide <elastic_net>`.  Parameters ---------- alpha : float     Constant that multiplies the penalty terms. Defaults to 1.0     See the notes for the exact mathematical meaning of this     parameter.     ``alpha = 0`` is equivalent to an ordinary least square, solved     by the :class:`LinearRegression` object. For numerical     reasons, using ``alpha = 0`` with the Lasso object is not advised     and you should prefer the LinearRegression object.  l1_ratio : float     The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For     ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it     is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a     combination of L1 and L2.  fit_intercept : bool     Whether the intercept should be estimated or not. If ``False``, the     data is assumed to be already centered.  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument. For sparse input     this option is always ``True`` to preserve sparsity.     WARNING : The ``'auto'`` option is deprecated and will     be removed in 0.18.  max_iter : int, optional     The maximum number of iterations  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  warm_start : bool, optional     When set to ``True``, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  positive : bool, optional     When set to ``True``, forces the coefficients to be positive.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  Attributes ---------- coef_ : array, shape (n_features,) | (n_targets, n_features)     parameter vector (w in the cost function formula)  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)     ``sparse_coef_`` is a readonly property derived from ``coef_``  intercept_ : float | array, shape (n_targets,)     independent term in decision function.  n_iter_ : array-like, shape (n_targets,)     number of iterations run by the coordinate descent solver to reach     the specified tolerance.  Notes ----- To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.  See also -------- SGDRegressor: implements elastic net regression with incremental training. SGDClassifier: implements logistic regression with elastic net penalty     (``SGDClassifier(loss='log', penalty='elasticnet')``).", [
d("__init__(self, alpha, l1_ratio, fit_intercept, normalize, precompute, max_iter, copy_X, tol, warm_start, positive, random_state, selection)"),
d("fit(self, X, y, check_input)"),
d("sparse_coef_(self)"),
d("decision_function(self, X)"),
d("_decision_function(self, X)"),]),
c("Lasso(ElasticNet)", "/linear_model/coordinate_descent.py; Linear Model trained with L1 prior as regularizer (aka the Lasso)  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Technically the Lasso model is optimizing the same objective function as the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).  Read more in the :ref:`User Guide <lasso>`.  Parameters ---------- alpha : float, optional     Constant that multiplies the L1 term. Defaults to 1.0.     ``alpha = 0`` is equivalent to an ordinary least square, solved     by the :class:`LinearRegression` object. For numerical     reasons, using ``alpha = 0`` is with the Lasso object is not advised     and you should prefer the LinearRegression object.  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument. For sparse input     this option is always ``True`` to preserve sparsity.     WARNING : The ``'auto'`` option is deprecated and will     be removed in 0.18.  max_iter : int, optional     The maximum number of iterations  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  warm_start : bool, optional     When set to True, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  positive : bool, optional     When set to ``True``, forces the coefficients to be positive.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  Attributes ---------- coef_ : array, shape (n_features,) | (n_targets, n_features)     parameter vector (w in the cost function formula)  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)     ``sparse_coef_`` is a readonly property derived from ``coef_``  intercept_ : float | array, shape (n_targets,)     independent term in decision function.  n_iter_ : int | array-like, shape (n_targets,)     number of iterations run by the coordinate descent solver to reach     the specified tolerance.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.Lasso(alpha=0.1) >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2]) Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,    normalize=False, positive=False, precompute=False, random_state=None,    selection='cyclic', tol=0.0001, warm_start=False) >>> print(clf.coef_) [ 0.85  0.  ] >>> print(clf.intercept_) 0.15  See also -------- lars_path lasso_path LassoLars LassoCV LassoLarsCV sklearn.decomposition.sparse_encode  Notes ----- The algorithm used to fit the model is coordinate descent.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.", [
d("__init__(self, alpha, fit_intercept, normalize, precompute, copy_X, max_iter, tol, warm_start, positive, random_state, selection)"),]),
c("LinearModelCV()", "/linear_model/coordinate_descent.py; Base class for iterative model fitting along a regularization path", [
d("__init__(self, eps, n_alphas, alphas, fit_intercept, normalize, precompute, max_iter, tol, copy_X, cv, verbose, n_jobs, positive, random_state, selection)"),
d("fit(self, X, y)"),]),
c("LassoCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py; Lasso linear model with iterative fitting along a regularization path  The best model is selected by cross-validation.  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <lasso>`.  Parameters ---------- eps : float, optional     Length of the path. ``eps=1e-3`` means that     ``alpha_min / alpha_max = 1e-3``.  n_alphas : int, optional     Number of alphas along the regularization path  alphas : numpy array, optional     List of alphas where to compute the models.     If ``None`` alphas are set automatically  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  max_iter : int, optional     The maximum number of iterations  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  cv : integer or cross-validation generator, optional     If an integer is passed, it is the number of fold (default 3).     Specific cross-validation objects can be passed, see the     :mod:`sklearn.cross_validation` module for the list of possible     objects.  verbose : bool or integer     Amount of verbosity.  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs.  positive : bool, optional     If positive, restrict regression coefficients to be positive  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  fit_intercept : boolean, default True     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  Attributes ---------- alpha_ : float     The amount of penalization chosen by cross validation  coef_ : array, shape (n_features,) | (n_targets, n_features)     parameter vector (w in the cost function formula)  intercept_ : float | array, shape (n_targets,)     independent term in decision function.  mse_path_ : array, shape (n_alphas, n_folds)     mean square error for the test set on each fold, varying alpha  alphas_ : numpy array, shape (n_alphas,)     The grid of alphas used for fitting  dual_gap_ : ndarray, shape ()     The dual gap at the end of the optimization for the optimal alpha     (``alpha_``).  n_iter_ : int     number of iterations run by the coordinate descent solver to reach     the specified tolerance for the optimal alpha.  Notes ----- See examples/linear_model/lasso_path_with_crossvalidation.py for an example.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.  See also -------- lars_path lasso_path LassoLars Lasso LassoLarsCV", [
d("__init__(self, eps, n_alphas, alphas, fit_intercept, normalize, precompute, max_iter, tol, copy_X, cv, verbose, n_jobs, positive, random_state, selection)"),]),
c("ElasticNetCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py; Elastic Net model with iterative fitting along a regularization path  The best model is selected by cross-validation.  Read more in the :ref:`User Guide <elastic_net>`.  Parameters ---------- l1_ratio : float, optional     float between 0 and 1 passed to ElasticNet (scaling between     l1 and l2 penalties). For ``l1_ratio = 0``     the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.     For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2     This parameter can be a list, in which case the different     values are tested by cross-validation and the one giving the best     prediction score is used. Note that a good choice of list of     values for l1_ratio is often to put more values close to 1     (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,     .9, .95, .99, 1]``  eps : float, optional     Length of the path. ``eps=1e-3`` means that     ``alpha_min / alpha_max = 1e-3``.  n_alphas : int, optional     Number of alphas along the regularization path, used for each l1_ratio.  alphas : numpy array, optional     List of alphas where to compute the models.     If None alphas are set automatically  precompute : True | False | 'auto' | array-like     Whether to use a precomputed Gram matrix to speed up     calculations. If set to ``'auto'`` let us decide. The Gram     matrix can also be passed as argument.  max_iter : int, optional     The maximum number of iterations  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  cv : integer or cross-validation generator, optional     If an integer is passed, it is the number of fold (default 3).     Specific cross-validation objects can be passed, see the     :mod:`sklearn.cross_validation` module for the list of possible     objects.  verbose : bool or integer     Amount of verbosity.  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs.  positive : bool, optional     When set to ``True``, forces the coefficients to be positive.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  Attributes ---------- alpha_ : float     The amount of penalization chosen by cross validation  l1_ratio_ : float     The compromise between l1 and l2 penalization chosen by     cross validation  coef_ : array, shape (n_features,) | (n_targets, n_features)     Parameter vector (w in the cost function formula),  intercept_ : float | array, shape (n_targets, n_features)     Independent term in the decision function.  mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds)     Mean square error for the test set on each fold, varying l1_ratio and     alpha.  alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)     The grid of alphas used for fitting, for each l1_ratio.  n_iter_ : int     number of iterations run by the coordinate descent solver to reach     the specified tolerance for the optimal alpha.  Notes ----- See examples/linear_model/lasso_path_with_crossvalidation.py for an example.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.  The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is::      1 / (2 * n_samples) * ||y - Xw||^2_2 +     + alpha * l1_ratio * ||w||_1     + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to::      a * L1 + b * L2  for::      alpha = a + b and l1_ratio = a / (a + b).  See also -------- enet_path ElasticNet", [
d("__init__(self, l1_ratio, eps, n_alphas, alphas, fit_intercept, normalize, precompute, max_iter, tol, cv, copy_X, verbose, n_jobs, positive, random_state, selection)"),]),
c("MultiTaskElasticNet(Lasso)", "/linear_model/coordinate_descent.py; Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  Parameters ---------- alpha : float, optional     Constant that multiplies the L1/L2 term. Defaults to 1.0  l1_ratio : float     The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.     For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it     is an L1 penalty.     For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, optional     The maximum number of iterations  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  warm_start : bool, optional     When set to ``True``, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  Attributes ---------- intercept_ : array, shape (n_tasks,)     Independent term in decision function.  coef_ : array, shape (n_tasks, n_features)     Parameter vector (W in the cost function formula). If a 1D y is         passed in at fit (non multi-task usage), ``coef_`` is then a 1D array  n_iter_ : int     number of iterations run by the coordinate descent solver to reach     the specified tolerance.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1) >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]]) ... #doctest: +NORMALIZE_WHITESPACE MultiTaskElasticNet(alpha=0.1, copy_X=True, fit_intercept=True,         l1_ratio=0.5, max_iter=1000, normalize=False, random_state=None,         selection='cyclic', tol=0.0001, warm_start=False) >>> print(clf.coef_) [[ 0.45663524  0.45612256]  [ 0.45663524  0.45612256]] >>> print(clf.intercept_) [ 0.0872422  0.0872422]  See also -------- ElasticNet, MultiTaskLasso  Notes ----- The algorithm used to fit the model is coordinate descent.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.", [
d("__init__(self, alpha, l1_ratio, fit_intercept, normalize, copy_X, max_iter, tol, warm_start, random_state, selection)"),
d("fit(self, X, y)"),]),
c("MultiTaskLasso(MultiTaskElasticNet)", "/linear_model/coordinate_descent.py; Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21  Where::      ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of earch row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  Parameters ---------- alpha : float, optional     Constant that multiplies the L1/L2 term. Defaults to 1.0  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, optional     The maximum number of iterations  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  warm_start : bool, optional     When set to ``True``, reuse the solution of the previous call to fit as     initialization, otherwise, just erase the previous solution.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  Attributes ---------- coef_ : array, shape (n_tasks, n_features)     parameter vector (W in the cost function formula)  intercept_ : array, shape (n_tasks,)     independent term in decision function.  n_iter_ : int     number of iterations run by the coordinate descent solver to reach     the specified tolerance.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.MultiTaskLasso(alpha=0.1) >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]]) MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,         normalize=False, random_state=None, selection='cyclic', tol=0.0001,         warm_start=False) >>> print(clf.coef_) [[ 0.89393398  0.        ]  [ 0.89393398  0.        ]] >>> print(clf.intercept_) [ 0.10606602  0.10606602]  See also -------- Lasso, MultiTaskElasticNet  Notes ----- The algorithm used to fit the model is coordinate descent.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.", [
d("__init__(self, alpha, fit_intercept, normalize, copy_X, max_iter, tol, warm_start, random_state, selection)"),]),
c("MultiTaskElasticNetCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py; Multi-task L1/L2 ElasticNet with built-in cross-validation.  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  Parameters ---------- eps : float, optional     Length of the path. ``eps=1e-3`` means that     ``alpha_min / alpha_max = 1e-3``.  alphas : array-like, optional     List of alphas where to compute the models.     If not provided, set automatically.  n_alphas : int, optional     Number of alphas along the regularization path  l1_ratio : float or array of floats     The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.     For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it     is an L1 penalty.     For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, optional     The maximum number of iterations  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  cv : integer or cross-validation generator, optional     If an integer is passed, it is the number of fold (default 3).     Specific cross-validation objects can be passed, see the     :mod:`sklearn.cross_validation` module for the list of possible     objects.  verbose : bool or integer     Amount of verbosity.  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs. Note that this is used only if multiple values for     l1_ratio are given.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  Attributes ---------- intercept_ : array, shape (n_tasks,)     Independent term in decision function.  coef_ : array, shape (n_tasks, n_features)     Parameter vector (W in the cost function formula).  alpha_ : float     The amount of penalization chosen by cross validation  mse_path_ : array, shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)     mean square error for the test set on each fold, varying alpha  alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)     The grid of alphas used for fitting, for each l1_ratio  l1_ratio_ : float     best l1_ratio obtained by cross-validation.  n_iter_ : int     number of iterations run by the coordinate descent solver to reach     the specified tolerance for the optimal alpha.  Examples -------- >>> from sklearn import linear_model >>> clf = linear_model.MultiTaskElasticNetCV() >>> clf.fit([[0,0], [1, 1], [2, 2]], ...         [[0, 0], [1, 1], [2, 2]]) ... #doctest: +NORMALIZE_WHITESPACE MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,        fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,        n_jobs=1, normalize=False, random_state=None, selection='cyclic',        tol=0.0001, verbose=0) >>> print(clf.coef_) [[ 0.52875032  0.46958558]  [ 0.52875032  0.46958558]] >>> print(clf.intercept_) [ 0.00166409  0.00166409]  See also -------- MultiTaskElasticNet ElasticNetCV MultiTaskLassoCV  Notes ----- The algorithm used to fit the model is coordinate descent.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.", [
d("__init__(self, l1_ratio, eps, n_alphas, alphas, fit_intercept, normalize, max_iter, tol, cv, copy_X, verbose, n_jobs, random_state, selection)"),]),
c("MultiTaskLassoCV(LinearModelCV, RegressorMixin)", "/linear_model/coordinate_descent.py; Multi-task L1/L2 Lasso with built-in cross-validation.  The optimization objective for MultiTaskLasso is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21  Where::      ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  Parameters ---------- eps : float, optional     Length of the path. ``eps=1e-3`` means that     ``alpha_min / alpha_max = 1e-3``.  alphas : array-like, optional     List of alphas where to compute the models.     If not provided, set automaticlly.  n_alphas : int, optional     Number of alphas along the regularization path  fit_intercept : boolean     whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (e.g. data is expected to be already centered).  normalize : boolean, optional, default False     If ``True``, the regressors X will be normalized before regression.  copy_X : boolean, optional, default True     If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, optional     The maximum number of iterations.  tol : float, optional     The tolerance for the optimization: if the updates are     smaller than ``tol``, the optimization code checks the     dual gap for optimality and continues until it is smaller     than ``tol``.  cv : integer or cross-validation generator, optional     If an integer is passed, it is the number of fold (default 3).     Specific cross-validation objects can be passed, see the     :mod:`sklearn.cross_validation` module for the list of possible     objects.  verbose : bool or integer     Amount of verbosity.  n_jobs : integer, optional     Number of CPUs to use during the cross validation. If ``-1``, use     all the CPUs. Note that this is used only if multiple values for     l1_ratio are given.  selection : str, default 'cyclic'     If set to 'random', a random coefficient is updated every iteration     rather than looping over features sequentially by default. This     (setting to 'random') often leads to significantly faster convergence     especially when tol is higher than 1e-4.  random_state : int, RandomState instance, or None (default)     The seed of the pseudo random number generator that selects     a random feature to update. Useful only when selection is set to     'random'.  Attributes ---------- intercept_ : array, shape (n_tasks,)     Independent term in decision function.  coef_ : array, shape (n_tasks, n_features)     Parameter vector (W in the cost function formula).  alpha_ : float     The amount of penalization chosen by cross validation  mse_path_ : array, shape (n_alphas, n_folds)     mean square error for the test set on each fold, varying alpha  alphas_ : numpy array, shape (n_alphas,)     The grid of alphas used for fitting.  n_iter_ : int     number of iterations run by the coordinate descent solver to reach     the specified tolerance for the optimal alpha.  See also -------- MultiTaskElasticNet ElasticNetCV MultiTaskElasticNetCV  Notes ----- The algorithm used to fit the model is coordinate descent.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.", [
d("__init__(self, eps, n_alphas, alphas, fit_intercept, normalize, max_iter, tol, copy_X, cv, verbose, n_jobs, random_state, selection)"),]),
c("MyPassiveAggressive(ClassifierMixin)", "/linear_model/tests/test_passive_aggressive.py; ", [
d("__init__(self, C, epsilon, loss, fit_intercept, n_iter, random_state)"),
d("fit(self, X, y)"),
d("project(self, X)"),]),
c("SparseSGDClassifier(SGDClassifier)", "/linear_model/tests/test_sgd.py; ", [
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("decision_function(self, X)"),
d("predict_proba(self, X)"),]),
c("SparseSGDRegressor(SGDRegressor)", "/linear_model/tests/test_sgd.py; ", [
d("fit(self, X, y)"),
d("partial_fit(self, X, y)"),
d("decision_function(self, X)"),]),
c("CommonTest(object)", "/linear_model/tests/test_sgd.py; ", [
d("factory(self)"),
d("asgd(self, X, y, eta, alpha, weight_init, intercept_init)"),
d("_test_warm_start(self, X, Y, lr)"),
d("test_warm_start_constant(self)"),
d("test_warm_start_invscaling(self)"),
d("test_warm_start_optimal(self)"),
d("test_input_format(self)"),
d("test_clone(self)"),
d("test_plain_has_no_average_attr(self)"),
d("test_late_onset_averaging_not_reached(self)"),
d("test_late_onset_averaging_reached(self)"),]),
c("DenseSGDClassifierTestCase(CommonTest)", "/linear_model/tests/test_sgd.py; Test suite for the dense representation variant of SGD", [
d("test_sgd(self)"),
d("test_sgd_bad_l1_ratio(self)"),
d("test_sgd_bad_learning_rate_schedule(self)"),
d("test_sgd_bad_eta0(self)"),
d("test_sgd_bad_alpha(self)"),
d("test_sgd_bad_penalty(self)"),
d("test_sgd_bad_loss(self)"),
d("test_sgd_n_iter_param(self)"),
d("test_sgd_shuffle_param(self)"),
d("test_argument_coef(self)"),
d("test_provide_coef(self)"),
d("test_set_intercept(self)"),
d("test_set_intercept_binary(self)"),
d("test_average_binary_computed_correctly(self)"),
d("test_set_intercept_to_intercept(self)"),
d("test_sgd_at_least_two_labels(self)"),
d("test_partial_fit_weight_class_balanced(self)"),
d("test_sgd_multiclass(self)"),
d("test_sgd_multiclass_average(self)"),
d("test_sgd_multiclass_with_init_coef(self)"),
d("test_sgd_multiclass_njobs(self)"),
d("test_set_coef_multiclass(self)"),
d("test_sgd_proba(self)"),
d("test_sgd_l1(self)"),
d("test_class_weights(self)"),
d("test_equal_class_weight(self)"),
d("test_wrong_class_weight_label(self)"),
d("test_wrong_class_weight_format(self)"),
d("test_weights_multiplied(self)"),
d("test_balanced_weight(self)"),
d("test_sample_weights(self)"),
d("test_wrong_sample_weights(self)"),
d("test_partial_fit_exception(self)"),
d("test_partial_fit_binary(self)"),
d("test_partial_fit_multiclass(self)"),
d("test_fit_then_partial_fit(self)"),
d("_test_partial_fit_equal_fit(self, lr)"),
d("test_partial_fit_equal_fit_constant(self)"),
d("test_partial_fit_equal_fit_optimal(self)"),
d("test_partial_fit_equal_fit_invscaling(self)"),
d("test_regression_losses(self)"),
d("test_warm_start_multiclass(self)"),
d("test_multiple_fit(self)"),]),
c("SparseSGDClassifierTestCase(DenseSGDClassifierTestCase)", "/linear_model/tests/test_sgd.py; Run exactly the same tests using the sparse representation variant", []),
c("DenseSGDRegressorTestCase(CommonTest)", "/linear_model/tests/test_sgd.py; Test suite for the dense representation variant of SGD", [
d("test_sgd(self)"),
d("test_sgd_bad_penalty(self)"),
d("test_sgd_bad_loss(self)"),
d("test_sgd_averaged_computed_correctly(self)"),
d("test_sgd_averaged_partial_fit(self)"),
d("test_average_sparse(self)"),
d("test_sgd_least_squares_fit(self)"),
d("test_sgd_epsilon_insensitive(self)"),
d("test_sgd_huber_fit(self)"),
d("test_elasticnet_convergence(self)"),
d("test_partial_fit(self)"),
d("_test_partial_fit_equal_fit(self, lr)"),
d("test_partial_fit_equal_fit_constant(self)"),
d("test_partial_fit_equal_fit_optimal(self)"),
d("test_partial_fit_equal_fit_invscaling(self)"),
d("test_loss_function_epsilon(self)"),]),
c("SparseSGDRegressorTestCase(DenseSGDRegressorTestCase)", "/linear_model/tests/test_sgd.py; ", []),
c("MyPerceptron(object)", "/linear_model/tests/test_perceptron.py; ", [
d("__init__(self, n_iter)"),
d("fit(self, X, y)"),
d("project(self, X)"),
d("predict(self, X)"),]),
c("DictVectorizer(BaseEstimator, TransformerMixin)", "/feature_extraction/dict_vectorizer.py; Transforms lists of feature-value mappings to vectors.  This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.  When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature 'f' that can take on the values 'ham' and 'spam' will become two features in the output, one signifying 'f=ham', the other 'f=spam'.  Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.  Read more in the :ref:`User Guide <dict_feature_extraction>`.  Parameters ---------- dtype : callable, optional     The type of feature values. Passed to Numpy array/scipy.sparse matrix     constructors as the dtype argument. separator: string, optional     Separator string used when constructing new features for one-hot     coding. sparse: boolean, optional.     Whether transform should produce scipy.sparse matrices.     True by default. sort: boolean, optional.     Whether ``feature_names_`` and ``vocabulary_`` should be sorted when fitting.     True by default.  Attributes ---------- vocabulary_ : dict     A dictionary mapping feature names to feature indices.  feature_names_ : list     A list of length n_features containing the feature names (e.g., 'f=ham'     and 'f=spam').  Examples -------- >>> from sklearn.feature_extraction import DictVectorizer >>> v = DictVectorizer(sparse=False) >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}] >>> X = v.fit_transform(D) >>> X array([[ 2.,  0.,  1.],        [ 0.,  1.,  3.]]) >>> v.inverse_transform(X) ==         [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}] True >>> v.transform({'foo': 4, 'unseen_feature': 3}) array([[ 0.,  0.,  4.]])  See also -------- FeatureHasher : performs vectorization using only a hash function. sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features   encoded as columns of integers.", [
d("__init__(self, dtype, separator, sparse, sort)"),
d("fit(self, X, y)"),
d("_transform(self, X, fitting)"),
d("fit_transform(self, X, y)"),
d("inverse_transform(self, X, dict_type)"),
d("transform(self, X, y)"),
d("get_feature_names(self)"),
d("restrict(self, support, indices)"),]),
c("VectorizerMixin(object)", "/feature_extraction/text.py; Provides common code for text vectorizers (tokenization logic).", [
d("decode(self, doc)"),
d("_word_ngrams(self, tokens, stop_words)"),
d("_char_ngrams(self, text_document)"),
d("_char_wb_ngrams(self, text_document)"),
d("build_preprocessor(self)"),
d("build_tokenizer(self)"),
d("get_stop_words(self)"),
d("build_analyzer(self)"),
d("_validate_vocabulary(self)"),
d("_check_vocabulary(self)"),
d("fixed_vocabulary(self)"),]),
c("HashingVectorizer(BaseEstimator, VectorizerMixin)", "/feature_extraction/text.py; Convert a collection of text documents to a matrix of token occurrences  It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm='l1' or projected on the euclidean unit sphere if norm='l2'.  This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.  This strategy has several advantages:  - it is very low memory scalable to large datasets as there is no need to   store a vocabulary dictionary in memory  - it is fast to pickle and un-pickle as it holds no state besides the   constructor parameters  - it can be used in a streaming (partial fit) or parallel pipeline as there   is no state computed during fit.  There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):  - there is no way to compute the inverse transform (from feature indices to   string feature names) which can be a problem when trying to introspect   which features are most important to a model.  - there can be collisions: distinct tokens can be mapped to the same   feature index. However in practice this is rarely an issue if n_features   is large enough (e.g. 2 ** 18 for text classification problems).  - no IDF weighting as this would render the transformer stateful.  The hash function employed is the signed 32-bit version of Murmurhash3.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ----------  input : string {'filename', 'file', 'content'}     If 'filename', the sequence passed as an argument to fit is     expected to be a list of filenames that need reading to fetch     the raw content to analyze.      If 'file', the sequence items must have a 'read' method (file-like     object) that is called to fetch the bytes in memory.      Otherwise the input is expected to be the sequence strings or     bytes items are expected to be analyzed directly.  encoding : string, default='utf-8'     If bytes or files are given to analyze, this encoding is used to     decode.  decode_error : {'strict', 'ignore', 'replace'}     Instruction on what to do if a byte sequence is given to analyze that     contains characters not of the given `encoding`. By default, it is     'strict', meaning that a UnicodeDecodeError will be raised. Other     values are 'ignore' and 'replace'.  strip_accents : {'ascii', 'unicode', None}     Remove accents during the preprocessing step.     'ascii' is a fast method that only works on characters that have     an direct ASCII mapping.     'unicode' is a slightly slower method that works on any characters.     None (default) does nothing.  analyzer : string, {'word', 'char', 'char_wb'} or callable     Whether the feature should be made of word or character n-grams.     Option 'char_wb' creates character n-grams only from text inside     word boundaries.      If a callable is passed it is used to extract the sequence of features     out of the raw, unprocessed input.  preprocessor : callable or None (default)     Override the preprocessing (string transformation) stage while     preserving the tokenizing and n-grams generation steps.  tokenizer : callable or None (default)     Override the string tokenization step while preserving the     preprocessing and n-grams generation steps.     Only applies if ``analyzer == 'word'``.  ngram_range : tuple (min_n, max_n), default=(1, 1)     The lower and upper boundary of the range of n-values for different     n-grams to be extracted. All values of n such that min_n <= n <= max_n     will be used.  stop_words : string {'english'}, list, or None (default)     If 'english', a built-in stop word list for English is used.      If a list, that list is assumed to contain stop words, all of which     will be removed from the resulting tokens.     Only applies if ``analyzer == 'word'``.  lowercase : boolean, default=True     Convert all characters to lowercase before tokenizing.  token_pattern : string     Regular expression denoting what constitutes a 'token', only used     if ``analyzer == 'word'``. The default regexp selects tokens of 2     or more alphanumeric characters (punctuation is completely ignored     and always treated as a token separator).  n_features : integer, default=(2 ** 20)     The number of features (columns) in the output matrices. Small numbers     of features are likely to cause hash collisions, but large numbers     will cause larger coefficient dimensions in linear learners.  norm : 'l1', 'l2' or None, optional     Norm used to normalize term vectors. None for no normalization.  binary: boolean, default=False.     If True, all non zero counts are set to 1. This is useful for discrete     probabilistic models that model binary events rather than integer     counts.  dtype: type, optional     Type of the matrix returned by fit_transform() or transform().  non_negative : boolean, default=False     Whether output matrices should contain non-negative values only;     effectively calls abs on the matrix prior to returning it.     When True, output values can be interpreted as frequencies.     When False, output values will have expected value zero.  See also -------- CountVectorizer, TfidfVectorizer", [
d("__init__(self, input, encoding, decode_error, strip_accents, lowercase, preprocessor, tokenizer, stop_words, token_pattern, ngram_range, analyzer, n_features, binary, norm, non_negative, dtype)"),
d("partial_fit(self, X, y)"),
d("fit(self, X, y)"),
d("transform(self, X, y)"),
d("_get_hasher(self)"),]),
c("CountVectorizer(BaseEstimator, VectorizerMixin)", "/feature_extraction/text.py; Convert a collection of text documents to a matrix of token counts  This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix.  If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- input : string {'filename', 'file', 'content'}     If 'filename', the sequence passed as an argument to fit is     expected to be a list of filenames that need reading to fetch     the raw content to analyze.      If 'file', the sequence items must have a 'read' method (file-like     object) that is called to fetch the bytes in memory.      Otherwise the input is expected to be the sequence strings or     bytes items are expected to be analyzed directly.  encoding : string, 'utf-8' by default.     If bytes or files are given to analyze, this encoding is used to     decode.  decode_error : {'strict', 'ignore', 'replace'}     Instruction on what to do if a byte sequence is given to analyze that     contains characters not of the given `encoding`. By default, it is     'strict', meaning that a UnicodeDecodeError will be raised. Other     values are 'ignore' and 'replace'.  strip_accents : {'ascii', 'unicode', None}     Remove accents during the preprocessing step.     'ascii' is a fast method that only works on characters that have     an direct ASCII mapping.     'unicode' is a slightly slower method that works on any characters.     None (default) does nothing.  analyzer : string, {'word', 'char', 'char_wb'} or callable     Whether the feature should be made of word or character n-grams.     Option 'char_wb' creates character n-grams only from text inside     word boundaries.      If a callable is passed it is used to extract the sequence of features     out of the raw, unprocessed input.     Only applies if ``analyzer == 'word'``.  preprocessor : callable or None (default)     Override the preprocessing (string transformation) stage while     preserving the tokenizing and n-grams generation steps.  tokenizer : callable or None (default)     Override the string tokenization step while preserving the     preprocessing and n-grams generation steps.     Only applies if ``analyzer == 'word'``.  ngram_range : tuple (min_n, max_n)     The lower and upper boundary of the range of n-values for different     n-grams to be extracted. All values of n such that min_n <= n <= max_n     will be used.  stop_words : string {'english'}, list, or None (default)     If 'english', a built-in stop word list for English is used.      If a list, that list is assumed to contain stop words, all of which     will be removed from the resulting tokens.     Only applies if ``analyzer == 'word'``.      If None, no stop words will be used. max_df can be set to a value     in the range [0.7, 1.0) to automatically detect and filter stop     words based on intra corpus document frequency of terms.  lowercase : boolean, True by default     Convert all characters to lowercase before tokenizing.  token_pattern : string     Regular expression denoting what constitutes a 'token', only used     if ``analyzer == 'word'``. The default regexp select tokens of 2     or more alphanumeric characters (punctuation is completely ignored     and always treated as a token separator).  max_df : float in range [0.0, 1.0] or int, default=1.0     When building the vocabulary ignore terms that have a document     frequency strictly higher than the given threshold (corpus-specific     stop words).     If float, the parameter represents a proportion of documents, integer     absolute counts.     This parameter is ignored if vocabulary is not None.  min_df : float in range [0.0, 1.0] or int, default=1     When building the vocabulary ignore terms that have a document     frequency strictly lower than the given threshold. This value is also     called cut-off in the literature.     If float, the parameter represents a proportion of documents, integer     absolute counts.     This parameter is ignored if vocabulary is not None.  max_features : int or None, default=None     If not None, build a vocabulary that only consider the top     max_features ordered by term frequency across the corpus.      This parameter is ignored if vocabulary is not None.  vocabulary : Mapping or iterable, optional     Either a Mapping (e.g., a dict) where keys are terms and values are     indices in the feature matrix, or an iterable over terms. If not     given, a vocabulary is determined from the input documents. Indices     in the mapping should not be repeated and should not have any gap     between 0 and the largest index.  binary : boolean, default=False     If True, all non zero counts are set to 1. This is useful for discrete     probabilistic models that model binary events rather than integer     counts.  dtype : type, optional     Type of the matrix returned by fit_transform() or transform().  Attributes ---------- vocabulary_ : dict     A mapping of terms to feature indices.  stop_words_ : set     Terms that were ignored because they either:        - occurred in too many documents (`max_df`)       - occurred in too few documents (`min_df`)       - were cut off by feature selection (`max_features`).      This is only available if no vocabulary was given.  See also -------- HashingVectorizer, TfidfVectorizer  Notes ----- The ``stop_words_`` attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.", [
d("__init__(self, input, encoding, decode_error, strip_accents, lowercase, preprocessor, tokenizer, stop_words, token_pattern, ngram_range, analyzer, max_df, min_df, max_features, vocabulary, binary, dtype)"),
d("_sort_features(self, X, vocabulary)"),
d("_limit_features(self, X, vocabulary, high, low, limit)"),
d("_count_vocab(self, raw_documents, fixed_vocab)"),
d("fit(self, raw_documents, y)"),
d("fit_transform(self, raw_documents, y)"),
d("transform(self, raw_documents)"),
d("inverse_transform(self, X)"),
d("get_feature_names(self)"),]),
c("TfidfTransformer(BaseEstimator, TransformerMixin)", "/feature_extraction/text.py; Transform a count matrix to a normalized tf or tf-idf representation  Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.  The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.  The actual formula used for tf-idf is tf * (idf + 1) = tf + tf * idf, instead of tf * idf. The effect of this is that terms with zero idf, i.e. that occur in all documents of a training set, will not be entirely ignored. The formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR, as follows:  Tf is 'n' (natural) by default, 'l' (logarithmic) when sublinear_tf=True. Idf is 't' when use_idf is given, 'n' (none) otherwise. Normalization is 'c' (cosine) when norm='l2', 'n' (none) when norm=None.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- norm : 'l1', 'l2' or None, optional     Norm used to normalize term vectors. None for no normalization.  use_idf : boolean, default=True     Enable inverse-document-frequency reweighting.  smooth_idf : boolean, default=True     Smooth idf weights by adding one to document frequencies, as if an     extra document was seen containing every term in the collection     exactly once. Prevents zero divisions.  sublinear_tf : boolean, default=False     Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).  References ----------  .. [Yates2011] `R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern                Information Retrieval. Addison Wesley, pp. 68-74.`  .. [MRS2008] `C.D. Manning, P. Raghavan and H. Schuetze  (2008).                Introduction to Information Retrieval. Cambridge University                Press, pp. 118-120.`", [
d("__init__(self, norm, use_idf, smooth_idf, sublinear_tf)"),
d("fit(self, X, y)"),
d("transform(self, X, copy)"),
d("idf_(self)"),]),
c("TfidfVectorizer(CountVectorizer)", "/feature_extraction/text.py; Convert a collection of raw documents to a matrix of TF-IDF features.  Equivalent to CountVectorizer followed by TfidfTransformer.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- input : string {'filename', 'file', 'content'}     If 'filename', the sequence passed as an argument to fit is     expected to be a list of filenames that need reading to fetch     the raw content to analyze.      If 'file', the sequence items must have a 'read' method (file-like     object) that is called to fetch the bytes in memory.      Otherwise the input is expected to be the sequence strings or     bytes items are expected to be analyzed directly.  encoding : string, 'utf-8' by default.     If bytes or files are given to analyze, this encoding is used to     decode.  decode_error : {'strict', 'ignore', 'replace'}     Instruction on what to do if a byte sequence is given to analyze that     contains characters not of the given `encoding`. By default, it is     'strict', meaning that a UnicodeDecodeError will be raised. Other     values are 'ignore' and 'replace'.  strip_accents : {'ascii', 'unicode', None}     Remove accents during the preprocessing step.     'ascii' is a fast method that only works on characters that have     an direct ASCII mapping.     'unicode' is a slightly slower method that works on any characters.     None (default) does nothing.  analyzer : string, {'word', 'char'} or callable     Whether the feature should be made of word or character n-grams.      If a callable is passed it is used to extract the sequence of features     out of the raw, unprocessed input.  preprocessor : callable or None (default)     Override the preprocessing (string transformation) stage while     preserving the tokenizing and n-grams generation steps.  tokenizer : callable or None (default)     Override the string tokenization step while preserving the     preprocessing and n-grams generation steps.     Only applies if ``analyzer == 'word'``.  ngram_range : tuple (min_n, max_n)     The lower and upper boundary of the range of n-values for different     n-grams to be extracted. All values of n such that min_n <= n <= max_n     will be used.  stop_words : string {'english'}, list, or None (default)     If a string, it is passed to _check_stop_list and the appropriate stop     list is returned. 'english' is currently the only supported string     value.      If a list, that list is assumed to contain stop words, all of which     will be removed from the resulting tokens.     Only applies if ``analyzer == 'word'``.      If None, no stop words will be used. max_df can be set to a value     in the range [0.7, 1.0) to automatically detect and filter stop     words based on intra corpus document frequency of terms.  lowercase : boolean, default True     Convert all characters to lowercase before tokenizing.  token_pattern : string     Regular expression denoting what constitutes a 'token', only used     if ``analyzer == 'word'``. The default regexp selects tokens of 2     or more alphanumeric characters (punctuation is completely ignored     and always treated as a token separator).  max_df : float in range [0.0, 1.0] or int, default=1.0     When building the vocabulary ignore terms that have a document     frequency strictly higher than the given threshold (corpus-specific     stop words).     If float, the parameter represents a proportion of documents, integer     absolute counts.     This parameter is ignored if vocabulary is not None.  min_df : float in range [0.0, 1.0] or int, default=1     When building the vocabulary ignore terms that have a document     frequency strictly lower than the given threshold. This value is also     called cut-off in the literature.     If float, the parameter represents a proportion of documents, integer     absolute counts.     This parameter is ignored if vocabulary is not None.  max_features : int or None, default=None     If not None, build a vocabulary that only consider the top     max_features ordered by term frequency across the corpus.      This parameter is ignored if vocabulary is not None.  vocabulary : Mapping or iterable, optional     Either a Mapping (e.g., a dict) where keys are terms and values are     indices in the feature matrix, or an iterable over terms. If not     given, a vocabulary is determined from the input documents.  binary : boolean, default=False     If True, all non-zero term counts are set to 1. This does not mean     outputs will have only 0/1 values, only that the tf term in tf-idf     is binary. (Set idf and normalization to False to get 0/1 outputs.)  dtype : type, optional     Type of the matrix returned by fit_transform() or transform().  norm : 'l1', 'l2' or None, optional     Norm used to normalize term vectors. None for no normalization.  use_idf : boolean, default=True     Enable inverse-document-frequency reweighting.  smooth_idf : boolean, default=True     Smooth idf weights by adding one to document frequencies, as if an     extra document was seen containing every term in the collection     exactly once. Prevents zero divisions.  sublinear_tf : boolean, default=False     Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).  Attributes ---------- idf_ : array, shape = [n_features], or None     The learned idf vector (global term weights)     when ``use_idf`` is set to True, None otherwise.  stop_words_ : set     Terms that were ignored because they either:        - occurred in too many documents (`max_df`)       - occurred in too few documents (`min_df`)       - were cut off by feature selection (`max_features`).      This is only available if no vocabulary was given.  See also -------- CountVectorizer     Tokenize the documents and count the occurrences of token and return     them as a sparse matrix  TfidfTransformer     Apply Term Frequency Inverse Document Frequency normalization to a     sparse matrix of occurrence counts.  Notes ----- The ``stop_words_`` attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.", [
d("__init__(self, input, encoding, decode_error, strip_accents, lowercase, preprocessor, tokenizer, analyzer, stop_words, token_pattern, ngram_range, max_df, min_df, max_features, vocabulary, binary, dtype, norm, use_idf, smooth_idf, sublinear_tf)"),
d("norm(self)"),
d("norm(self, value)"),
d("use_idf(self)"),
d("use_idf(self, value)"),
d("smooth_idf(self)"),
d("smooth_idf(self, value)"),
d("sublinear_tf(self)"),
d("sublinear_tf(self, value)"),
d("idf_(self)"),
d("fit(self, raw_documents, y)"),
d("fit_transform(self, raw_documents, y)"),
d("transform(self, raw_documents, copy)"),]),
c("PatchExtractor(BaseEstimator)", "/feature_extraction/image.py; Extracts patches from a collection of images  Read more in the :ref:`User Guide <image_feature_extraction>`.  Parameters ---------- patch_size : tuple of ints (patch_height, patch_width)     the dimensions of one patch  max_patches : integer or float, optional default is None     The maximum number of patches per image to extract. If max_patches is a     float in (0, 1), it is taken to mean a proportion of the total number     of patches.  random_state : int or RandomState     Pseudo number generator state used for random sampling.", [
d("__init__(self, patch_size, max_patches, random_state)"),
d("fit(self, X, y)"),
d("transform(self, X)"),]),
c("FeatureHasher(BaseEstimator, TransformerMixin)", "/feature_extraction/hashing.py; Implements feature hashing, aka the hashing trick.  This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.  Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.  This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.  Read more in the :ref:`User Guide <feature_hashing>`.  Parameters ---------- n_features : integer, optional     The number of features (columns) in the output matrices. Small numbers     of features are likely to cause hash collisions, but large numbers     will cause larger coefficient dimensions in linear learners. dtype : numpy type, optional     The type of feature values. Passed to scipy.sparse matrix constructors     as the dtype argument. Do not set this to bool, np.boolean or any     unsigned integer type. input_type : string, optional     Either 'dict' (the default) to accept dictionaries over     (feature_name, value); 'pair' to accept pairs of (feature_name, value);     or 'string' to accept single strings.     feature_name should be a string, while value should be a number.     In the case of 'string', a value of 1 is implied.     The feature_name is hashed to find the appropriate column for the     feature. The value's sign might be flipped in the output (but see     non_negative, below). non_negative : boolean, optional, default np.float64     Whether output matrices should contain non-negative values only;     effectively calls abs on the matrix prior to returning it.     When True, output values can be interpreted as frequencies.     When False, output values will have expected value zero.  Examples -------- >>> from sklearn.feature_extraction import FeatureHasher >>> h = FeatureHasher(n_features=10) >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}] >>> f = h.transform(D) >>> f.toarray() array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],        [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])         See also -------- DictVectorizer : vectorizes string-valued features using a hash table. sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features   encoded as columns of integers.", [
d("__init__(self, n_features, input_type, dtype, non_negative)"),
d("_validate_params(n_features, input_type)"),
d("fit(self, X, y)"),
d("transform(self, raw_X, y)"),]),
c("BaseSpectral()", "/cluster/bicluster.py; Base class for spectral biclustering.", [
d("__init__(self, n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, n_jobs, random_state)"),
d("_check_parameters(self)"),
d("fit(self, X)"),
d("_svd(self, array, n_components, n_discard)"),
d("_k_means(self, data, n_clusters)"),]),
c("SpectralCoclustering(BaseSpectral)", "/cluster/bicluster.py; Spectral Co-Clustering algorithm (Dhillon, 2001).  Clusters rows and columns of an array `X` to solve the relaxed normalized cut of the bipartite graph created from `X` as follows: the edge between row vertex `i` and column vertex `j` has weight `X[i, j]`.  The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.  Supports sparse matrices, as long as they are nonnegative.  Read more in the :ref:`User Guide <spectral_coclustering>`.  Parameters ---------- n_clusters : integer, optional, default: 3     The number of biclusters to find.  svd_method : string, optional, default: 'randomized'     Selects the algorithm for finding singular vectors. May be     'randomized' or 'arpack'. If 'randomized', use     :func:`sklearn.utils.extmath.randomized_svd`, which may be faster     for large matrices. If 'arpack', use     :func:`sklearn.utils.arpack.svds`, which is more accurate, but     possibly slower in some cases.  n_svd_vecs : int, optional, default: None     Number of vectors to use in calculating the SVD. Corresponds     to `ncv` when `svd_method=arpack` and `n_oversamples` when     `svd_method` is 'randomized`.  mini_batch : bool, optional, default: False     Whether to use mini-batch k-means, which is faster but may get     different results.  init : {'k-means++', 'random' or an ndarray}      Method for initialization of k-means algorithm; defaults to      'k-means++'.  n_init : int, optional, default: 10     Number of random initializations that are tried with the     k-means algorithm.      If mini-batch k-means is used, the best initialization is     chosen and the algorithm runs once. Otherwise, the algorithm     is run for each initialization and the best solution chosen.  n_jobs : int, optional, default: 1     The number of jobs to use for the computation. This works by breaking     down the pairwise matrix into n_jobs even slices and computing them in     parallel.      If -1 all CPUs are used. If 1 is given, no parallel computing code is     used at all, which is useful for debugging. For n_jobs below -1,     (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one     are used.  random_state : int seed, RandomState instance, or None (default)     A pseudo random number generator used by the K-Means     initialization.  Attributes ---------- rows_ : array-like, shape (n_row_clusters, n_rows)     Results of the clustering. `rows[i, r]` is True if     cluster `i` contains row `r`. Available only after calling ``fit``.  columns_ : array-like, shape (n_column_clusters, n_columns)     Results of the clustering, like `rows`.  row_labels_ : array-like, shape (n_rows,)     The bicluster label of each row.  column_labels_ : array-like, shape (n_cols,)     The bicluster label of each column.  References ----------  * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using   bipartite spectral graph partitioning   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.", [
d("__init__(self, n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, n_jobs, random_state)"),
d("_fit(self, X)"),]),
c("SpectralBiclustering(BaseSpectral)", "/cluster/bicluster.py; Spectral biclustering (Kluger, 2003).  Partitions rows and columns under the assumption that the data has an underlying checkerboard structure. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters. The outer product of the corresponding row and column label vectors gives this checkerboard structure.  Read more in the :ref:`User Guide <spectral_biclustering>`.  Parameters ---------- n_clusters : integer or tuple (n_row_clusters, n_column_clusters)     The number of row and column clusters in the checkerboard     structure.  method : string, optional, default: 'bistochastic'     Method of normalizing and converting singular vectors into     biclusters. May be one of 'scale', 'bistochastic', or 'log'.     The authors recommend using 'log'. If the data is sparse,     however, log normalization will not work, which is why the     default is 'bistochastic'. CAUTION: if `method='log'`, the     data must not be sparse.  n_components : integer, optional, default: 6     Number of singular vectors to check.  n_best : integer, optional, default: 3     Number of best singular vectors to which to project the data     for clustering.  svd_method : string, optional, default: 'randomized'     Selects the algorithm for finding singular vectors. May be     'randomized' or 'arpack'. If 'randomized', uses     `sklearn.utils.extmath.randomized_svd`, which may be faster     for large matrices. If 'arpack', uses     `sklearn.utils.arpack.svds`, which is more accurate, but     possibly slower in some cases.  n_svd_vecs : int, optional, default: None     Number of vectors to use in calculating the SVD. Corresponds     to `ncv` when `svd_method=arpack` and `n_oversamples` when     `svd_method` is 'randomized`.  mini_batch : bool, optional, default: False     Whether to use mini-batch k-means, which is faster but may get     different results.  init : {'k-means++', 'random' or an ndarray}      Method for initialization of k-means algorithm; defaults to      'k-means++'.  n_init : int, optional, default: 10     Number of random initializations that are tried with the     k-means algorithm.      If mini-batch k-means is used, the best initialization is     chosen and the algorithm runs once. Otherwise, the algorithm     is run for each initialization and the best solution chosen.  n_jobs : int, optional, default: 1     The number of jobs to use for the computation. This works by breaking     down the pairwise matrix into n_jobs even slices and computing them in     parallel.      If -1 all CPUs are used. If 1 is given, no parallel computing code is     used at all, which is useful for debugging. For n_jobs below -1,     (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one     are used.  random_state : int seed, RandomState instance, or None (default)     A pseudo random number generator used by the K-Means     initialization.  Attributes ---------- rows_ : array-like, shape (n_row_clusters, n_rows)     Results of the clustering. `rows[i, r]` is True if     cluster `i` contains row `r`. Available only after calling ``fit``.  columns_ : array-like, shape (n_column_clusters, n_columns)     Results of the clustering, like `rows`.  row_labels_ : array-like, shape (n_rows,)     Row partition labels.  column_labels_ : array-like, shape (n_cols,)     Column partition labels.  References ----------  * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray   data: coclustering genes and conditions   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.", [
d("__init__(self, n_clusters, method, n_components, n_best, svd_method, n_svd_vecs, mini_batch, init, n_init, n_jobs, random_state)"),
d("_check_parameters(self)"),
d("_fit(self, X)"),
d("_fit_best_piecewise(self, vectors, n_best, n_clusters)"),
d("_project_and_cluster(self, data, vectors, n_clusters)"),]),
c("AgglomerativeClustering(BaseEstimator, ClusterMixin)", "/cluster/hierarchical.py; Agglomerative Clustering  Recursively merges the pair of clusters that minimally increases a given linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int, default=2     The number of clusters to find.  connectivity : array-like or callable, optional     Connectivity matrix. Defines for each sample the neighboring     samples following a given structure of the data.     This can be a connectivity matrix itself or a callable that transforms     the data into a connectivity matrix, such as derived from     kneighbors_graph. Default is None, i.e, the     hierarchical clustering algorithm is unstructured.  affinity : string or callable, default: 'euclidean'     Metric used to compute the linkage. Can be 'euclidean', 'l1', 'l2',     'manhattan', 'cosine', or 'precomputed'.     If linkage is 'ward', only 'euclidean' is accepted.  memory : Instance of joblib.Memory or string (optional)     Used to cache the output of the computation of the tree.     By default, no caching is done. If a string is given, it is the     path to the caching directory.  n_components : int (optional)     Number of connected components. If None the number of connected     components is estimated from the connectivity matrix.     NOTE: This parameter is now directly determined from the connectivity     matrix and will be removed in 0.18  compute_full_tree : bool or 'auto' (optional)     Stop early the construction of the tree at n_clusters. This is     useful to decrease computation time if the number of clusters is     not small compared to the number of samples. This option is     useful only when specifying a connectivity matrix. Note also that     when varying the number of clusters and using caching, it may     be advantageous to compute the full tree.  linkage : {'ward', 'complete', 'average'}, optional, default: 'ward'     Which linkage criterion to use. The linkage criterion determines which     distance to use between sets of observation. The algorithm will merge     the pairs of cluster that minimize this criterion.      - ward minimizes the variance of the clusters being merged.     - average uses the average of the distances of each observation of       the two sets.     - complete or maximum linkage uses the maximum distances between       all observations of the two sets.  pooling_func : callable, default=np.mean     This combines the values of agglomerated features into a single     value, and should accept an array of shape [M, N] and the keyword     argument ``axis=1``, and reduce it to an array of size [M].  Attributes ---------- labels_ : array [n_samples]     cluster labels for each point  n_leaves_ : int     Number of leaves in the hierarchical tree.  n_components_ : int     The estimated number of connected components in the graph.  children_ : array-like, shape (n_nodes-1, 2)     The children of each non-leaf node. Values less than `n_samples`     correspond to leaves of the tree which are the original samples.     A node `i` greater than or equal to `n_samples` is a non-leaf     node and has children `children_[i - n_samples]`. Alternatively     at the i-th iteration, children[i][0] and children[i][1]     are merged to form node `n_samples + i`", [
d("__init__(self, n_clusters, affinity, memory, connectivity, n_components, compute_full_tree, linkage, pooling_func)"),
d("fit(self, X, y)"),]),
c("FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform)", "/cluster/hierarchical.py; Agglomerate features.  Similar to AgglomerativeClustering, but recursively merges features instead of samples.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int, default 2     The number of clusters to find.  connectivity : array-like or callable, optional     Connectivity matrix. Defines for each feature the neighboring     features following a given structure of the data.     This can be a connectivity matrix itself or a callable that transforms     the data into a connectivity matrix, such as derived from     kneighbors_graph. Default is None, i.e, the     hierarchical clustering algorithm is unstructured.  affinity : string or callable, default 'euclidean'     Metric used to compute the linkage. Can be 'euclidean', 'l1', 'l2',     'manhattan', 'cosine', or 'precomputed'.     If linkage is 'ward', only 'euclidean' is accepted.  memory : Instance of joblib.Memory or string, optional     Used to cache the output of the computation of the tree.     By default, no caching is done. If a string is given, it is the     path to the caching directory.  n_components : int (optional)     Number of connected components. If None the number of connected     components is estimated from the connectivity matrix.     NOTE: This parameter is now directly determined from the connectivity     matrix and will be removed in 0.18  compute_full_tree : bool or 'auto', optional, default 'auto'     Stop early the construction of the tree at n_clusters. This is     useful to decrease computation time if the number of clusters is     not small compared to the number of features. This option is     useful only when specifying a connectivity matrix. Note also that     when varying the number of clusters and using caching, it may     be advantageous to compute the full tree.  linkage : {'ward', 'complete', 'average'}, optional, default 'ward'     Which linkage criterion to use. The linkage criterion determines which     distance to use between sets of features. The algorithm will merge     the pairs of cluster that minimize this criterion.      - ward minimizes the variance of the clusters being merged.     - average uses the average of the distances of each feature of       the two sets.     - complete or maximum linkage uses the maximum distances between       all features of the two sets.  pooling_func : callable, default np.mean     This combines the values of agglomerated features into a single     value, and should accept an array of shape [M, N] and the keyword     argument `axis=1`, and reduce it to an array of size [M].  Attributes ---------- labels_ : array-like, (n_features,)     cluster labels for each feature.  n_leaves_ : int     Number of leaves in the hierarchical tree.  n_components_ : int     The estimated number of connected components in the graph.  children_ : array-like, shape (n_nodes-1, 2)     The children of each non-leaf node. Values less than `n_features`     correspond to leaves of the tree which are the original samples.     A node `i` greater than or equal to `n_features` is a non-leaf     node and has children `children_[i - n_features]`. Alternatively     at the i-th iteration, children[i][0] and children[i][1]     are merged to form node `n_features + i`", [
d("fit(self, X, y)"),
d("fit_predict(self)"),]),
c("_CFNode(object)", "/cluster/birch.py; Each node in a CFTree is called a CFNode.  The CFNode can have a maximum of branching_factor number of CFSubclusters.  Parameters ---------- threshold : float     Threshold needed for a new subcluster to enter a CFSubcluster.  branching_factor : int     Maximum number of CF subclusters in each node.  is_leaf : bool     We need to know if the CFNode is a leaf or not, in order to     retrieve the final subclusters.  n_features : int     The number of features.  Attributes ---------- subclusters_ : array-like     list of subclusters for a particular CFNode.  prev_leaf_ : _CFNode     prev_leaf. Useful only if is_leaf is True.  next_leaf_ : _CFNode     next_leaf. Useful only if is_leaf is True.     the final subclusters.  init_centroids_ : ndarray, shape (branching_factor + 1, n_features)     manipulate ``init_centroids_`` throughout rather than centroids_ since     the centroids are just a view of the ``init_centroids_`` .  init_sq_norm_ : ndarray, shape (branching_factor + 1,)     manipulate init_sq_norm_ throughout. similar to ``init_centroids_``.  centroids_ : ndarray     view of ``init_centroids_``.  squared_norm_ : ndarray     view of ``init_sq_norm_``.", [
d("__init__(self, threshold, branching_factor, is_leaf, n_features)"),
d("append_subcluster(self, subcluster)"),
d("update_split_subclusters(self, subcluster, new_subcluster1, new_subcluster2)"),
d("insert_cf_subcluster(self, subcluster)"),]),
c("_CFSubcluster(object)", "/cluster/birch.py; Each subcluster in a CFNode is called a CFSubcluster.  A CFSubcluster can have a CFNode has its child.  Parameters ---------- linear_sum : ndarray, shape (n_features,), optional     Sample. This is kept optional to allow initialization of empty     subclusters.  Attributes ---------- n_samples_ : int     Number of samples that belong to each subcluster.  linear_sum_ : ndarray     Linear sum of all the samples in a subcluster. Prevents holding     all sample data in memory.  squared_sum_ : float     Sum of the squared l2 norms of all samples belonging to a subcluster.  centroid_ : ndarray     Centroid of the subcluster. Prevent recomputing of centroids when     ``CFNode.centroids_`` is called.  child_ : _CFNode     Child Node of the subcluster. Once a given _CFNode is set as the child     of the _CFNode, it is set to ``self.child_``.  sq_norm_ : ndarray     Squared norm of the subcluster. Used to prevent recomputing when     pairwise minimum distances are computed.", [
d("__init__(self, linear_sum)"),
d("update(self, subcluster)"),
d("merge_subcluster(self, nominee_cluster, threshold)"),
d("radius(self)"),]),
c("Birch(BaseEstimator, TransformerMixin, ClusterMixin)", "/cluster/birch.py; Implements the Birch clustering algorithm.  Every new sample is inserted into the root of the Clustering Feature Tree. It is then clubbed together with the subcluster that has the centroid closest to the new sample. This is done recursively till it ends up at the subcluster of the leaf of the tree has the closest centroid.  Read more in the :ref:`User Guide <birch>`.  Parameters ---------- threshold : float, default 0.5     The radius of the subcluster obtained by merging a new sample and the     closest subcluster should be lesser than the threshold. Otherwise a new     subcluster is started.  branching_factor : int, default 50     Maximum number of CF subclusters in each node. If a new samples enters     such that the number of subclusters exceed the branching_factor then     the node has to be split. The corresponding parent also has to be     split and if the number of subclusters in the parent is greater than     the branching factor, then it has to be split recursively.  n_clusters : int, instance of sklearn.cluster model, default None     Number of clusters after the final clustering step, which treats the     subclusters from the leaves as new samples. By default, this final     clustering step is not performed and the subclusters are returned     as they are. If a model is provided, the model is fit treating     the subclusters as new samples and the initial data is mapped to the     label of the closest subcluster. If an int is provided, the model     fit is AgglomerativeClustering with n_clusters set to the int.  compute_labels : bool, default True     Whether or not to compute labels for each fit.  copy : bool, default True     Whether or not to make a copy of the given data. If set to False,     the initial data will be overwritten.  Attributes ---------- root_ : _CFNode     Root of the CFTree.  dummy_leaf_ : _CFNode     Start pointer to all the leaves.  subcluster_centers_ : ndarray,     Centroids of all subclusters read directly from the leaves.  subcluster_labels_ : ndarray,     Labels assigned to the centroids of the subclusters after     they are clustered globally.  labels_ : ndarray, shape (n_samples,)     Array of labels assigned to the input data.     if partial_fit is used instead of fit, they are assigned to the     last batch of data.  Examples -------- >>> from sklearn.cluster import Birch >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]] >>> brc = Birch(branching_factor=50, n_clusters=None, threshold=0.5, ... compute_labels=True) >>> brc.fit(X) Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=None,    threshold=0.5) >>> brc.predict(X) array([0, 0, 0, 1, 1, 1])  References ---------- * Tian Zhang, Raghu Ramakrishnan, Maron Livny   BIRCH: An efficient data clustering method for large databases.   http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf  * Roberto Perdisci   JBirch - Java implementation of BIRCH clustering algorithm   https://code.google.com/p/jbirch/", [
d("__init__(self, threshold, branching_factor, n_clusters, compute_labels, copy)"),
d("fit(self, X, y)"),
d("_fit(self, X)"),
d("_get_leaves(self)"),
d("partial_fit(self, X, y)"),
d("_check_fit(self, X)"),
d("predict(self, X)"),
d("transform(self, X, y)"),
d("_global_clustering(self, X)"),]),
c("AgglomerationTransform(TransformerMixin)", "/cluster/_feature_agglomeration.py; A class for feature agglomeration via the transform interface", [
d("transform(self, X, pooling_func)"),
d("inverse_transform(self, Xred)"),]),
c("KMeans(BaseEstimator, ClusterMixin, TransformerMixin)", "/cluster/k_means_.py; K-Means clustering  Read more in the :ref:`User Guide <k_means>`.  Parameters ----------  n_clusters : int, optional, default: 8     The number of clusters to form as well as the number of     centroids to generate.  max_iter : int, default: 300     Maximum number of iterations of the k-means algorithm for a     single run.  n_init : int, default: 10     Number of time the k-means algorithm will be run with different     centroid seeds. The final results will be the best output of     n_init consecutive runs in terms of inertia.  init : {'k-means++', 'random' or an ndarray}     Method for initialization, defaults to 'k-means++':      'k-means++' : selects initial cluster centers for k-mean     clustering in a smart way to speed up convergence. See section     Notes in k_init for more details.      'random': choose k observations (rows) at random from data for     the initial centroids.      If an ndarray is passed, it should be of shape (n_clusters, n_features)     and gives the initial centers.  precompute_distances : {'auto', True, False}     Precompute distances (faster but takes more memory).      'auto' : do not precompute distances if n_samples * n_clusters > 12     million. This corresponds to about 100MB overhead per job using     double precision.      True : always precompute distances      False : never precompute distances  tol : float, default: 1e-4     Relative tolerance with regards to inertia to declare convergence  n_jobs : int     The number of jobs to use for the computation. This works by computing     each of the n_init runs in parallel.      If -1 all CPUs are used. If 1 is given, no parallel computing code is     used at all, which is useful for debugging. For n_jobs below -1,     (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one     are used.  random_state : integer or numpy.RandomState, optional     The generator used to initialize the centers. If an integer is     given, it fixes the seed. Defaults to the global numpy random     number generator.  verbose : int, default 0     Verbosity mode.  copy_x : boolean, default True     When pre-computing distances it is more numerically accurate to center     the data first.  If copy_x is True, then the original data is not     modified.  If False, the original data is modified, and put back before     the function returns, but small numerical differences may be introduced     by subtracting and then adding the data mean.  Attributes ---------- cluster_centers_ : array, [n_clusters, n_features]     Coordinates of cluster centers  labels_ :     Labels of each point  inertia_ : float     Sum of distances of samples to their closest cluster center.  Notes ------ The k-means problem is solved using Lloyd's algorithm.  The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.  The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, 'How slow is the k-means method?' SoCG2006)  In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That's why it can be useful to restart it several times.  See also --------  MiniBatchKMeans:     Alternative online implementation that does incremental updates     of the centers positions using mini-batches.     For large scale learning (say n_samples > 10k) MiniBatchKMeans is     probably much faster to than the default batch implementation.", [
d("__init__(self, n_clusters, init, n_init, max_iter, tol, precompute_distances, verbose, random_state, copy_x, n_jobs)"),
d("_check_fit_data(self, X)"),
d("_check_test_data(self, X)"),
d("fit(self, X, y)"),
d("fit_predict(self, X, y)"),
d("fit_transform(self, X, y)"),
d("transform(self, X, y)"),
d("_transform(self, X)"),
d("predict(self, X)"),
d("score(self, X, y)"),]),
c("MiniBatchKMeans(KMeans)", "/cluster/k_means_.py; Mini-Batch K-Means clustering  Parameters ----------  n_clusters : int, optional, default: 8     The number of clusters to form as well as the number of     centroids to generate.  max_iter : int, optional     Maximum number of iterations over the complete dataset before     stopping independently of any early stopping criterion heuristics.  max_no_improvement : int, default: 10     Control early stopping based on the consecutive number of mini     batches that does not yield an improvement on the smoothed inertia.      To disable convergence detection based on inertia, set     max_no_improvement to None.  tol : float, default: 0.0     Control early stopping based on the relative center changes as     measured by a smoothed, variance-normalized of the mean center     squared position changes. This early stopping heuristics is     closer to the one used for the batch variant of the algorithms     but induces a slight computational and memory overhead over the     inertia heuristic.      To disable convergence detection based on normalized center     change, set tol to 0.0 (default).  batch_size : int, optional, default: 100     Size of the mini batches.  init_size : int, optional, default: 3 * batch_size     Number of samples to randomly sample for speeding up the     initialization (sometimes at the expense of accuracy): the     only algorithm is initialized by running a batch KMeans on a     random subset of the data. This needs to be larger than n_clusters.  init : {'k-means++', 'random' or an ndarray}, default: 'k-means++'     Method for initialization, defaults to 'k-means++':      'k-means++' : selects initial cluster centers for k-mean     clustering in a smart way to speed up convergence. See section     Notes in k_init for more details.      'random': choose k observations (rows) at random from data for     the initial centroids.      If an ndarray is passed, it should be of shape (n_clusters, n_features)     and gives the initial centers.  n_init : int, default=3     Number of random initializations that are tried.     In contrast to KMeans, the algorithm is only run once, using the     best of the ``n_init`` initializations as measured by inertia.  compute_labels : boolean, default=True     Compute label assignment and inertia for the complete dataset     once the minibatch optimization has converged in fit.  random_state : integer or numpy.RandomState, optional     The generator used to initialize the centers. If an integer is     given, it fixes the seed. Defaults to the global numpy random     number generator.  reassignment_ratio : float, default: 0.01     Control the fraction of the maximum number of counts for a     center to be reassigned. A higher value means that low count     centers are more easily reassigned, which means that the     model will take longer to converge, but should converge in a     better clustering.  verbose : boolean, optional     Verbosity mode.  Attributes ----------  cluster_centers_ : array, [n_clusters, n_features]     Coordinates of cluster centers  labels_ :     Labels of each point (if compute_labels is set to True).  inertia_ : float     The value of the inertia criterion associated with the chosen     partition (if compute_labels is set to True). The inertia is     defined as the sum of square distances of samples to their nearest     neighbor.  Notes ----- See http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf", [
d("__init__(self, n_clusters, init, max_iter, batch_size, verbose, compute_labels, random_state, tol, max_no_improvement, init_size, n_init, reassignment_ratio)"),
d("fit(self, X, y)"),
d("_labels_inertia_minibatch(self, X)"),
d("partial_fit(self, X, y)"),
d("predict(self, X)"),]),
c("DBSCAN(BaseEstimator, ClusterMixin)", "/cluster/dbscan_.py; Perform DBSCAN clustering from vector array or distance matrix.  DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.  Read more in the :ref:`User Guide <dbscan>`.  Parameters ---------- eps : float, optional     The maximum distance between two samples for them to be considered     as in the same neighborhood. min_samples : int, optional     The number of samples (or total weight) in a neighborhood for a point     to be considered as a core point. This includes the point itself. metric : string, or callable     The metric to use when calculating distance between instances in a     feature array. If metric is a string or callable, it must be one of     the options allowed by metrics.pairwise.calculate_distance for its     metric parameter.     If metric is 'precomputed', X is assumed to be a distance matrix and     must be square. algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional     The algorithm to be used by the NearestNeighbors module     to compute pointwise distances and find nearest neighbors.     See NearestNeighbors module documentation for details. leaf_size : int, optional (default = 30)     Leaf size passed to BallTree or cKDTree. This can affect the speed     of the construction and query, as well as the memory required     to store the tree. The optimal value depends     on the nature of the problem. random_state: numpy.RandomState, optional     Deprecated and ignored as of version 0.16, will be removed in version     0.18. DBSCAN does not use random initialization.  Attributes ---------- core_sample_indices_ : array, shape = [n_core_samples]     Indices of core samples.  components_ : array, shape = [n_core_samples, n_features]     Copy of each core sample found by training.  labels_ : array, shape = [n_samples]     Cluster labels for each point in the dataset given to fit().     Noisy samples are given the label -1.  Notes ----- See examples/cluster/plot_dbscan.py for an example.  This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n).  References ---------- Ester, M., H. P. Kriegel, J. Sander, and X. Xu, 'A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise'. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996", [
d("__init__(self, eps, min_samples, metric, algorithm, leaf_size, p, random_state)"),
d("fit(self, X, y, sample_weight)"),
d("fit_predict(self, X, y, sample_weight)"),]),
c("AffinityPropagation(BaseEstimator, ClusterMixin)", "/cluster/affinity_propagation_.py; Perform Affinity Propagation Clustering of data.  Read more in the :ref:`User Guide <affinity_propagation>`.  Parameters ---------- damping : float, optional, default: 0.5     Damping factor between 0.5 and 1.  convergence_iter : int, optional, default: 15     Number of iterations with no change in the number     of estimated clusters that stops the convergence.  max_iter : int, optional, default: 200     Maximum number of iterations.  copy : boolean, optional, default: True     Make a copy of input data.  preference : array-like, shape (n_samples,) or float, optional     Preferences for each point - points with larger values of     preferences are more likely to be chosen as exemplars. The number     of exemplars, ie of clusters, is influenced by the input     preferences value. If the preferences are not passed as arguments,     they will be set to the median of the input similarities.  affinity : string, optional, default=``euclidean``     Which affinity to use. At the moment ``precomputed`` and     ``euclidean`` are supported. ``euclidean`` uses the     negative squared euclidean distance between points.  verbose : boolean, optional, default: False     Whether to be verbose.   Attributes ---------- cluster_centers_indices_ : array, shape (n_clusters,)     Indices of cluster centers  cluster_centers_ : array, shape (n_clusters, n_features)     Cluster centers (if affinity != ``precomputed``).  labels_ : array, shape (n_samples,)     Labels of each point  affinity_matrix_ : array, shape (n_samples, n_samples)     Stores the affinity matrix used in ``fit``.  n_iter_ : int     Number of iterations taken to converge.  Notes ----- See examples/cluster/plot_affinity_propagation.py for an example.  The algorithmic complexity of affinity propagation is quadratic in the number of points.  References ----------  Brendan J. Frey and Delbert Dueck, 'Clustering by Passing Messages Between Data Points', Science Feb. 2007", [
d("__init__(self, damping, max_iter, convergence_iter, copy, preference, affinity, verbose)"),
d("_pairwise(self)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("MeanShift(BaseEstimator, ClusterMixin)", "/cluster/mean_shift_.py; Mean shift clustering using a flat kernel.  Mean shift clustering aims to discover 'blobs' in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.  Seeding is performed using a binning technique for scalability.  Read more in the :ref:`User Guide <mean_shift>`.  Parameters ---------- bandwidth : float, optional     Bandwidth used in the RBF kernel.      If not given, the bandwidth is estimated using     sklearn.cluster.estimate_bandwidth; see the documentation for that     function for hints on scalability (see also the Notes, below).  seeds : array, shape=[n_samples, n_features], optional     Seeds used to initialize kernels. If not set,     the seeds are calculated by clustering.get_bin_seeds     with bandwidth as the grid size and default values for     other parameters.  bin_seeding : boolean, optional     If true, initial kernel locations are not locations of all     points, but rather the location of the discretized version of     points, where points are binned onto a grid whose coarseness     corresponds to the bandwidth. Setting this option to True will speed     up the algorithm because fewer seeds will be initialized.     default value: False     Ignored if seeds argument is not None.  min_bin_freq : int, optional    To speed up the algorithm, accept only those bins with at least    min_bin_freq points as seeds. If not defined, set to 1.  cluster_all : boolean, default True     If true, then all points are clustered, even those orphans that are     not within any kernel. Orphans are assigned to the nearest kernel.     If false, then orphans are given cluster label -1.  Attributes ---------- cluster_centers_ : array, [n_clusters, n_features]     Coordinates of cluster centers.  labels_ :     Labels of each point.  Notes -----  Scalability:  Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will is to O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2).  Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the get_bin_seeds function.  Note that the estimate_bandwidth function is much less scalable than the mean shift algorithm and will be the bottleneck if it is used.  References ----------  Dorin Comaniciu and Peter Meer, 'Mean Shift: A robust approach toward feature space analysis'. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619.", [
d("__init__(self, bandwidth, seeds, bin_seeding, min_bin_freq, cluster_all)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("SpectralClustering(BaseEstimator, ClusterMixin)", "/cluster/spectral.py; Apply clustering to a projection to the normalized laplacian.  In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plan.  If affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.  When calling ``fit``, an affinity matrix is constructed using either kernel function such the Gaussian (aka RBF) kernel of the euclidean distanced ``d(X, X)``::          np.exp(-gamma * d(X,X) ** 2)  or a k-nearest neighbors connectivity matrix.  Alternatively, using ``precomputed``, a user-provided affinity matrix can be used.  Read more in the :ref:`User Guide <spectral_clustering>`.  Parameters ----------- n_clusters : integer, optional     The dimension of the projection subspace.  affinity : string, array-like or callable, default 'rbf'     If a string, this may be one of 'nearest_neighbors', 'precomputed',     'rbf' or one of the kernels supported by     `sklearn.metrics.pairwise_kernels`.      Only kernels that produce similarity scores (non-negative values that     increase with similarity) should be used. This property is not checked     by the clustering algorithm.  gamma : float     Scaling factor of RBF, polynomial, exponential chi^2 and     sigmoid affinity kernel. Ignored for     ``affinity='nearest_neighbors'``.  degree : float, default=3     Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1     Zero coefficient for polynomial and sigmoid kernels.     Ignored by other kernels.  n_neighbors : integer     Number of neighbors to use when constructing the affinity matrix using     the nearest neighbors method. Ignored for ``affinity='rbf'``.  eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}     The eigenvalue decomposition strategy to use. AMG requires pyamg     to be installed. It can be faster on very large, sparse problems,     but may also lead to instabilities  random_state : int seed, RandomState instance, or None (default)     A pseudo random number generator used for the initialization     of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'     and by the K-Means initialization.  n_init : int, optional, default: 10     Number of time the k-means algorithm will be run with different     centroid seeds. The final results will be the best output of     n_init consecutive runs in terms of inertia.  eigen_tol : float, optional, default: 0.0     Stopping criterion for eigendecomposition of the Laplacian matrix     when using arpack eigen_solver.  assign_labels : {'kmeans', 'discretize'}, default: 'kmeans'     The strategy to use to assign labels in the embedding     space. There are two ways to assign labels after the laplacian     embedding. k-means can be applied and is a popular choice. But it can     also be sensitive to initialization. Discretization is another approach     which is less sensitive to random initialization.  kernel_params : dictionary of string to any, optional     Parameters (keyword arguments) and values for kernel passed as     callable object. Ignored by other kernels.  Attributes ---------- affinity_matrix_ : array-like, shape (n_samples, n_samples)     Affinity matrix used for clustering. Available only if after calling     ``fit``.  labels_ :     Labels of each point  Notes ----- If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel::      np.exp(- X ** 2 / (2. * delta ** 2))  Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points.  If the pyamg package is installed, it is used: this greatly speeds up computation.  References ----------  - Normalized cuts and image segmentation, 2000   Jianbo Shi, Jitendra Malik   http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324  - A Tutorial on Spectral Clustering, 2007   Ulrike von Luxburg   http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323  - Multiclass spectral clustering, 2003   Stella X. Yu, Jianbo Shi   http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf", [
d("__init__(self, n_clusters, eigen_solver, random_state, n_init, gamma, affinity, n_neighbors, eigen_tol, assign_labels, degree, coef0, kernel_params)"),
d("fit(self, X, y)"),
d("_pairwise(self)"),]),
c("MockBiclustering(BaseEstimator, BiclusterMixin)", "/cluster/tests/test_bicluster.py; ", [
d("__init__(self)"),
d("get_indices(self, i)"),]),
c("BaseDecisionTree()", "/tree/tree.py; Base class for decision trees.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, random_state, class_weight)"),
d("fit(self, X, y, sample_weight, check_input)"),
d("_validate_X_predict(self, X, check_input)"),
d("predict(self, X, check_input)"),
d("apply(self, X, check_input)"),
d("feature_importances_(self)"),]),
c("DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin)", "/tree/tree.py; A decision tree classifier.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : string, optional (default='gini')     The function to measure the quality of a split. Supported criteria are     'gini' for the Gini impurity and 'entropy' for the information gain.  splitter : string, optional (default='best')     The strategy used to choose the split at each node. Supported     strategies are 'best' to choose the best split and 'random' to choose     the best random split.  max_features : int, float, string or None, optional (default=None)     The number of features to consider when looking for the best split:       - If int, then consider `max_features` features at each split.       - If float, then `max_features` is a percentage and         `int(max_features * n_features)` features are considered at each         split.       - If 'auto', then `max_features=sqrt(n_features)`.       - If 'sqrt', then `max_features=sqrt(n_features)`.       - If 'log2', then `max_features=log2(n_features)`.       - If None, then `max_features=n_features`.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.  max_depth : int or None, optional (default=None)     The maximum depth of the tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.  min_samples_split : int, optional (default=2)     The minimum number of samples required to split an internal node.  min_samples_leaf : int, optional (default=1)     The minimum number of samples required to be at a leaf node.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.  max_leaf_nodes : int or None, optional (default=None)     Grow a tree with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.  class_weight : dict, list of dicts, 'balanced' or None, optional                (default=None)     Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one. For     multi-output problems, a list of dicts can be provided in the same     order as the columns of y.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``      For multi-output, the weights of each column of y will be multiplied.      Note that these weights will be multiplied with sample_weight (passed     through the fit method) if sample_weight is specified.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  Attributes ---------- classes_ : array of shape = [n_classes] or a list of such arrays     The classes labels (single output problem),     or a list of arrays of class labels (multi-output problem).  feature_importances_ : array of shape = [n_features]     The feature importances. The higher, the more important the     feature. The importance of a feature is computed as the (normalized)     total reduction of the criterion brought by that feature.  It is also     known as the Gini importance [4]_.  max_features_ : int,     The inferred value of max_features.  n_classes_ : int or list     The number of classes (for single output problems),     or a list containing the number of classes for each     output (for multi-output problems).  n_features_ : int     The number of features when ``fit`` is performed.  n_outputs_ : int     The number of outputs when ``fit`` is performed.  tree_ : Tree object     The underlying Tree object.  See also -------- DecisionTreeRegressor  References ----------  .. [1] http://en.wikipedia.org/wiki/Decision_tree_learning  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, 'Classification        and Regression Trees', Wadsworth, Belmont, CA, 1984.  .. [3] T. Hastie, R. Tibshirani and J. Friedman. 'Elements of Statistical        Learning', Springer, 2009.  .. [4] L. Breiman, and A. Cutler, 'Random Forests',        http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm  Examples -------- >>> from sklearn.datasets import load_iris >>> from sklearn.cross_validation import cross_val_score >>> from sklearn.tree import DecisionTreeClassifier >>> clf = DecisionTreeClassifier(random_state=0) >>> iris = load_iris() >>> cross_val_score(clf, iris.data, iris.target, cv=10) ...                             # doctest: +SKIP ... array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,         0.93...,  0.93...,  1.     ,  0.93...,  1.      ])", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, class_weight)"),
d("predict_proba(self, X, check_input)"),
d("predict_log_proba(self, X)"),]),
c("DecisionTreeRegressor(BaseDecisionTree, RegressorMixin)", "/tree/tree.py; A decision tree regressor.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : string, optional (default='mse')     The function to measure the quality of a split. The only supported     criterion is 'mse' for the mean squared error, which is equal to     variance reduction as feature selection criterion.  splitter : string, optional (default='best')     The strategy used to choose the split at each node. Supported     strategies are 'best' to choose the best split and 'random' to choose     the best random split.  max_features : int, float, string or None, optional (default=None)     The number of features to consider when looking for the best split:       - If int, then consider `max_features` features at each split.       - If float, then `max_features` is a percentage and         `int(max_features * n_features)` features are considered at each         split.       - If 'auto', then `max_features=n_features`.       - If 'sqrt', then `max_features=sqrt(n_features)`.       - If 'log2', then `max_features=log2(n_features)`.       - If None, then `max_features=n_features`.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.  max_depth : int or None, optional (default=None)     The maximum depth of the tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.  min_samples_split : int, optional (default=2)     The minimum number of samples required to split an internal node.  min_samples_leaf : int, optional (default=1)     The minimum number of samples required to be at a leaf node.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.  max_leaf_nodes : int or None, optional (default=None)     Grow a tree with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  Attributes ---------- feature_importances_ : array of shape = [n_features]     The feature importances.     The higher, the more important the feature.     The importance of a feature is computed as the     (normalized) total reduction of the criterion brought     by that feature. It is also known as the Gini importance [4]_.  max_features_ : int,     The inferred value of max_features.  n_features_ : int     The number of features when ``fit`` is performed.  n_outputs_ : int     The number of outputs when ``fit`` is performed.  tree_ : Tree object     The underlying Tree object.  See also -------- DecisionTreeClassifier  References ----------  .. [1] http://en.wikipedia.org/wiki/Decision_tree_learning  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, 'Classification        and Regression Trees', Wadsworth, Belmont, CA, 1984.  .. [3] T. Hastie, R. Tibshirani and J. Friedman. 'Elements of Statistical        Learning', Springer, 2009.  .. [4] L. Breiman, and A. Cutler, 'Random Forests',        http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm  Examples -------- >>> from sklearn.datasets import load_boston >>> from sklearn.cross_validation import cross_val_score >>> from sklearn.tree import DecisionTreeRegressor >>> boston = load_boston() >>> regressor = DecisionTreeRegressor(random_state=0) >>> cross_val_score(regressor, boston.data, boston.target, cv=10) ...                    # doctest: +SKIP ... array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,         0.07..., 0.29..., 0.33..., -1.42..., -1.77...])", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes)"),]),
c("ExtraTreeClassifier(DecisionTreeClassifier)", "/tree/tree.py; An extremely randomized tree classifier.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split among those is chosen. When `max_features` is set 1, this amounts to building a totally random decision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref:`User Guide <tree>`.  See also -------- ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor  References ----------  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, 'Extremely randomized trees',        Machine Learning, 63(1), 3-42, 2006.", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, class_weight)"),]),
c("ExtraTreeRegressor(DecisionTreeRegressor)", "/tree/tree.py; An extremely randomized tree regressor.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split among those is chosen. When `max_features` is set 1, this amounts to building a totally random decision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref:`User Guide <tree>`.  See also -------- ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor  References ----------  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, 'Extremely randomized trees',        Machine Learning, 63(1), 3-42, 2006.", [
d("__init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes)"),]),
c("_BaseScorer()", "/metrics/scorer.py; ", [
d("__init__(self, score_func, sign, kwargs)"),
d("__call__(self, estimator, X, y, sample_weight)"),
d("__repr__(self)"),
d("_factory_args(self)"),]),
c("_PredictScorer(_BaseScorer)", "/metrics/scorer.py; ", [
d("__call__(self, estimator, X, y_true, sample_weight)"),]),
c("_ProbaScorer(_BaseScorer)", "/metrics/scorer.py; ", [
d("__call__(self, clf, X, y, sample_weight)"),
d("_factory_args(self)"),]),
c("_ThresholdScorer(_BaseScorer)", "/metrics/scorer.py; ", [
d("__call__(self, clf, X, y, sample_weight)"),
d("_factory_args(self)"),]),
c("UndefinedMetricWarning(UserWarning)", "/metrics/base.py; ", []),
c("EstimatorWithoutFit(object)", "/metrics/tests/test_score_objects.py; Dummy estimator to test check_scoring", []),
c("EstimatorWithFit(BaseEstimator)", "/metrics/tests/test_score_objects.py; Dummy estimator to test check_scoring", [
d("fit(self, X, y)"),]),
c("EstimatorWithFitAndScore(object)", "/metrics/tests/test_score_objects.py; Dummy estimator to test check_scoring", [
d("fit(self, X, y)"),
d("score(self, X, y)"),]),
c("EstimatorWithFitAndPredict(object)", "/metrics/tests/test_score_objects.py; Dummy estimator to test check_scoring", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("DummyScorer(object)", "/metrics/tests/test_score_objects.py; Dummy scorer that always returns 1.", [
d("__call__(self, est, X, y)"),]),
c("Bunch(dict)", "/datasets/base.py; Container object for datasets  Dictionary-like object that exposes its keys as attributes.  >>> b = Bunch(a=1, b=2) >>> b['b'] 2 >>> b.b 2 >>> b.a = 3 >>> b['a'] 3 >>> b.c = 6 >>> b['c'] 6", [
d("__init__(self)"),
d("__setattr__(self, key, value)"),
d("__getattr__(self, key)"),
d("__getstate__(self)"),]),
c("LinearSVC(BaseEstimator, LinearClassifierMixin, _LearntSelectorMixin, SparseCoefMixin)", "/svm/classes.py; Linear Support Vector Classification.  Similar to SVC with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- C : float, optional (default=1.0)     Penalty parameter C of the error term.  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')     Specifies the loss function. 'hinge' is the standard SVM loss     (used e.g. by the SVC class) while 'squared_hinge' is the     square of the hinge loss.  penalty : string, 'l1' or 'l2' (default='l2')     Specifies the norm used in the penalization. The 'l2'     penalty is the standard used in SVC. The 'l1' leads to `coef_`     vectors that are sparse.  dual : bool, (default=True)     Select the algorithm to either solve the dual or primal     optimization problem. Prefer dual=False when n_samples > n_features.  tol : float, optional (default=1e-4)     Tolerance for stopping criteria.  multi_class: string, 'ovr' or 'crammer_singer' (default='ovr')     Determines the multi-class strategy if `y` contains more than     two classes.     `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`     optimizes a joint objective over all classes.     While `crammer_singer` is interesting from a theoretical perspective     as it is consistent, it is seldom used in practice as it rarely leads     to better accuracy and is more expensive to compute.     If `crammer_singer` is chosen, the options loss, penalty and dual will     be ignored.  fit_intercept : boolean, optional (default=True)     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (i.e. data is expected to be already centered).  intercept_scaling : float, optional (default=1)     When self.fit_intercept is True, instance vector x becomes     [x, self.intercept_scaling],     i.e. a 'synthetic' feature with constant value equals to     intercept_scaling is appended to the instance vector.     The intercept becomes intercept_scaling * synthetic feature weight     Note! the synthetic feature weight is subject to l1/l2 regularization     as all other features.     To lessen the effect of regularization on synthetic feature weight     (and therefore on the intercept) intercept_scaling has to be increased.  class_weight : {dict, 'balanced'}, optional     Set the parameter C of class i to class_weight[i]*C for     SVC. If not given, all classes are supposed to have     weight one.     The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  verbose : int, (default=0)     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in liblinear that, if enabled, may not work     properly in a multithreaded context.  random_state : int seed, RandomState instance, or None (default=None)     The seed of the pseudo random number generator to use when     shuffling the data.  max_iter : int, (default=1000)     The maximum number of iterations to be run.  Attributes ---------- coef_ : array, shape = [n_features] if n_classes == 2         else [n_classes, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is a readonly property derived from `raw_coef_` that     follows the internal memory layout of liblinear.  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]     Constants in decision function.  Notes ----- The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller ``tol`` parameter.  The underlying implementation (liblinear) uses a sparse internal representation for the data that will incur a memory copy.  Predict output may not match that of standalone liblinear in certain cases. See :ref:`differences from liblinear <liblinear_differences>` in the narrative documentation.  **References:** `LIBLINEAR: A Library for Large Linear Classification <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__  See also -------- SVC     Implementation of Support Vector Machine classifier using libsvm:     the kernel can be non-linear but its SMO algorithm does not     scale to large number of samples as LinearSVC does.      Furthermore SVC multi-class mode is implemented using one     vs one scheme while LinearSVC uses one vs the rest. It is     possible to implement one vs the rest with SVC by using the     :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.      Finally SVC can fit dense data without memory copy if the input     is C-contiguous. Sparse data will still incur memory copy though.  sklearn.linear_model.SGDClassifier     SGDClassifier can optimize the same cost function as LinearSVC     by adjusting the penalty and loss parameters. In addition it requires     less memory, allows incremental (online) learning, and implements     various loss functions and regularization regimes.", [
d("__init__(self, penalty, loss, dual, tol, C, multi_class, fit_intercept, intercept_scaling, class_weight, verbose, random_state, max_iter)"),
d("fit(self, X, y)"),]),
c("LinearSVR(LinearModel, RegressorMixin)", "/svm/classes.py; Linear Support Vector Regression.  Similar to SVR with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  This class supports both dense and sparse input.  Read more in the :ref:`User Guide <svm_regression>`.  Parameters ---------- C : float, optional (default=1.0)     Penalty parameter C of the error term. The penalty is a squared     l2 penalty. The bigger this parameter, the less regularization is used.  loss : string, 'epsilon_insensitive' or 'squared_epsilon_insensitive'        (default='epsilon_insensitive')     Specifies the loss function. 'l1' is the epsilon-insensitive loss     (standard SVR) while 'l2' is the squared epsilon-insensitive loss.  epsilon : float, optional (default=0.1)     Epsilon parameter in the epsilon-insensitive loss function. Note     that the value of this parameter depends on the scale of the target     variable y. If unsure, set epsilon=0.  dual : bool, (default=True)     Select the algorithm to either solve the dual or primal     optimization problem. Prefer dual=False when n_samples > n_features.  tol : float, optional (default=1e-4)     Tolerance for stopping criteria.  fit_intercept : boolean, optional (default=True)     Whether to calculate the intercept for this model. If set     to false, no intercept will be used in calculations     (i.e. data is expected to be already centered).  intercept_scaling : float, optional (default=1)     When self.fit_intercept is True, instance vector x becomes     [x, self.intercept_scaling],     i.e. a 'synthetic' feature with constant value equals to     intercept_scaling is appended to the instance vector.     The intercept becomes intercept_scaling * synthetic feature weight     Note! the synthetic feature weight is subject to l1/l2 regularization     as all other features.     To lessen the effect of regularization on synthetic feature weight     (and therefore on the intercept) intercept_scaling has to be increased.  verbose : int, (default=0)     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in liblinear that, if enabled, may not work     properly in a multithreaded context.  random_state : int seed, RandomState instance, or None (default=None)     The seed of the pseudo random number generator to use when     shuffling the data.  max_iter : int, (default=1000)     The maximum number of iterations to be run.  Attributes ---------- coef_ : array, shape = [n_features] if n_classes == 2         else [n_classes, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is a readonly property derived from `raw_coef_` that     follows the internal memory layout of liblinear.  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]     Constants in decision function.   See also -------- LinearSVC     Implementation of Support Vector Machine classifier using the     same library as this class (liblinear).  SVR     Implementation of Support Vector Machine regression using libsvm:     the kernel can be non-linear but its SMO algorithm does not     scale to large number of samples as LinearSVC does.  sklearn.linear_model.SGDRegressor     SGDRegressor can optimize the same cost function as LinearSVR     by adjusting the penalty and loss parameters. In addition it requires     less memory, allows incremental (online) learning, and implements     various loss functions and regularization regimes.", [
d("__init__(self, epsilon, tol, C, loss, fit_intercept, intercept_scaling, dual, verbose, random_state, max_iter)"),
d("fit(self, X, y)"),]),
c("SVC(BaseSVC)", "/svm/classes.py; C-Support Vector Classification.  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.  The multiclass support is handled according to a one-vs-one scheme.  For details on the precise mathematical formulation of the provided kernel functions and how `gamma`, `coef0` and `degree` affect each other, see the corresponding section in the narrative documentation: :ref:`svm_kernels`.  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- C : float, optional (default=1.0)     Penalty parameter C of the error term.  kernel : string, optional (default='rbf')      Specifies the kernel type to be used in the algorithm.      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or      a callable.      If none is given, 'rbf' will be used. If a callable is given it is      used to pre-compute the kernel matrix from data matrices; that matrix      should be an array of shape ``(n_samples, n_samples)``.  degree : int, optional (default=3)     Degree of the polynomial kernel function ('poly').     Ignored by all other kernels.  gamma : float, optional (default='auto')     Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.     If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0)     Independent term in kernel function.     It is only significant in 'poly' and 'sigmoid'.  probability : boolean, optional (default=False)     Whether to enable probability estimates. This must be enabled prior     to calling `fit`, and will slow down that method.  shrinking : boolean, optional (default=True)     Whether to use the shrinking heuristic.  tol : float, optional (default=1e-3)     Tolerance for stopping criterion.  cache_size : float, optional     Specify the size of the kernel cache (in MB).  class_weight : {dict, 'balanced'}, optional     Set the parameter C of class i to class_weight[i]*C for     SVC. If not given, all classes are supposed to have     weight one.     The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``  verbose : bool, default: False     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in libsvm that, if enabled, may not work     properly in a multithreaded context.  max_iter : int, optional (default=-1)     Hard limit on iterations within solver, or -1 for no limit.  decision_function_shape : 'ovo', 'ovr' or None, default=None     Whether to return a one-vs-rest ('ovr') ecision function of shape     (n_samples, n_classes) as all other classifiers, or the original     one-vs-one ('ovo') decision function of libsvm which has shape     (n_samples, n_classes * (n_classes - 1) / 2).     The default of None will currently behave as 'ovo' for backward     compatibility and raise a deprecation warning, but will change 'ovr'     in 0.18.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data for probability estimation.  Attributes ---------- support_ : array-like, shape = [n_SV]     Indices of support vectors.  support_vectors_ : array-like, shape = [n_SV, n_features]     Support vectors.  n_support_ : array-like, dtype=int32, shape = [n_class]     Number of support vectors for each class.  dual_coef_ : array, shape = [n_class-1, n_SV]     Coefficients of the support vector in the decision function.     For multiclass, coefficient for all 1-vs-1 classifiers.     The layout of the coefficients in the multiclass case is somewhat     non-trivial. See the section about multi-class classification in the     SVM section of the User Guide for details.  coef_ : array, shape = [n_class-1, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is a readonly property derived from `dual_coef_` and     `support_vectors_`.  intercept_ : array, shape = [n_class * (n_class-1) / 2]     Constants in decision function.  Examples -------- >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) >>> y = np.array([1, 1, 2, 2]) >>> from sklearn.svm import SVC >>> clf = SVC() >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,     decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',     max_iter=-1, probability=False, random_state=None, shrinking=True,     tol=0.001, verbose=False) >>> print(clf.predict([[-0.8, -1]])) [1]  See also -------- SVR     Support Vector Machine for Regression implemented using libsvm.  LinearSVC     Scalable Linear Support Vector Machine for classification     implemented using liblinear. Check the See also section of     LinearSVC for more comparison element.", [
d("__init__(self, C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state)"),]),
c("NuSVC(BaseSVC)", "/svm/classes.py; Nu-Support Vector Classification.  Similar to SVC but uses a parameter to control the number of support vectors.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- nu : float, optional (default=0.5)     An upper bound on the fraction of training errors and a lower     bound of the fraction of support vectors. Should be in the     interval (0, 1].  kernel : string, optional (default='rbf')      Specifies the kernel type to be used in the algorithm.      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or      a callable.      If none is given, 'rbf' will be used. If a callable is given it is      used to precompute the kernel matrix.  degree : int, optional (default=3)     Degree of the polynomial kernel function ('poly').     Ignored by all other kernels.  gamma : float, optional (default='auto')     Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.     If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0)     Independent term in kernel function.     It is only significant in 'poly' and 'sigmoid'.  probability : boolean, optional (default=False)     Whether to enable probability estimates. This must be enabled prior     to calling `fit`, and will slow down that method.  shrinking : boolean, optional (default=True)     Whether to use the shrinking heuristic.  tol : float, optional (default=1e-3)     Tolerance for stopping criterion.  cache_size : float, optional     Specify the size of the kernel cache (in MB).  class_weight : {dict, 'auto'}, optional     Set the parameter C of class i to class_weight[i]*C for     SVC. If not given, all classes are supposed to have     weight one. The 'auto' mode uses the values of y to     automatically adjust weights inversely proportional to     class frequencies.  verbose : bool, default: False     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in libsvm that, if enabled, may not work     properly in a multithreaded context.  max_iter : int, optional (default=-1)     Hard limit on iterations within solver, or -1 for no limit.  decision_function_shape : 'ovo', 'ovr' or None, default=None     Whether to return a one-vs-rest ('ovr') ecision function of shape     (n_samples, n_classes) as all other classifiers, or the original     one-vs-one ('ovo') decision function of libsvm which has shape     (n_samples, n_classes * (n_classes - 1) / 2).     The default of None will currently behave as 'ovo' for backward     compatibility and raise a deprecation warning, but will change 'ovr'     in 0.18.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data for probability estimation.  Attributes ---------- support_ : array-like, shape = [n_SV]     Indices of support vectors.  support_vectors_ : array-like, shape = [n_SV, n_features]     Support vectors.  n_support_ : array-like, dtype=int32, shape = [n_class]     Number of support vectors for each class.  dual_coef_ : array, shape = [n_class-1, n_SV]     Coefficients of the support vector in the decision function.     For multiclass, coefficient for all 1-vs-1 classifiers.     The layout of the coefficients in the multiclass case is somewhat     non-trivial. See the section about multi-class classification in     the SVM section of the User Guide for details.  coef_ : array, shape = [n_class-1, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is readonly property derived from `dual_coef_` and     `support_vectors_`.  intercept_ : array, shape = [n_class * (n_class-1) / 2]     Constants in decision function.  Examples -------- >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) >>> y = np.array([1, 1, 2, 2]) >>> from sklearn.svm import NuSVC >>> clf = NuSVC() >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE NuSVC(cache_size=200, class_weight=None, coef0=0.0,       decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',       max_iter=-1, nu=0.5, probability=False, random_state=None,       shrinking=True, tol=0.001, verbose=False) >>> print(clf.predict([[-0.8, -1]])) [1]  See also -------- SVC     Support Vector Machine for classification using libsvm.  LinearSVC     Scalable linear Support Vector Machine for classification using     liblinear.", [
d("__init__(self, nu, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state)"),]),
c("SVR(BaseLibSVM, RegressorMixin)", "/svm/classes.py; Epsilon-Support Vector Regression.  The free parameters in the model are C and epsilon.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_regression>`.  Parameters ---------- C : float, optional (default=1.0)     Penalty parameter C of the error term.  epsilon : float, optional (default=0.1)      Epsilon in the epsilon-SVR model. It specifies the epsilon-tube      within which no penalty is associated in the training loss function      with points predicted within a distance epsilon from the actual      value.  kernel : string, optional (default='rbf')      Specifies the kernel type to be used in the algorithm.      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or      a callable.      If none is given, 'rbf' will be used. If a callable is given it is      used to precompute the kernel matrix.  degree : int, optional (default=3)     Degree of the polynomial kernel function ('poly').     Ignored by all other kernels.  gamma : float, optional (default='auto')     Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.     If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0)     Independent term in kernel function.     It is only significant in 'poly' and 'sigmoid'.  shrinking : boolean, optional (default=True)     Whether to use the shrinking heuristic.  tol : float, optional (default=1e-3)     Tolerance for stopping criterion.  cache_size : float, optional     Specify the size of the kernel cache (in MB).  verbose : bool, default: False     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in libsvm that, if enabled, may not work     properly in a multithreaded context.  max_iter : int, optional (default=-1)     Hard limit on iterations within solver, or -1 for no limit.  Attributes ---------- support_ : array-like, shape = [n_SV]     Indices of support vectors.  support_vectors_ : array-like, shape = [nSV, n_features]     Support vectors.  dual_coef_ : array, shape = [1, n_SV]     Coefficients of the support vector in the decision function.  coef_ : array, shape = [1, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is readonly property derived from `dual_coef_` and     `support_vectors_`.  intercept_ : array, shape = [1]     Constants in decision function.  Examples -------- >>> from sklearn.svm import SVR >>> import numpy as np >>> n_samples, n_features = 10, 5 >>> np.random.seed(0) >>> y = np.random.randn(n_samples) >>> X = np.random.randn(n_samples, n_features) >>> clf = SVR(C=1.0, epsilon=0.2) >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',     kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)  See also -------- NuSVR     Support Vector Machine for regression implemented using libsvm     using a parameter to control the number of support vectors.  LinearSVR     Scalable Linear Support Vector Machine for regression     implemented using liblinear.", [
d("__init__(self, kernel, degree, gamma, coef0, tol, C, epsilon, shrinking, cache_size, verbose, max_iter)"),]),
c("NuSVR(BaseLibSVM, RegressorMixin)", "/svm/classes.py; Nu Support Vector Regression.  Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_regression>`.  Parameters ---------- C : float, optional (default=1.0)     Penalty parameter C of the error term.  nu : float, optional     An upper bound on the fraction of training errors and a lower bound of     the fraction of support vectors. Should be in the interval (0, 1].  By     default 0.5 will be taken.  kernel : string, optional (default='rbf')      Specifies the kernel type to be used in the algorithm.      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or      a callable.      If none is given, 'rbf' will be used. If a callable is given it is      used to precompute the kernel matrix.  degree : int, optional (default=3)     Degree of the polynomial kernel function ('poly').     Ignored by all other kernels.  gamma : float, optional (default='auto')     Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.     If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0)     Independent term in kernel function.     It is only significant in 'poly' and 'sigmoid'.  shrinking : boolean, optional (default=True)     Whether to use the shrinking heuristic.  tol : float, optional (default=1e-3)     Tolerance for stopping criterion.  cache_size : float, optional     Specify the size of the kernel cache (in MB).  verbose : bool, default: False     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in libsvm that, if enabled, may not work     properly in a multithreaded context.  max_iter : int, optional (default=-1)     Hard limit on iterations within solver, or -1 for no limit.  Attributes ---------- support_ : array-like, shape = [n_SV]     Indices of support vectors.  support_vectors_ : array-like, shape = [nSV, n_features]     Support vectors.  dual_coef_ : array, shape = [1, n_SV]     Coefficients of the support vector in the decision function.  coef_ : array, shape = [1, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is readonly property derived from `dual_coef_` and     `support_vectors_`.  intercept_ : array, shape = [1]     Constants in decision function.  Examples -------- >>> from sklearn.svm import NuSVR >>> import numpy as np >>> n_samples, n_features = 10, 5 >>> np.random.seed(0) >>> y = np.random.randn(n_samples) >>> X = np.random.randn(n_samples, n_features) >>> clf = NuSVR(C=1.0, nu=0.1) >>> clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto',       kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,       verbose=False)  See also -------- NuSVC     Support Vector Machine for classification implemented with libsvm     with a parameter to control the number of support vectors.  SVR     epsilon Support Vector Machine for regression implemented with libsvm.", [
d("__init__(self, nu, C, kernel, degree, gamma, coef0, shrinking, tol, cache_size, verbose, max_iter)"),]),
c("OneClassSVM(BaseLibSVM)", "/svm/classes.py; Unsupervised Outlier Detection.  Estimate the support of a high-dimensional distribution.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_outlier_detection>`.  Parameters ---------- kernel : string, optional (default='rbf')      Specifies the kernel type to be used in the algorithm.      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or      a callable.      If none is given, 'rbf' will be used. If a callable is given it is      used to precompute the kernel matrix.  nu : float, optional     An upper bound on the fraction of training     errors and a lower bound of the fraction of support     vectors. Should be in the interval (0, 1]. By default 0.5     will be taken.  degree : int, optional (default=3)     Degree of the polynomial kernel function ('poly').     Ignored by all other kernels.  gamma : float, optional (default='auto')     Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.     If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0)     Independent term in kernel function.     It is only significant in 'poly' and 'sigmoid'.  tol : float, optional     Tolerance for stopping criterion.  shrinking : boolean, optional     Whether to use the shrinking heuristic.  cache_size : float, optional     Specify the size of the kernel cache (in MB).  verbose : bool, default: False     Enable verbose output. Note that this setting takes advantage of a     per-process runtime setting in libsvm that, if enabled, may not work     properly in a multithreaded context.  max_iter : int, optional (default=-1)     Hard limit on iterations within solver, or -1 for no limit.  random_state : int seed, RandomState instance, or None (default)     The seed of the pseudo random number generator to use when     shuffling the data for probability estimation.  Attributes ---------- support_ : array-like, shape = [n_SV]     Indices of support vectors.  support_vectors_ : array-like, shape = [nSV, n_features]     Support vectors.  dual_coef_ : array, shape = [n_classes-1, n_SV]     Coefficients of the support vectors in the decision function.  coef_ : array, shape = [n_classes-1, n_features]     Weights assigned to the features (coefficients in the primal     problem). This is only available in the case of a linear kernel.      `coef_` is readonly property derived from `dual_coef_` and     `support_vectors_`  intercept_ : array, shape = [n_classes-1]     Constants in decision function.", [
d("__init__(self, kernel, degree, gamma, coef0, tol, nu, shrinking, cache_size, verbose, max_iter, random_state)"),
d("fit(self, X, y, sample_weight)"),
d("decision_function(self, X)"),]),
c("BaseLibSVM()", "/svm/base.py; Base class for estimators that use libsvm as backing library  This implements support vector machine classification and regression.  Parameter documentation is in the derived `SVC` class.", [
d("__init__(self, impl, kernel, degree, gamma, coef0, tol, C, nu, epsilon, shrinking, probability, cache_size, class_weight, verbose, max_iter, random_state)"),
d("_pairwise(self)"),
d("fit(self, X, y, sample_weight)"),
d("_validate_targets(self, y)"),
d("_warn_from_fit_status(self)"),
d("_dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed)"),
d("_sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed)"),
d("predict(self, X)"),
d("_dense_predict(self, X)"),
d("_sparse_predict(self, X)"),
d("_compute_kernel(self, X)"),
d("decision_function(self, X)"),
d("_decision_function(self, X)"),
d("_dense_decision_function(self, X)"),
d("_sparse_decision_function(self, X)"),
d("_validate_for_predict(self, X)"),
d("coef_(self)"),
d("_get_coef(self)"),]),
c("BaseSVC()", "/svm/base.py; ABC for LibSVM-based classifiers.", [
d("__init__(self, impl, kernel, degree, gamma, coef0, tol, C, nu, shrinking, probability, cache_size, class_weight, verbose, max_iter, decision_function_shape, random_state)"),
d("_validate_targets(self, y)"),
d("decision_function(self, X)"),
d("predict(self, X)"),
d("_check_proba(self)"),
d("predict_proba(self)"),
d("_predict_proba(self, X)"),
d("predict_log_proba(self)"),
d("_predict_log_proba(self, X)"),
d("_dense_predict_proba(self, X)"),
d("_sparse_predict_proba(self, X)"),
d("_get_coef(self)"),]),
c("QuantileEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py; An estimator predicting the alpha-quantile of the training targets.", [
d("__init__(self, alpha)"),
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("MeanEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py; An estimator predicting the mean of the training targets.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("LogOddsEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py; An estimator predicting the log odds ratio.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("ScaledLogOddsEstimator(LogOddsEstimator)", "/ensemble/gradient_boosting.py; Log odds ratio scaled by 0.5 -- for exponential loss. ", []),
c("PriorProbabilityEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py; An estimator predicting the probability of each class in the training data.", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("ZeroEstimator(BaseEstimator)", "/ensemble/gradient_boosting.py; An estimator that simply predicts zero. ", [
d("fit(self, X, y, sample_weight)"),
d("predict(self, X)"),]),
c("LossFunction()", "/ensemble/gradient_boosting.py; Abstract base class for various loss functions.  Attributes ---------- K : int     The number of regression trees to be induced;     1 for regression and binary classification;     ``n_classes`` for multi-class classification.", [
d("__init__(self, n_classes)"),
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, y_pred)"),
d("update_terminal_regions(self, tree, X, y, residual, y_pred, sample_weight, sample_mask, learning_rate, k)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),]),
c("RegressionLossFunction()", "/ensemble/gradient_boosting.py; Base class for regression loss functions. ", [
d("__init__(self, n_classes)"),]),
c("LeastSquaresError(RegressionLossFunction)", "/ensemble/gradient_boosting.py; Loss function for least squares (LS) estimation. Terminal regions need not to be updated for least squares. ", [
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred)"),
d("update_terminal_regions(self, tree, X, y, residual, y_pred, sample_weight, sample_mask, learning_rate, k)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),]),
c("LeastAbsoluteError(RegressionLossFunction)", "/ensemble/gradient_boosting.py; Loss function for least absolute deviation (LAD) regression. ", [
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),]),
c("HuberLossFunction(RegressionLossFunction)", "/ensemble/gradient_boosting.py; Huber loss function for robust regression.  M-Regression proposed in Friedman 2001.  References ---------- J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.", [
d("__init__(self, n_classes, alpha)"),
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred, sample_weight)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),]),
c("QuantileLossFunction(RegressionLossFunction)", "/ensemble/gradient_boosting.py; Loss function for quantile regression.  Quantile regression allows to estimate the percentiles of the conditional distribution of the target.", [
d("__init__(self, n_classes, alpha)"),
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),]),
c("ClassificationLossFunction()", "/ensemble/gradient_boosting.py; Base class for classification loss functions. ", [
d("_score_to_proba(self, score)"),
d("_score_to_decision(self, score)"),]),
c("BinomialDeviance(ClassificationLossFunction)", "/ensemble/gradient_boosting.py; Binomial deviance loss function for binary classification.  Binary classification is a special case; here, we only need to fit one tree instead of ``n_classes`` trees.", [
d("__init__(self, n_classes)"),
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("_score_to_proba(self, score)"),
d("_score_to_decision(self, score)"),]),
c("MultinomialDeviance(ClassificationLossFunction)", "/ensemble/gradient_boosting.py; Multinomial deviance loss function for multi-class classification.  For multi-class classification we need to fit ``n_classes`` trees at each stage.", [
d("__init__(self, n_classes)"),
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred, k)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("_score_to_proba(self, score)"),
d("_score_to_decision(self, score)"),]),
c("ExponentialLoss(ClassificationLossFunction)", "/ensemble/gradient_boosting.py; Exponential loss function for binary classification.  Same loss as AdaBoost.  References ---------- Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007", [
d("__init__(self, n_classes)"),
d("init_estimator(self)"),
d("__call__(self, y, pred, sample_weight)"),
d("negative_gradient(self, y, pred)"),
d("_update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight)"),
d("_score_to_proba(self, score)"),
d("_score_to_decision(self, score)"),]),
c("VerboseReporter(object)", "/ensemble/gradient_boosting.py; Reports verbose output to stdout.  If ``verbose==1`` output is printed once in a while (when iteration mod verbose_mod is zero).; if larger than 1 then output is printed for each update.", [
d("__init__(self, verbose)"),
d("init(self, est, begin_at_stage)"),
d("update(self, j, est)"),]),
c("BaseGradientBoosting()", "/ensemble/gradient_boosting.py; Abstract base class for Gradient Boosting. ", [
d("__init__(self, loss, learning_rate, n_estimators, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, init, subsample, max_features, random_state, alpha, verbose, max_leaf_nodes, warm_start)"),
d("_fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask, criterion, splitter, random_state)"),
d("_check_params(self)"),
d("_init_state(self)"),
d("_clear_state(self)"),
d("_resize_state(self)"),
d("_is_initialized(self)"),
d("fit(self, X, y, sample_weight, monitor)"),
d("_fit_stages(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor)"),
d("_make_estimator(self, append)"),
d("_init_decision_function(self, X)"),
d("_decision_function(self, X)"),
d("decision_function(self, X)"),
d("_staged_decision_function(self, X)"),
d("staged_decision_function(self, X)"),
d("feature_importances_(self)"),
d("_validate_y(self, y)"),]),
c("GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin)", "/ensemble/gradient_boosting.py; Gradient Boosting for classification.  GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'deviance', 'exponential'}, optional (default='deviance')     loss function to be optimized. 'deviance' refers to     deviance (= logistic regression) for classification     with probabilistic outputs. For loss 'exponential' gradient     boosting recovers the AdaBoost algorithm.  learning_rate : float, optional (default=0.1)     learning rate shrinks the contribution of each tree by `learning_rate`.     There is a trade-off between learning_rate and n_estimators.  n_estimators : int (default=100)     The number of boosting stages to perform. Gradient boosting     is fairly robust to over-fitting so a large number usually     results in better performance.  max_depth : integer, optional (default=3)     maximum depth of the individual regression estimators. The maximum     depth limits the number of nodes in the tree. Tune this parameter     for best performance; the best value depends on the interaction     of the input variables.     Ignored if ``max_leaf_nodes`` is not None.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples required to be at a leaf node.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.  subsample : float, optional (default=1.0)     The fraction of samples to be used for fitting the individual base     learners. If smaller than 1.0 this results in Stochastic Gradient     Boosting. `subsample` interacts with the parameter `n_estimators`.     Choosing `subsample < 1.0` leads to a reduction of variance     and an increase in bias.  max_features : int, float, string or None, optional (default=None)     The number of features to consider when looking for the best split:       - If int, then consider `max_features` features at each split.       - If float, then `max_features` is a percentage and         `int(max_features * n_features)` features are considered at each         split.       - If 'auto', then `max_features=sqrt(n_features)`.       - If 'sqrt', then `max_features=sqrt(n_features)`.       - If 'log2', then `max_features=log2(n_features)`.       - If None, then `max_features=n_features`.      Choosing `max_features < n_features` leads to a reduction of variance     and an increase in bias.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.  init : BaseEstimator, None, optional (default=None)     An estimator object that is used to compute the initial     predictions. ``init`` has to provide ``fit`` and ``predict``.     If None it uses ``loss.init_estimator``.  verbose : int, default: 0     Enable verbose output. If 1 then it prints progress and performance     once in a while (the more trees the lower the frequency). If greater     than 1 then it prints progress and performance for every tree.  warm_start : bool, default: False     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just erase the     previous solution.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  Attributes ---------- feature_importances_ : array, shape = [n_features]     The feature importances (the higher, the more important the feature).  oob_improvement_ : array, shape = [n_estimators]     The improvement in loss (= deviance) on the out-of-bag samples     relative to the previous iteration.     ``oob_improvement_[0]`` is the improvement in     loss of the first stage over the ``init`` estimator.  train_score_ : array, shape = [n_estimators]     The i-th score ``train_score_[i]`` is the deviance (= loss) of the     model at iteration ``i`` on the in-bag sample.     If ``subsample == 1`` this is the deviance on the training data.  loss_ : LossFunction     The concrete ``LossFunction`` object.  init : BaseEstimator     The estimator that provides the initial predictions.     Set via the ``init`` argument or ``loss.init_estimator``.  estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, loss_.K]     The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary     classification, otherwise n_classes.   See also -------- sklearn.tree.DecisionTreeClassifier, RandomForestClassifier AdaBoostClassifier  References ---------- J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.  J. Friedman, Stochastic Gradient Boosting, 1999  T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.", [
d("__init__(self, loss, learning_rate, n_estimators, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, init, random_state, max_features, verbose, max_leaf_nodes, warm_start)"),
d("_validate_y(self, y)"),
d("decision_function(self, X)"),
d("staged_decision_function(self, X)"),
d("predict(self, X)"),
d("staged_predict(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),
d("staged_predict_proba(self, X)"),]),
c("GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin)", "/ensemble/gradient_boosting.py; Gradient Boosting for regression.  GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')     loss function to be optimized. 'ls' refers to least squares     regression. 'lad' (least absolute deviation) is a highly robust     loss function solely based on order information of the input     variables. 'huber' is a combination of the two. 'quantile'     allows quantile regression (use `alpha` to specify the quantile).  learning_rate : float, optional (default=0.1)     learning rate shrinks the contribution of each tree by `learning_rate`.     There is a trade-off between learning_rate and n_estimators.  n_estimators : int (default=100)     The number of boosting stages to perform. Gradient boosting     is fairly robust to over-fitting so a large number usually     results in better performance.  max_depth : integer, optional (default=3)     maximum depth of the individual regression estimators. The maximum     depth limits the number of nodes in the tree. Tune this parameter     for best performance; the best value depends on the interaction     of the input variables.     Ignored if ``max_leaf_nodes`` is not None.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples required to be at a leaf node.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.  subsample : float, optional (default=1.0)     The fraction of samples to be used for fitting the individual base     learners. If smaller than 1.0 this results in Stochastic Gradient     Boosting. `subsample` interacts with the parameter `n_estimators`.     Choosing `subsample < 1.0` leads to a reduction of variance     and an increase in bias.  max_features : int, float, string or None, optional (default=None)     The number of features to consider when looking for the best split:       - If int, then consider `max_features` features at each split.       - If float, then `max_features` is a percentage and         `int(max_features * n_features)` features are considered at each         split.       - If 'auto', then `max_features=n_features`.       - If 'sqrt', then `max_features=sqrt(n_features)`.       - If 'log2', then `max_features=log2(n_features)`.       - If None, then `max_features=n_features`.      Choosing `max_features < n_features` leads to a reduction of variance     and an increase in bias.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.  alpha : float (default=0.9)     The alpha-quantile of the huber loss function and the quantile     loss function. Only if ``loss='huber'`` or ``loss='quantile'``.  init : BaseEstimator, None, optional (default=None)     An estimator object that is used to compute the initial     predictions. ``init`` has to provide ``fit`` and ``predict``.     If None it uses ``loss.init_estimator``.  verbose : int, default: 0     Enable verbose output. If 1 then it prints progress and performance     once in a while (the more trees the lower the frequency). If greater     than 1 then it prints progress and performance for every tree.  warm_start : bool, default: False     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just erase the     previous solution.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.   Attributes ---------- feature_importances_ : array, shape = [n_features]     The feature importances (the higher, the more important the feature).  oob_improvement_ : array, shape = [n_estimators]     The improvement in loss (= deviance) on the out-of-bag samples     relative to the previous iteration.     ``oob_improvement_[0]`` is the improvement in     loss of the first stage over the ``init`` estimator.  train_score_ : array, shape = [n_estimators]     The i-th score ``train_score_[i]`` is the deviance (= loss) of the     model at iteration ``i`` on the in-bag sample.     If ``subsample == 1`` this is the deviance on the training data.  loss_ : LossFunction     The concrete ``LossFunction`` object.  `init` : BaseEstimator     The estimator that provides the initial predictions.     Set via the ``init`` argument or ``loss.init_estimator``.  estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]     The collection of fitted sub-estimators.  See also -------- DecisionTreeRegressor, RandomForestRegressor  References ---------- J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.  J. Friedman, Stochastic Gradient Boosting, 1999  T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.", [
d("__init__(self, loss, learning_rate, n_estimators, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, init, random_state, max_features, alpha, verbose, max_leaf_nodes, warm_start)"),
d("predict(self, X)"),
d("staged_predict(self, X)"),]),
c("BaseBagging()", "/ensemble/bagging.py; Base class for Bagging meta-estimator.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, oob_score, warm_start, n_jobs, random_state, verbose)"),
d("fit(self, X, y, sample_weight)"),
d("_set_oob_score(self, X, y)"),
d("_validate_y(self, y)"),]),
c("BaggingClassifier(BaseBagging, ClassifierMixin)", "/ensemble/bagging.py; A Bagging classifier.  A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]_. If samples are drawn with replacement, then the method is known as Bagging [2]_. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]_. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4]_.  Read more in the :ref:`User Guide <bagging>`.  Parameters ---------- base_estimator : object or None, optional (default=None)     The base estimator to fit on random subsets of the dataset.     If None, then the base estimator is a decision tree.  n_estimators : int, optional (default=10)     The number of base estimators in the ensemble.  max_samples : int or float, optional (default=1.0)     The number of samples to draw from X to train each base estimator.         - If int, then draw `max_samples` samples.         - If float, then draw `max_samples * X.shape[0]` samples.  max_features : int or float, optional (default=1.0)     The number of features to draw from X to train each base estimator.         - If int, then draw `max_features` features.         - If float, then draw `max_features * X.shape[1]` features.  bootstrap : boolean, optional (default=True)     Whether samples are drawn with replacement.  bootstrap_features : boolean, optional (default=False)     Whether features are drawn with replacement.  oob_score : bool     Whether to use out-of-bag samples to estimate     the generalization error.  warm_start : bool, optional (default=False)     When set to True, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit     a whole new ensemble.  n_jobs : int, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the building process.  Attributes ---------- base_estimator_ : list of estimators     The base estimator from which the ensemble is grown.  estimators_ : list of estimators     The collection of fitted base estimators.  estimators_samples_ : list of arrays     The subset of drawn samples (i.e., the in-bag samples) for each base     estimator.  estimators_features_ : list of arrays     The subset of drawn features for each base estimator.  classes_ : array of shape = [n_classes]     The classes labels.  n_classes_ : int or list     The number of classes.  oob_score_ : float     Score of the training dataset obtained using an out-of-bag estimate.  oob_decision_function_ : array of shape = [n_samples, n_classes]     Decision function computed with out-of-bag estimate on the training     set. If n_estimators is small it might be possible that a data point     was never left out during the bootstrap. In this case,     `oob_decision_function_` might contain NaN.  References ----------  .. [1] L. Breiman, 'Pasting small votes for classification in large        databases and on-line', Machine Learning, 36(1), 85-103, 1999.  .. [2] L. Breiman, 'Bagging predictors', Machine Learning, 24(2), 123-140,        1996.  .. [3] T. Ho, 'The random subspace method for constructing decision        forests', Pattern Analysis and Machine Intelligence, 20(8), 832-844,        1998.  .. [4] G. Louppe and P. Geurts, 'Ensembles on Random Patches', Machine        Learning and Knowledge Discovery in Databases, 346-361, 2012.", [
d("__init__(self, base_estimator, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, oob_score, warm_start, n_jobs, random_state, verbose)"),
d("_validate_estimator(self)"),
d("_set_oob_score(self, X, y)"),
d("_validate_y(self, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),
d("decision_function(self, X)"),]),
c("BaggingRegressor(BaseBagging, RegressorMixin)", "/ensemble/bagging.py; A Bagging regressor.  A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]_. If samples are drawn with replacement, then the method is known as Bagging [2]_. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]_. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4]_.  Read more in the :ref:`User Guide <bagging>`.  Parameters ---------- base_estimator : object or None, optional (default=None)     The base estimator to fit on random subsets of the dataset.     If None, then the base estimator is a decision tree.  n_estimators : int, optional (default=10)     The number of base estimators in the ensemble.  max_samples : int or float, optional (default=1.0)     The number of samples to draw from X to train each base estimator.         - If int, then draw `max_samples` samples.         - If float, then draw `max_samples * X.shape[0]` samples.  max_features : int or float, optional (default=1.0)     The number of features to draw from X to train each base estimator.         - If int, then draw `max_features` features.         - If float, then draw `max_features * X.shape[1]` features.  bootstrap : boolean, optional (default=True)     Whether samples are drawn with replacement.  bootstrap_features : boolean, optional (default=False)     Whether features are drawn with replacement.  oob_score : bool     Whether to use out-of-bag samples to estimate     the generalization error.  warm_start : bool, optional (default=False)     When set to True, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit     a whole new ensemble.  n_jobs : int, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the building process.  Attributes ---------- estimators_ : list of estimators     The collection of fitted sub-estimators.  estimators_samples_ : list of arrays     The subset of drawn samples (i.e., the in-bag samples) for each base     estimator.  estimators_features_ : list of arrays     The subset of drawn features for each base estimator.  oob_score_ : float     Score of the training dataset obtained using an out-of-bag estimate.  oob_prediction_ : array of shape = [n_samples]     Prediction computed with out-of-bag estimate on the training     set. If n_estimators is small it might be possible that a data point     was never left out during the bootstrap. In this case,     `oob_prediction_` might contain NaN.  References ----------  .. [1] L. Breiman, 'Pasting small votes for classification in large        databases and on-line', Machine Learning, 36(1), 85-103, 1999.  .. [2] L. Breiman, 'Bagging predictors', Machine Learning, 24(2), 123-140,        1996.  .. [3] T. Ho, 'The random subspace method for constructing decision        forests', Pattern Analysis and Machine Intelligence, 20(8), 832-844,        1998.  .. [4] G. Louppe and P. Geurts, 'Ensembles on Random Patches', Machine        Learning and Knowledge Discovery in Databases, 346-361, 2012.", [
d("__init__(self, base_estimator, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, oob_score, warm_start, n_jobs, random_state, verbose)"),
d("predict(self, X)"),
d("_validate_estimator(self)"),
d("_set_oob_score(self, X, y)"),]),
c("BaseForest()", "/ensemble/forest.py; Base class for forests of trees.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),
d("apply(self, X)"),
d("fit(self, X, y, sample_weight)"),
d("_set_oob_score(self, X, y)"),
d("_validate_y_class_weight(self, y)"),
d("_validate_X_predict(self, X)"),
d("feature_importances_(self)"),]),
c("ForestClassifier()", "/ensemble/forest.py; Base class for forest of trees-based classifiers.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),
d("_set_oob_score(self, X, y)"),
d("_validate_y_class_weight(self, y)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("ForestRegressor()", "/ensemble/forest.py; Base class for forest of trees-based regressors.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)"),
d("predict(self, X)"),
d("_set_oob_score(self, X, y)"),]),
c("RandomForestClassifier(ForestClassifier)", "/ensemble/forest.py; A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if `bootstrap=True` (default).  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : integer, optional (default=10)     The number of trees in the forest.  criterion : string, optional (default='gini')     The function to measure the quality of a split. Supported criteria are     'gini' for the Gini impurity and 'entropy' for the information gain.     Note: this parameter is tree-specific.  max_features : int, float, string or None, optional (default='auto')     The number of features to consider when looking for the best split:      - If int, then consider `max_features` features at each split.     - If float, then `max_features` is a percentage and       `int(max_features * n_features)` features are considered at each       split.     - If 'auto', then `max_features=sqrt(n_features)`.     - If 'sqrt', then `max_features=sqrt(n_features)` (same as 'auto').     - If 'log2', then `max_features=log2(n_features)`.     - If None, then `max_features=n_features`.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.     Note: this parameter is tree-specific.  max_depth : integer or None, optional (default=None)     The maximum depth of the tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.     Note: this parameter is tree-specific.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.     Note: this parameter is tree-specific.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples in newly created leaves.  A split is     discarded if after the split, one of the leaves would contain less then     ``min_samples_leaf`` samples.     Note: this parameter is tree-specific.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.     Note: this parameter is tree-specific.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.     Note: this parameter is tree-specific.  bootstrap : boolean, optional (default=True)     Whether bootstrap samples are used when building trees.  oob_score : bool     Whether to use out-of-bag samples to estimate     the generalization error.  n_jobs : integer, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the tree building process.  warm_start : bool, optional (default=False)     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit a whole     new forest.  class_weight : dict, list of dicts, 'balanced', 'balanced_subsample' or None, optional      Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one. For     multi-output problems, a list of dicts can be provided in the same     order as the columns of y.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``      The 'balanced_subsample' mode is the same as 'balanced' except that weights are     computed based on the bootstrap sample for every tree grown.      For multi-output, the weights of each column of y will be multiplied.      Note that these weights will be multiplied with sample_weight (passed     through the fit method) if sample_weight is specified.  Attributes ---------- estimators_ : list of DecisionTreeClassifier     The collection of fitted sub-estimators.  classes_ : array of shape = [n_classes] or a list of such arrays     The classes labels (single output problem), or a list of arrays of     class labels (multi-output problem).  n_classes_ : int or list     The number of classes (single output problem), or a list containing the     number of classes for each output (multi-output problem).  n_features_ : int     The number of features when ``fit`` is performed.  n_outputs_ : int     The number of outputs when ``fit`` is performed.  feature_importances_ : array of shape = [n_features]     The feature importances (the higher, the more important the feature).  oob_score_ : float     Score of the training dataset obtained using an out-of-bag estimate.  oob_decision_function_ : array of shape = [n_samples, n_classes]     Decision function computed with out-of-bag estimate on the training     set. If n_estimators is small it might be possible that a data point     was never left out during the bootstrap. In this case,     `oob_decision_function_` might contain NaN.  References ----------  .. [1] L. Breiman, 'Random Forests', Machine Learning, 45(1), 5-32, 2001.  See also -------- DecisionTreeClassifier, ExtraTreesClassifier", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),]),
c("RandomForestRegressor(ForestRegressor)", "/ensemble/forest.py; A random forest regressor.  A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if `bootstrap=True` (default).  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : integer, optional (default=10)     The number of trees in the forest.  criterion : string, optional (default='mse')     The function to measure the quality of a split. The only supported     criterion is 'mse' for the mean squared error.     Note: this parameter is tree-specific.  max_features : int, float, string or None, optional (default='auto')     The number of features to consider when looking for the best split:      - If int, then consider `max_features` features at each split.     - If float, then `max_features` is a percentage and       `int(max_features * n_features)` features are considered at each       split.     - If 'auto', then `max_features=n_features`.     - If 'sqrt', then `max_features=sqrt(n_features)`.     - If 'log2', then `max_features=log2(n_features)`.     - If None, then `max_features=n_features`.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.     Note: this parameter is tree-specific.  max_depth : integer or None, optional (default=None)     The maximum depth of the tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.     Note: this parameter is tree-specific.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.     Note: this parameter is tree-specific.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples in newly created leaves.  A split is     discarded if after the split, one of the leaves would contain less then     ``min_samples_leaf`` samples.     Note: this parameter is tree-specific.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.     Note: this parameter is tree-specific.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.     Note: this parameter is tree-specific.  bootstrap : boolean, optional (default=True)     Whether bootstrap samples are used when building trees.  oob_score : bool     whether to use out-of-bag samples to estimate     the generalization error.  n_jobs : integer, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the tree building process.  warm_start : bool, optional (default=False)     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit a whole     new forest.  Attributes ---------- estimators_ : list of DecisionTreeRegressor     The collection of fitted sub-estimators.  feature_importances_ : array of shape = [n_features]     The feature importances (the higher, the more important the feature).  n_features_ : int     The number of features when ``fit`` is performed.  n_outputs_ : int     The number of outputs when ``fit`` is performed.  oob_score_ : float     Score of the training dataset obtained using an out-of-bag estimate.  oob_prediction_ : array of shape = [n_samples]     Prediction computed with out-of-bag estimate on the training set.  References ----------  .. [1] L. Breiman, 'Random Forests', Machine Learning, 45(1), 5-32, 2001.  See also -------- DecisionTreeRegressor, ExtraTreesRegressor", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)"),]),
c("ExtraTreesClassifier(ForestClassifier)", "/ensemble/forest.py; An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : integer, optional (default=10)     The number of trees in the forest.  criterion : string, optional (default='gini')     The function to measure the quality of a split. Supported criteria are     'gini' for the Gini impurity and 'entropy' for the information gain.     Note: this parameter is tree-specific.  max_features : int, float, string or None, optional (default='auto')     The number of features to consider when looking for the best split:      - If int, then consider `max_features` features at each split.     - If float, then `max_features` is a percentage and       `int(max_features * n_features)` features are considered at each       split.     - If 'auto', then `max_features=sqrt(n_features)`.     - If 'sqrt', then `max_features=sqrt(n_features)`.     - If 'log2', then `max_features=log2(n_features)`.     - If None, then `max_features=n_features`.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.     Note: this parameter is tree-specific.  max_depth : integer or None, optional (default=None)     The maximum depth of the tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.     Note: this parameter is tree-specific.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.     Note: this parameter is tree-specific.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples in newly created leaves.  A split is     discarded if after the split, one of the leaves would contain less then     ``min_samples_leaf`` samples.     Note: this parameter is tree-specific.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.     Note: this parameter is tree-specific.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.     Note: this parameter is tree-specific.  bootstrap : boolean, optional (default=False)     Whether bootstrap samples are used when building trees.  oob_score : bool     Whether to use out-of-bag samples to estimate     the generalization error.  n_jobs : integer, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the tree building process.  warm_start : bool, optional (default=False)     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit a whole     new forest.  class_weight : dict, list of dicts, 'balanced', 'balanced_subsample' or None, optional      Weights associated with classes in the form ``{class_label: weight}``.     If not given, all classes are supposed to have weight one. For     multi-output problems, a list of dicts can be provided in the same     order as the columns of y.      The 'balanced' mode uses the values of y to automatically adjust     weights inversely proportional to class frequencies in the input data     as ``n_samples / (n_classes * np.bincount(y))``      The 'balanced_subsample' mode is the same as 'balanced' except that weights are     computed based on the bootstrap sample for every tree grown.      For multi-output, the weights of each column of y will be multiplied.      Note that these weights will be multiplied with sample_weight (passed     through the fit method) if sample_weight is specified.  Attributes ---------- estimators_ : list of DecisionTreeClassifier     The collection of fitted sub-estimators.  classes_ : array of shape = [n_classes] or a list of such arrays     The classes labels (single output problem), or a list of arrays of     class labels (multi-output problem).  n_classes_ : int or list     The number of classes (single output problem), or a list containing the     number of classes for each output (multi-output problem).  feature_importances_ : array of shape = [n_features]     The feature importances (the higher, the more important the feature).  n_features_ : int     The number of features when ``fit`` is performed.  n_outputs_ : int     The number of outputs when ``fit`` is performed.  oob_score_ : float     Score of the training dataset obtained using an out-of-bag estimate.  oob_decision_function_ : array of shape = [n_samples, n_classes]     Decision function computed with out-of-bag estimate on the training     set. If n_estimators is small it might be possible that a data point     was never left out during the bootstrap. In this case,     `oob_decision_function_` might contain NaN.  References ----------  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, 'Extremely randomized trees',        Machine Learning, 63(1), 3-42, 2006.  See also -------- sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble. RandomForestClassifier : Ensemble Classifier based on trees with optimal     splits.", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)"),]),
c("ExtraTreesRegressor(ForestRegressor)", "/ensemble/forest.py; An extra-trees regressor.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : integer, optional (default=10)     The number of trees in the forest.  criterion : string, optional (default='mse')     The function to measure the quality of a split. The only supported     criterion is 'mse' for the mean squared error.     Note: this parameter is tree-specific.  max_features : int, float, string or None, optional (default='auto')     The number of features to consider when looking for the best split:      - If int, then consider `max_features` features at each split.     - If float, then `max_features` is a percentage and       `int(max_features * n_features)` features are considered at each       split.     - If 'auto', then `max_features=n_features`.     - If 'sqrt', then `max_features=sqrt(n_features)`.     - If 'log2', then `max_features=log2(n_features)`.     - If None, then `max_features=n_features`.      Note: the search for a split does not stop until at least one     valid partition of the node samples is found, even if it requires to     effectively inspect more than ``max_features`` features.     Note: this parameter is tree-specific.  max_depth : integer or None, optional (default=None)     The maximum depth of the tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.     Note: this parameter is tree-specific.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.     Note: this parameter is tree-specific.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples in newly created leaves.  A split is     discarded if after the split, one of the leaves would contain less then     ``min_samples_leaf`` samples.     Note: this parameter is tree-specific.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.     Note: this parameter is tree-specific.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.     Note: this parameter is tree-specific.  bootstrap : boolean, optional (default=False)     Whether bootstrap samples are used when building trees.     Note: this parameter is tree-specific.  oob_score : bool     Whether to use out-of-bag samples to estimate     the generalization error.  n_jobs : integer, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the tree building process.  warm_start : bool, optional (default=False)     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit a whole     new forest.  Attributes ---------- estimators_ : list of DecisionTreeRegressor     The collection of fitted sub-estimators.  feature_importances_ : array of shape = [n_features]     The feature importances (the higher, the more important the feature).  n_features_ : int     The number of features.  n_outputs_ : int     The number of outputs.  oob_score_ : float     Score of the training dataset obtained using an out-of-bag estimate.  oob_prediction_ : array of shape = [n_samples]     Prediction computed with out-of-bag estimate on the training set.  References ----------  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, 'Extremely randomized trees',        Machine Learning, 63(1), 3-42, 2006.  See also -------- sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble. RandomForestRegressor: Ensemble regressor using trees with optimal splits.", [
d("__init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start)"),]),
c("RandomTreesEmbedding(BaseForest)", "/ensemble/forest.py; An ensemble of totally random trees.  An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.  The dimensionality of the resulting representation is ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``, the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.  Read more in the :ref:`User Guide <random_trees_embedding>`.  Parameters ---------- n_estimators : int     Number of trees in the forest.  max_depth : int     The maximum depth of each tree. If None, then nodes are expanded until     all leaves are pure or until all leaves contain less than     min_samples_split samples.     Ignored if ``max_leaf_nodes`` is not None.  min_samples_split : integer, optional (default=2)     The minimum number of samples required to split an internal node.  min_samples_leaf : integer, optional (default=1)     The minimum number of samples in newly created leaves.  A split is     discarded if after the split, one of the leaves would contain less then     ``min_samples_leaf`` samples.  min_weight_fraction_leaf : float, optional (default=0.)     The minimum weighted fraction of the input samples required to be at a     leaf node.  max_leaf_nodes : int or None, optional (default=None)     Grow trees with ``max_leaf_nodes`` in best-first fashion.     Best nodes are defined as relative reduction in impurity.     If None then unlimited number of leaf nodes.     If not None then ``max_depth`` will be ignored.  sparse_output : bool, optional (default=True)     Whether or not to return a sparse CSR matrix, as default behavior,     or to return a dense array compatible with dense pipeline operators.  n_jobs : integer, optional (default=1)     The number of jobs to run in parallel for both `fit` and `predict`.     If -1, then the number of jobs is set to the number of cores.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  verbose : int, optional (default=0)     Controls the verbosity of the tree building process.  warm_start : bool, optional (default=False)     When set to ``True``, reuse the solution of the previous call to fit     and add more estimators to the ensemble, otherwise, just fit a whole     new forest.  Attributes ---------- estimators_ : list of DecisionTreeClassifier     The collection of fitted sub-estimators.  References ---------- .. [1] P. Geurts, D. Ernst., and L. Wehenkel, 'Extremely randomized trees',        Machine Learning, 63(1), 3-42, 2006. .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  'Fast discriminative        visual codebooks using randomized clustering forests'        NIPS 2007", [
d("__init__(self, n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, sparse_output, n_jobs, random_state, verbose, warm_start)"),
d("_set_oob_score(self, X, y)"),
d("fit(self, X, y, sample_weight)"),
d("fit_transform(self, X, y, sample_weight)"),
d("transform(self, X)"),]),
c("BaseEnsemble(BaseEstimator, MetaEstimatorMixin)", "/ensemble/base.py; Base class for all ensemble classes.  Warning: This class should not be used directly. Use derived classes instead.  Parameters ---------- base_estimator : object, optional (default=None)     The base estimator from which the ensemble is built.  n_estimators : integer     The number of estimators in the ensemble.  estimator_params : list of strings     The list of attributes to use as parameters when instantiating a     new base estimator. If none are given, default parameters are used.  Attributes ---------- base_estimator_ : list of estimators     The base estimator from which the ensemble is grown.  estimators_ : list of estimators     The collection of fitted base estimators.", [
d("__init__(self, base_estimator, n_estimators, estimator_params)"),
d("_validate_estimator(self, default)"),
d("_make_estimator(self, append)"),
d("__len__(self)"),
d("__getitem__(self, index)"),
d("__iter__(self)"),]),
c("VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin)", "/ensemble/voting_classifier.py; Soft Voting/Majority Rule classifier for unfitted estimators.  Read more in the :ref:`User Guide <voting_classifier>`.  Parameters ---------- estimators : list of (string, estimator) tuples     Invoking the `fit` method on the `VotingClassifier` will fit clones     of those original estimators that will be stored in the class attribute     `self.estimators_`.  voting : str, {'hard', 'soft'} (default='hard')     If 'hard', uses predicted class labels for majority rule voting.     Else if 'soft', predicts the class label based on the argmax of     the sums of the predicted probalities, which is recommended for     an ensemble of well-calibrated classifiers.  weights : array-like, shape = [n_classifiers], optional (default=`None`)     Sequence of weights (`float` or `int`) to weight the occurances of     predicted class labels (`hard` voting) or class probabilities     before averaging (`soft` voting). Uses uniform weights if `None`.  Attributes ---------- classes_ : array-like, shape = [n_predictions]  Examples -------- >>> import numpy as np >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.ensemble import RandomForestClassifier >>> clf1 = LogisticRegression(random_state=1) >>> clf2 = RandomForestClassifier(random_state=1) >>> clf3 = GaussianNB() >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> eclf1 = VotingClassifier(estimators=[ ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard') >>> eclf1 = eclf1.fit(X, y) >>> print(eclf1.predict(X)) [1 1 1 2 2 2] >>> eclf2 = VotingClassifier(estimators=[ ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], ...         voting='soft') >>> eclf2 = eclf2.fit(X, y) >>> print(eclf2.predict(X)) [1 1 1 2 2 2] >>> eclf3 = VotingClassifier(estimators=[ ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], ...        voting='soft', weights=[2,1,1]) >>> eclf3 = eclf3.fit(X, y) >>> print(eclf3.predict(X)) [1 1 1 2 2 2] >>>", [
d("__init__(self, estimators, voting, weights)"),
d("fit(self, X, y)"),
d("predict(self, X)"),
d("_collect_probas(self, X)"),
d("_predict_proba(self, X)"),
d("predict_proba(self)"),
d("transform(self, X)"),
d("get_params(self, deep)"),
d("_predict(self, X)"),]),
c("BaseWeightBoosting()", "/ensemble/weight_boosting.py; Base class for AdaBoost estimators.  Warning: This class should not be used directly. Use derived classes instead.", [
d("__init__(self, base_estimator, n_estimators, estimator_params, learning_rate, random_state)"),
d("fit(self, X, y, sample_weight)"),
d("_boost(self, iboost, X, y, sample_weight)"),
d("staged_score(self, X, y, sample_weight)"),
d("feature_importances_(self)"),
d("_check_sample_weight(self)"),
d("_validate_X_predict(self, X)"),]),
c("AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin)", "/ensemble/weight_boosting.py; An AdaBoost classifier.  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.  This class implements the algorithm known as AdaBoost-SAMME [2].  Read more in the :ref:`User Guide <adaboost>`.  Parameters ---------- base_estimator : object, optional (default=DecisionTreeClassifier)     The base estimator from which the boosted ensemble is built.     Support for sample weighting is required, as well as proper `classes_`     and `n_classes_` attributes.  n_estimators : integer, optional (default=50)     The maximum number of estimators at which boosting is terminated.     In case of perfect fit, the learning procedure is stopped early.  learning_rate : float, optional (default=1.)     Learning rate shrinks the contribution of each classifier by     ``learning_rate``. There is a trade-off between ``learning_rate`` and     ``n_estimators``.  algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')     If 'SAMME.R' then use the SAMME.R real boosting algorithm.     ``base_estimator`` must support calculation of class probabilities.     If 'SAMME' then use the SAMME discrete boosting algorithm.     The SAMME.R algorithm typically converges faster than SAMME,     achieving a lower test error with fewer boosting iterations.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  Attributes ---------- estimators_ : list of classifiers     The collection of fitted sub-estimators.  classes_ : array of shape = [n_classes]     The classes labels.  n_classes_ : int     The number of classes.  estimator_weights_ : array of floats     Weights for each estimator in the boosted ensemble.  estimator_errors_ : array of floats     Classification error for each estimator in the boosted     ensemble.  feature_importances_ : array of shape = [n_features]     The feature importances if supported by the ``base_estimator``.  See also -------- AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier  References ---------- .. [1] Y. Freund, R. Schapire, 'A Decision-Theoretic Generalization of        on-Line Learning and an Application to Boosting', 1995.  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, 'Multi-class AdaBoost', 2009.", [
d("__init__(self, base_estimator, n_estimators, learning_rate, algorithm, random_state)"),
d("fit(self, X, y, sample_weight)"),
d("_validate_estimator(self)"),
d("_boost(self, iboost, X, y, sample_weight)"),
d("_boost_real(self, iboost, X, y, sample_weight)"),
d("_boost_discrete(self, iboost, X, y, sample_weight)"),
d("predict(self, X)"),
d("staged_predict(self, X)"),
d("decision_function(self, X)"),
d("staged_decision_function(self, X)"),
d("predict_proba(self, X)"),
d("staged_predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("AdaBoostRegressor(BaseWeightBoosting, RegressorMixin)", "/ensemble/weight_boosting.py; An AdaBoost regressor.  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.  This class implements the algorithm known as AdaBoost.R2 [2].  Read more in the :ref:`User Guide <adaboost>`.  Parameters ---------- base_estimator : object, optional (default=DecisionTreeRegressor)     The base estimator from which the boosted ensemble is built.     Support for sample weighting is required.  n_estimators : integer, optional (default=50)     The maximum number of estimators at which boosting is terminated.     In case of perfect fit, the learning procedure is stopped early.  learning_rate : float, optional (default=1.)     Learning rate shrinks the contribution of each regressor by     ``learning_rate``. There is a trade-off between ``learning_rate`` and     ``n_estimators``.  loss : {'linear', 'square', 'exponential'}, optional (default='linear')     The loss function to use when updating the weights after each     boosting iteration.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  Attributes ---------- estimators_ : list of classifiers     The collection of fitted sub-estimators.  estimator_weights_ : array of floats     Weights for each estimator in the boosted ensemble.  estimator_errors_ : array of floats     Regression error for each estimator in the boosted ensemble.  feature_importances_ : array of shape = [n_features]     The feature importances if supported by the ``base_estimator``.  See also -------- AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor  References ---------- .. [1] Y. Freund, R. Schapire, 'A Decision-Theoretic Generalization of        on-Line Learning and an Application to Boosting', 1995.  .. [2] H. Drucker, 'Improving Regressors using Boosting Techniques', 1997.", [
d("__init__(self, base_estimator, n_estimators, learning_rate, loss, random_state)"),
d("fit(self, X, y, sample_weight)"),
d("_validate_estimator(self)"),
d("_boost(self, iboost, X, y, sample_weight)"),
d("_get_median_predict(self, X, limit)"),
d("predict(self, X)"),
d("staged_predict(self, X)"),]),
c("DummyZeroEstimator(BaseEstimator)", "/ensemble/tests/test_bagging.py; ", [
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin)", "/feature_selection/rfe.py; Feature ranking with recursive feature elimination.  Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.  Read more in the :ref:`User Guide <rfe>`.  Parameters ---------- estimator : object     A supervised learning estimator with a `fit` method that updates a     `coef_` attribute that holds the fitted parameters. Important features     must correspond to high absolute values in the `coef_` array.      For instance, this is the case for most supervised learning     algorithms such as Support Vector Classifiers and Generalized     Linear Models from the `svm` and `linear_model` modules.  n_features_to_select : int or None (default=None)     The number of features to select. If `None`, half of the features     are selected.  step : int or float, optional (default=1)     If greater than or equal to 1, then `step` corresponds to the (integer)     number of features to remove at each iteration.     If within (0.0, 1.0), then `step` corresponds to the percentage     (rounded down) of features to remove at each iteration.  estimator_params : dict     Parameters for the external estimator.     This attribute is deprecated as of version 0.16 and will be removed in     0.18. Use estimator initialisation or set_params method instead.  verbose : int, default=0     Controls verbosity of output.  Attributes ---------- n_features_ : int     The number of selected features.  support_ : array of shape [n_features]     The mask of selected features.  ranking_ : array of shape [n_features]     The feature ranking, such that ``ranking_[i]`` corresponds to the     ranking position of the i-th feature. Selected (i.e., estimated     best) features are assigned rank 1.  estimator_ : object     The external estimator fit on the reduced dataset.  Examples -------- The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.  >>> from sklearn.datasets import make_friedman1 >>> from sklearn.feature_selection import RFE >>> from sklearn.svm import SVR >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0) >>> estimator = SVR(kernel='linear') >>> selector = RFE(estimator, 5, step=1) >>> selector = selector.fit(X, y) >>> selector.support_ # doctest: +NORMALIZE_WHITESPACE array([ True,  True,  True,  True,  True,         False, False, False, False, False], dtype=bool) >>> selector.ranking_ array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])  References ----------  .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., 'Gene selection        for cancer classification using support vector machines',        Mach. Learn., 46(1-3), 389--422, 2002.", [
d("__init__(self, estimator, n_features_to_select, step, estimator_params, verbose)"),
d("_estimator_type(self)"),
d("fit(self, X, y)"),
d("_fit(self, X, y, step_score)"),
d("predict(self, X)"),
d("score(self, X, y)"),
d("_get_support_mask(self)"),
d("decision_function(self, X)"),
d("predict_proba(self, X)"),
d("predict_log_proba(self, X)"),]),
c("RFECV(RFE, MetaEstimatorMixin)", "/feature_selection/rfe.py; Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.  Read more in the :ref:`User Guide <rfe>`.  Parameters ---------- estimator : object     A supervised learning estimator with a `fit` method that updates a     `coef_` attribute that holds the fitted parameters. Important features     must correspond to high absolute values in the `coef_` array.      For instance, this is the case for most supervised learning     algorithms such as Support Vector Classifiers and Generalized     Linear Models from the `svm` and `linear_model` modules.  step : int or float, optional (default=1)     If greater than or equal to 1, then `step` corresponds to the (integer)     number of features to remove at each iteration.     If within (0.0, 1.0), then `step` corresponds to the percentage     (rounded down) of features to remove at each iteration.  cv : int or cross-validation generator, optional (default=None)     If int, it is the number of folds.     If None, 3-fold cross-validation is performed by default.     Specific cross-validation objects can also be passed, see     `sklearn.cross_validation module` for details.  scoring : string, callable or None, optional, default: None     A string (see model evaluation documentation) or     a scorer callable object / function with signature     ``scorer(estimator, X, y)``.  estimator_params : dict     Parameters for the external estimator.     This attribute is deprecated as of version 0.16 and will be removed in     0.18. Use estimator initialisation or set_params method instead.  verbose : int, default=0     Controls verbosity of output.  Attributes ---------- n_features_ : int     The number of selected features with cross-validation.  support_ : array of shape [n_features]     The mask of selected features.  ranking_ : array of shape [n_features]     The feature ranking, such that `ranking_[i]`     corresponds to the ranking     position of the i-th feature.     Selected (i.e., estimated best)     features are assigned rank 1.  grid_scores_ : array of shape [n_subsets_of_features]     The cross-validation scores such that     ``grid_scores_[i]`` corresponds to     the CV score of the i-th subset of features.  estimator_ : object     The external estimator fit on the reduced dataset.  Notes ----- The size of ``grid_scores_`` is equal to ceil((n_features - 1) / step) + 1, where step is the number of features removed at each iteration.  Examples -------- The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.  >>> from sklearn.datasets import make_friedman1 >>> from sklearn.feature_selection import RFECV >>> from sklearn.svm import SVR >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0) >>> estimator = SVR(kernel='linear') >>> selector = RFECV(estimator, step=1, cv=5) >>> selector = selector.fit(X, y) >>> selector.support_ # doctest: +NORMALIZE_WHITESPACE array([ True,  True,  True,  True,  True,         False, False, False, False, False], dtype=bool) >>> selector.ranking_ array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])  References ----------  .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., 'Gene selection        for cancer classification using support vector machines',        Mach. Learn., 46(1-3), 389--422, 2002.", [
d("__init__(self, estimator, step, cv, scoring, estimator_params, verbose)"),
d("fit(self, X, y)"),]),
c("_LearntSelectorMixin(TransformerMixin)", "/feature_selection/from_model.py; Transformer mixin selecting features based on importance weights.  This implementation can be mixin on any estimator that exposes a ``feature_importances_`` or ``coef_`` attribute to evaluate the relative importance of individual features for feature selection.", [
d("transform(self, X, threshold)"),]),
c("VarianceThreshold(BaseEstimator, SelectorMixin)", "/feature_selection/variance_threshold.py; Feature selector that removes all low-variance features.  This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.  Read more in the :ref:`User Guide <variance_threshold>`.  Parameters ---------- threshold : float, optional     Features with a training-set variance lower than this threshold will     be removed. The default is to keep all features with non-zero variance,     i.e. remove the features that have the same value in all samples.  Attributes ---------- variances_ : array, shape (n_features,)     Variances of individual features.  Examples -------- The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold::      >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]     >>> selector = VarianceThreshold()     >>> selector.fit_transform(X)     array([[2, 0],            [1, 4],            [1, 1]])", [
d("__init__(self, threshold)"),
d("fit(self, X, y)"),
d("_get_support_mask(self)"),]),
c("SelectorMixin()", "/feature_selection/base.py; Tranformer mixin that performs feature selection given a support mask  This mixin provides a feature selector implementation with `transform` and `inverse_transform` functionality given an implementation of `_get_support_mask`.", [
d("get_support(self, indices)"),
d("_get_support_mask(self)"),
d("transform(self, X)"),
d("inverse_transform(self, X)"),]),
c("_BaseFilter(BaseEstimator, SelectorMixin)", "/feature_selection/univariate_selection.py; Initialize the univariate feature selection.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).", [
d("__init__(self, score_func)"),
d("fit(self, X, y)"),
d("_check_params(self, X, y)"),]),
c("SelectPercentile(_BaseFilter)", "/feature_selection/univariate_selection.py; Select features according to a percentile of the highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  percentile : int, optional, default=10     Percent of features to keep.  Attributes ---------- scores_ : array-like, shape=(n_features,)     Scores of features.  pvalues_ : array-like, shape=(n_features,)     p-values of feature scores.  Notes ----- Ties between features with equal scores will be broken in an unspecified way.  See also -------- f_classif: ANOVA F-value between labe/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectKBest: Select features based on the k highest scores. SelectFpr: Select features based on a false positive rate test. SelectFdr: Select features based on an estimated false discovery rate. SelectFwe: Select features based on family-wise error rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.", [
d("__init__(self, score_func, percentile)"),
d("_check_params(self, X, y)"),
d("_get_support_mask(self)"),]),
c("SelectKBest(_BaseFilter)", "/feature_selection/univariate_selection.py; Select features according to the k highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  k : int or 'all', optional, default=10     Number of top features to select.     The 'all' option bypasses selection, for use in a parameter search.  Attributes ---------- scores_ : array-like, shape=(n_features,)     Scores of features.  pvalues_ : array-like, shape=(n_features,)     p-values of feature scores.  Notes ----- Ties between features with equal scores will be broken in an unspecified way.  See also -------- f_classif: ANOVA F-value between labe/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectPercentile: Select features based on percentile of the highest scores. SelectFpr: Select features based on a false positive rate test. SelectFdr: Select features based on an estimated false discovery rate. SelectFwe: Select features based on family-wise error rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.", [
d("__init__(self, score_func, k)"),
d("_check_params(self, X, y)"),
d("_get_support_mask(self)"),]),
c("SelectFpr(_BaseFilter)", "/feature_selection/univariate_selection.py; Filter: Select the pvalues below alpha based on a FPR test.  FPR test stands for False Positive Rate test. It controls the total amount of false detections.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  alpha : float, optional     The highest p-value for features to be kept.  Attributes ---------- scores_ : array-like, shape=(n_features,)     Scores of features.  pvalues_ : array-like, shape=(n_features,)     p-values of feature scores.  See also -------- f_classif: ANOVA F-value between labe/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFdr: Select features based on an estimated false discovery rate. SelectFwe: Select features based on family-wise error rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.", [
d("__init__(self, score_func, alpha)"),
d("_get_support_mask(self)"),]),
c("SelectFdr(_BaseFilter)", "/feature_selection/univariate_selection.py; Filter: Select the p-values for an estimated false discovery rate  This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound on the expected false discovery rate.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  alpha : float, optional     The highest uncorrected p-value for features to keep.   Attributes ---------- scores_ : array-like, shape=(n_features,)     Scores of features.  pvalues_ : array-like, shape=(n_features,)     p-values of feature scores.  References ---------- http://en.wikipedia.org/wiki/False_discovery_rate  See also -------- f_classif: ANOVA F-value between labe/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFpr: Select features based on a false positive rate test. SelectFwe: Select features based on family-wise error rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.", [
d("__init__(self, score_func, alpha)"),
d("_get_support_mask(self)"),]),
c("SelectFwe(_BaseFilter)", "/feature_selection/univariate_selection.py; Filter: Select the p-values corresponding to Family-wise error rate  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  alpha : float, optional     The highest uncorrected p-value for features to keep.  Attributes ---------- scores_ : array-like, shape=(n_features,)     Scores of features.  pvalues_ : array-like, shape=(n_features,)     p-values of feature scores.  See also -------- f_classif: ANOVA F-value between labe/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFpr: Select features based on a false positive rate test. SelectFdr: Select features based on an estimated false discovery rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.", [
d("__init__(self, score_func, alpha)"),
d("_get_support_mask(self)"),]),
c("GenericUnivariateSelect(_BaseFilter)", "/feature_selection/univariate_selection.py; Univariate feature selector with configurable strategy.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable     Function taking two arrays X and y, and returning a pair of arrays     (scores, pvalues).  mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}     Feature selection mode.  param : float or int depending on the feature selection mode     Parameter of the corresponding mode.  Attributes ---------- scores_ : array-like, shape=(n_features,)     Scores of features.  pvalues_ : array-like, shape=(n_features,)     p-values of feature scores.  See also -------- f_classif: ANOVA F-value between labe/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFpr: Select features based on a false positive rate test. SelectFdr: Select features based on an estimated false discovery rate. SelectFwe: Select features based on family-wise error rate.", [
d("__init__(self, score_func, mode, param)"),
d("_make_selector(self)"),
d("_check_params(self, X, y)"),
d("_get_support_mask(self)"),]),
c("StepSelector(SelectorMixin, BaseEstimator)", "/feature_selection/tests/test_base.py; Retain every `step` features (beginning with 0)", [
d("__init__(self, step)"),
d("fit(self, X, y)"),
d("_get_support_mask(self)"),]),
c("MockClassifier(object)", "/feature_selection/tests/test_rfe.py; Dummy classifier to test recursive feature ellimination", [
d("__init__(self, foo_param)"),
d("fit(self, X, Y)"),
d("predict(self, T)"),
d("score(self, X, Y)"),
d("get_params(self, deep)"),
d("set_params(self)"),]),
c("KNeighborsClassifier(NeighborsBase, KNeighborsMixin, SupervisedIntegerMixin, ClassifierMixin)", "/neighbors/classification.py; Classifier implementing the k-nearest neighbors vote.  Read more in the :ref:`User Guide <classification>`.  Parameters ---------- n_neighbors : int, optional (default = 5)     Number of neighbors to use by default for :meth:`k_neighbors` queries.  weights : str or callable     weight function used in prediction.  Possible values:      - 'uniform' : uniform weights.  All points in each neighborhood       are weighted equally.     - 'distance' : weight points by the inverse of their distance.       in this case, closer neighbors of a query point will have a       greater influence than neighbors which are further away.     - [callable] : a user-defined function which accepts an       array of distances, and returns an array of the same shape       containing the weights.      Uniform weights are used by default.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional     Algorithm used to compute the nearest neighbors:      - 'ball_tree' will use :class:`BallTree`     - 'kd_tree' will use :class:`KDTree`     - 'brute' will use a brute-force search.     - 'auto' will attempt to decide the most appropriate algorithm       based on the values passed to :meth:`fit` method.      Note: fitting on sparse input will override the setting of     this parameter, using brute force.  leaf_size : int, optional (default = 30)     Leaf size passed to BallTree or KDTree.  This can affect the     speed of the construction and query, as well as the memory     required to store the tree.  The optimal value depends on the     nature of the problem.  metric : string or DistanceMetric object (default = 'minkowski')     the distance metric to use for the tree.  The default metric is     minkowski, and with p=2 is equivalent to the standard Euclidean     metric. See the documentation of the DistanceMetric class for a     list of available metrics.  p : integer, optional (default = 2)     Power parameter for the Minkowski metric. When p = 1, this is     equivalent to using manhattan_distance (l1), and euclidean_distance     (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  metric_params: dict, optional (default = None)     additional keyword arguments for the metric function.  Examples -------- >>> X = [[0], [1], [2], [3]] >>> y = [0, 0, 1, 1] >>> from sklearn.neighbors import KNeighborsClassifier >>> neigh = KNeighborsClassifier(n_neighbors=3) >>> neigh.fit(X, y) # doctest: +ELLIPSIS KNeighborsClassifier(...) >>> print(neigh.predict([[1.1]])) [0] >>> print(neigh.predict_proba([[0.9]])) [[ 0.66666667  0.33333333]]  See also -------- RadiusNeighborsClassifier KNeighborsRegressor RadiusNeighborsRegressor NearestNeighbors  Notes ----- See :ref:`Nearest Neighbors <neighbors>` in the online documentation for a discussion of the choice of ``algorithm`` and ``leaf_size``.  .. warning::     Regarding the Nearest Neighbors algorithms, if it is found that two    neighbors, neighbor `k+1` and `k`, have identical distances but    but different labels, the results will depend on the ordering of the    training data.  http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", [
d("__init__(self, n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params)"),
d("predict(self, X)"),
d("predict_proba(self, X)"),]),
c("RadiusNeighborsClassifier(NeighborsBase, RadiusNeighborsMixin, SupervisedIntegerMixin, ClassifierMixin)", "/neighbors/classification.py; Classifier implementing a vote among neighbors within a given radius  Read more in the :ref:`User Guide <classification>`.  Parameters ---------- radius : float, optional (default = 1.0)     Range of parameter space to use by default for :meth`radius_neighbors`     queries.  weights : str or callable     weight function used in prediction.  Possible values:      - 'uniform' : uniform weights.  All points in each neighborhood       are weighted equally.     - 'distance' : weight points by the inverse of their distance.       in this case, closer neighbors of a query point will have a       greater influence than neighbors which are further away.     - [callable] : a user-defined function which accepts an       array of distances, and returns an array of the same shape       containing the weights.      Uniform weights are used by default.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional     Algorithm used to compute the nearest neighbors:      - 'ball_tree' will use :class:`BallTree`     - 'kd_tree' will use :class:`KDtree`     - 'brute' will use a brute-force search.     - 'auto' will attempt to decide the most appropriate algorithm       based on the values passed to :meth:`fit` method.      Note: fitting on sparse input will override the setting of     this parameter, using brute force.  leaf_size : int, optional (default = 30)     Leaf size passed to BallTree or KDTree.  This can affect the     speed of the construction and query, as well as the memory     required to store the tree.  The optimal value depends on the     nature of the problem.  metric : string or DistanceMetric object (default='minkowski')     the distance metric to use for the tree.  The default metric is     minkowski, and with p=2 is equivalent to the standard Euclidean     metric. See the documentation of the DistanceMetric class for a     list of available metrics.  p : integer, optional (default = 2)     Power parameter for the Minkowski metric. When p = 1, this is     equivalent to using manhattan_distance (l1), and euclidean_distance     (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  outlier_label : int, optional (default = None)     Label, which is given for outlier samples (samples with no     neighbors on given radius).     If set to None, ValueError is raised, when outlier is detected.  metric_params: dict, optional (default = None)     additional keyword arguments for the metric function.  Examples -------- >>> X = [[0], [1], [2], [3]] >>> y = [0, 0, 1, 1] >>> from sklearn.neighbors import RadiusNeighborsClassifier >>> neigh = RadiusNeighborsClassifier(radius=1.0) >>> neigh.fit(X, y) # doctest: +ELLIPSIS RadiusNeighborsClassifier(...) >>> print(neigh.predict([[1.5]])) [0]  See also -------- KNeighborsClassifier RadiusNeighborsRegressor KNeighborsRegressor NearestNeighbors  Notes ----- See :ref:`Nearest Neighbors <neighbors>` in the online documentation for a discussion of the choice of ``algorithm`` and ``leaf_size``.  http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", [
d("__init__(self, radius, weights, algorithm, leaf_size, p, metric, outlier_label, metric_params)"),
d("predict(self, X)"),]),
c("KNeighborsRegressor(NeighborsBase, KNeighborsMixin, SupervisedFloatMixin, RegressorMixin)", "/neighbors/regression.py; Regression based on k-nearest neighbors.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.  Parameters ---------- n_neighbors : int, optional (default = 5)     Number of neighbors to use by default for :meth:`k_neighbors` queries.  weights : str or callable     weight function used in prediction.  Possible values:      - 'uniform' : uniform weights.  All points in each neighborhood       are weighted equally.     - 'distance' : weight points by the inverse of their distance.       in this case, closer neighbors of a query point will have a       greater influence than neighbors which are further away.     - [callable] : a user-defined function which accepts an       array of distances, and returns an array of the same shape       containing the weights.      Uniform weights are used by default.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional     Algorithm used to compute the nearest neighbors:      - 'ball_tree' will use :class:`BallTree`     - 'kd_tree' will use :class:`KDtree`     - 'brute' will use a brute-force search.     - 'auto' will attempt to decide the most appropriate algorithm       based on the values passed to :meth:`fit` method.      Note: fitting on sparse input will override the setting of     this parameter, using brute force.  leaf_size : int, optional (default = 30)     Leaf size passed to BallTree or KDTree.  This can affect the     speed of the construction and query, as well as the memory     required to store the tree.  The optimal value depends on the     nature of the problem.  metric : string or DistanceMetric object (default='minkowski')     the distance metric to use for the tree.  The default metric is     minkowski, and with p=2 is equivalent to the standard Euclidean     metric. See the documentation of the DistanceMetric class for a     list of available metrics.  p : integer, optional (default = 2)     Power parameter for the Minkowski metric. When p = 1, this is     equivalent to using manhattan_distance (l1), and euclidean_distance     (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  metric_params: dict, optional (default = None)     additional keyword arguments for the metric function.  Examples -------- >>> X = [[0], [1], [2], [3]] >>> y = [0, 0, 1, 1] >>> from sklearn.neighbors import KNeighborsRegressor >>> neigh = KNeighborsRegressor(n_neighbors=2) >>> neigh.fit(X, y) # doctest: +ELLIPSIS KNeighborsRegressor(...) >>> print(neigh.predict([[1.5]])) [ 0.5]  See also -------- NearestNeighbors RadiusNeighborsRegressor KNeighborsClassifier RadiusNeighborsClassifier  Notes ----- See :ref:`Nearest Neighbors <neighbors>` in the online documentation for a discussion of the choice of ``algorithm`` and ``leaf_size``.  .. warning::     Regarding the Nearest Neighbors algorithms, if it is found that two    neighbors, neighbor `k+1` and `k`, have identical distances but    but different labels, the results will depend on the ordering of the    training data.  http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", [
d("__init__(self, n_neighbors, weights, algorithm, leaf_size, p, metric, metric_params)"),
d("predict(self, X)"),]),
c("RadiusNeighborsRegressor(NeighborsBase, RadiusNeighborsMixin, SupervisedFloatMixin, RegressorMixin)", "/neighbors/regression.py; Regression based on neighbors within a fixed radius.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.  Parameters ---------- radius : float, optional (default = 1.0)     Range of parameter space to use by default for :meth`radius_neighbors`     queries.  weights : str or callable     weight function used in prediction.  Possible values:      - 'uniform' : uniform weights.  All points in each neighborhood       are weighted equally.     - 'distance' : weight points by the inverse of their distance.       in this case, closer neighbors of a query point will have a       greater influence than neighbors which are further away.     - [callable] : a user-defined function which accepts an       array of distances, and returns an array of the same shape       containing the weights.      Uniform weights are used by default.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional     Algorithm used to compute the nearest neighbors:      - 'ball_tree' will use :class:`BallTree`     - 'kd_tree' will use :class:`KDtree`     - 'brute' will use a brute-force search.     - 'auto' will attempt to decide the most appropriate algorithm       based on the values passed to :meth:`fit` method.      Note: fitting on sparse input will override the setting of     this parameter, using brute force.  leaf_size : int, optional (default = 30)     Leaf size passed to BallTree or KDTree.  This can affect the     speed of the construction and query, as well as the memory     required to store the tree.  The optimal value depends on the     nature of the problem.  metric : string or DistanceMetric object (default='minkowski')     the distance metric to use for the tree.  The default metric is     minkowski, and with p=2 is equivalent to the standard Euclidean     metric. See the documentation of the DistanceMetric class for a     list of available metrics.  p : integer, optional (default = 2)     Power parameter for the Minkowski metric. When p = 1, this is     equivalent to using manhattan_distance (l1), and euclidean_distance     (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  metric_params: dict, optional (default = None)     additional keyword arguments for the metric function.  Examples -------- >>> X = [[0], [1], [2], [3]] >>> y = [0, 0, 1, 1] >>> from sklearn.neighbors import RadiusNeighborsRegressor >>> neigh = RadiusNeighborsRegressor(radius=1.0) >>> neigh.fit(X, y) # doctest: +ELLIPSIS RadiusNeighborsRegressor(...) >>> print(neigh.predict([[1.5]])) [ 0.5]  See also -------- NearestNeighbors KNeighborsRegressor KNeighborsClassifier RadiusNeighborsClassifier  Notes ----- See :ref:`Nearest Neighbors <neighbors>` in the online documentation for a discussion of the choice of ``algorithm`` and ``leaf_size``.  http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", [
d("__init__(self, radius, weights, algorithm, leaf_size, p, metric, metric_params)"),
d("predict(self, X)"),]),
c("NearestNeighbors(NeighborsBase, KNeighborsMixin, RadiusNeighborsMixin, UnsupervisedMixin)", "/neighbors/unsupervised.py; Unsupervised learner for implementing neighbor searches.  Read more in the :ref:`User Guide <unsupervised_neighbors>`.  Parameters ---------- n_neighbors : int, optional (default = 5)     Number of neighbors to use by default for :meth:`k_neighbors` queries.  radius : float, optional (default = 1.0)     Range of parameter space to use by default for :meth`radius_neighbors`     queries.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional     Algorithm used to compute the nearest neighbors:      - 'ball_tree' will use :class:`BallTree`     - 'kd_tree' will use :class:`KDtree`     - 'brute' will use a brute-force search.     - 'auto' will attempt to decide the most appropriate algorithm       based on the values passed to :meth:`fit` method.      Note: fitting on sparse input will override the setting of     this parameter, using brute force.  leaf_size : int, optional (default = 30)     Leaf size passed to BallTree or KDTree.  This can affect the     speed of the construction and query, as well as the memory     required to store the tree.  The optimal value depends on the     nature of the problem.  p: integer, optional (default = 2)     Parameter for the Minkowski metric from     sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is     equivalent to using manhattan_distance (l1), and euclidean_distance     (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  metric : string or callable, default 'minkowski'     metric to use for distance computation. Any metric from scikit-learn     or scipy.spatial.distance can be used.      If metric is a callable function, it is called on each     pair of instances (rows) and the resulting value recorded. The callable     should take two arrays as input and return one value indicating the     distance between them. This works for Scipy's metrics, but is less     efficient than passing the metric name as a string.      Distance matrices are not supported.      Valid values for metric are:      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',       'manhattan']      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',       'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',       'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',       'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',       'sqeuclidean', 'yule']      See the documentation for scipy.spatial.distance for details on these     metrics.  metric_params: dict, optional (default = None)     additional keyword arguments for the metric function.  Examples --------   >>> import numpy as np   >>> from sklearn.neighbors import NearestNeighbors   >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]    >>> neigh = NearestNeighbors(2, 0.4)   >>> neigh.fit(samples)  #doctest: +ELLIPSIS   NearestNeighbors(...)    >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)   ... #doctest: +ELLIPSIS   array([[2, 0]]...)    >>> rng = neigh.radius_neighbors([0, 0, 1.3], 0.4, return_distance=False)   >>> np.asarray(rng[0][0])   array(2)  See also -------- KNeighborsClassifier RadiusNeighborsClassifier KNeighborsRegressor RadiusNeighborsRegressor BallTree  Notes ----- See :ref:`Nearest Neighbors <neighbors>` in the online documentation for a discussion of the choice of ``algorithm`` and ``leaf_size``.  http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", [
d("__init__(self, n_neighbors, radius, algorithm, leaf_size, metric, p, metric_params)"),]),
c("NearestCentroid(BaseEstimator, ClassifierMixin)", "/neighbors/nearest_centroid.py; Nearest centroid classifier.  Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.  Read more in the :ref:`User Guide <nearest_centroid_classifier>`.  Parameters ---------- metric: string, or callable     The metric to use when calculating distance between instances in a     feature array. If metric is a string or callable, it must be one of     the options allowed by metrics.pairwise.pairwise_distances for its     metric parameter.     The centroids for the samples corresponding to each class is the point     from which the sum of the distances (according to the metric) of all     samples that belong to that particular class are minimized.     If the 'manhattan' metric is provided, this centroid is the median and     for all other metrics, the centroid is now set to be the mean.  shrink_threshold : float, optional (default = None)     Threshold for shrinking centroids to remove features.  Attributes ---------- centroids_ : array-like, shape = [n_classes, n_features]     Centroid of each class  Examples -------- >>> from sklearn.neighbors.nearest_centroid import NearestCentroid >>> import numpy as np >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> clf = NearestCentroid() >>> clf.fit(X, y) NearestCentroid(metric='euclidean', shrink_threshold=None) >>> print(clf.predict([[-0.8, -1]])) [1]  See also -------- sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier  Notes ----- When used for text classification with tf-idf vectors, this classifier is also known as the Rocchio classifier.  References ---------- Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America, 99(10), 6567-6572. The National Academy of Sciences.", [
d("__init__(self, metric, shrink_threshold)"),
d("fit(self, X, y)"),
d("predict(self, X)"),]),
c("NeighborsWarning(UserWarning)", "/neighbors/base.py; ", []),
c("NeighborsBase()", "/neighbors/base.py; Base class for nearest neighbors estimators.", [
d("__init__(self)"),
d("_init_params(self, n_neighbors, radius, algorithm, leaf_size, metric, p, metric_params)"),
d("_fit(self, X)"),]),
c("KNeighborsMixin(object)", "/neighbors/base.py; Mixin for k-neighbors searches", [
d("kneighbors(self, X, n_neighbors, return_distance)"),
d("kneighbors_graph(self, X, n_neighbors, mode)"),]),
c("RadiusNeighborsMixin(object)", "/neighbors/base.py; Mixin for radius-based neighbors searches", [
d("radius_neighbors(self, X, radius, return_distance)"),
d("radius_neighbors_graph(self, X, radius, mode)"),]),
c("SupervisedFloatMixin(object)", "/neighbors/base.py; ", [
d("fit(self, X, y)"),]),
c("SupervisedIntegerMixin(object)", "/neighbors/base.py; ", [
d("fit(self, X, y)"),]),
c("UnsupervisedMixin(object)", "/neighbors/base.py; ", [
d("fit(self, X, y)"),]),
c("ProjectionToHashMixin(object)", "/neighbors/approximate.py; Turn a transformed real-valued array into a hash", [
d("_to_hash(projected)"),
d("fit_transform(self, X, y)"),
d("transform(self, X, y)"),]),
c("GaussianRandomProjectionHash(ProjectionToHashMixin, GaussianRandomProjection)", "/neighbors/approximate.py; Use GaussianRandomProjection to produce a cosine LSH fingerprint", [
d("__init__(self, n_components, random_state)"),]),
c("LSHForest(BaseEstimator, KNeighborsMixin, RadiusNeighborsMixin)", "/neighbors/approximate.py; Performs approximate nearest neighbor search using LSH forest.  LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.  The cosine distance is defined as ``1 - cosine_similarity``: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.  Read more in the :ref:`User Guide <approximate_nearest_neighbors>`.  Parameters ----------  n_estimators : int (default = 10)     Number of trees in the LSH Forest.  min_hash_match : int (default = 4)     lowest hash length to be searched when candidate selection is     performed for nearest neighbors.  n_candidates : int (default = 10)     Minimum number of candidates evaluated per estimator, assuming enough     items meet the `min_hash_match` constraint.  n_neighbors : int (default = 5)     Number of neighbors to be returned from query function when     it is not provided to the :meth:`kneighbors` method.  radius : float, optinal (default = 1.0)     Radius from the data point to its neighbors. This is the parameter     space to use by default for the :meth`radius_neighbors` queries.  radius_cutoff_ratio : float, optional (default = 0.9)     A value ranges from 0 to 1. Radius neighbors will be searched until     the ratio between total neighbors within the radius and the total     candidates becomes less than this value unless it is terminated by     hash length reaching `min_hash_match`.  random_state : int, RandomState instance or None, optional (default=None)     If int, random_state is the seed used by the random number generator;     If RandomState instance, random_state is the random number generator;     If None, the random number generator is the RandomState instance used     by `np.random`.  Attributes ----------  hash_functions_ : list of GaussianRandomProjectionHash objects     Hash function g(p,x) for a tree is an array of 32 randomly generated     float arrays with the same dimenstion as the data set. This array is     stored in GaussianRandomProjectionHash object and can be obtained     from ``components_`` attribute.  trees_ : array, shape (n_estimators, n_samples)     Each tree (corresponding to a hash function) contains an array of     sorted hashed values. The array representation may change in future     versions.  original_indices_ : array, shape (n_estimators, n_samples)     Original indices of sorted hashed values in the fitted index.  References ----------  .. [1] M. Bawa, T. Condie and P. Ganesan, 'LSH Forest: Self-Tuning        Indexes for Similarity Search', WWW '05 Proceedings of the        14th international conference on World Wide Web,  651-660,        2005.  Examples --------   >>> from sklearn.neighbors import LSHForest    >>> X_train = [[5, 5, 2], [21, 5, 5], [1, 1, 1], [8, 9, 1], [6, 10, 2]]   >>> X_test = [[9, 1, 6], [3, 1, 10], [7, 10, 3]]   >>> lshf = LSHForest()   >>> lshf.fit(X_train)  # doctest: +NORMALIZE_WHITESPACE   LSHForest(min_hash_match=4, n_candidates=50, n_estimators=10,             n_neighbors=5, radius=1.0, radius_cutoff_ratio=0.9,             random_state=None)   >>> distances, indices = lshf.kneighbors(X_test, n_neighbors=2)   >>> distances                                        # doctest: +ELLIPSIS   array([[ 0.069...,  0.149...],          [ 0.229...,  0.481...],          [ 0.004...,  0.014...]])   >>> indices   array([[1, 2],          [2, 0],          [4, 0]])", [
d("__init__(self, n_estimators, radius, n_candidates, n_neighbors, min_hash_match, radius_cutoff_ratio, random_state)"),
d("_compute_distances(self, query, candidates)"),
d("_generate_masks(self)"),
d("_get_candidates(self, query, max_depth, bin_queries, n_neighbors)"),
d("_get_radius_neighbors(self, query, max_depth, bin_queries, radius)"),
d("fit(self, X, y)"),
d("_query(self, X)"),
d("kneighbors(self, X, n_neighbors, return_distance)"),
d("radius_neighbors(self, X, radius, return_distance)"),
d("partial_fit(self, X, y)"),]),
c("KernelDensity(BaseEstimator)", "/neighbors/kde.py; Kernel Density Estimation  Read more in the :ref:`User Guide <kernel_density>`.  Parameters ---------- bandwidth : float     The bandwidth of the kernel.  algorithm : string     The tree algorithm to use.  Valid options are     ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'.  kernel : string     The kernel to use.  Valid kernels are     ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']     Default is 'gaussian'.  metric : string     The distance metric to use.  Note that not all metrics are     valid with all algorithms.  Refer to the documentation of     :class:`BallTree` and :class:`KDTree` for a description of     available algorithms.  Note that the normalization of the density     output is correct only for the Euclidean distance metric. Default     is 'euclidean'.  atol : float     The desired absolute tolerance of the result.  A larger tolerance will     generally lead to faster execution. Default is 0.  rtol : float     The desired relative tolerance of the result.  A larger tolerance will     generally lead to faster execution.  Default is 1E-8.  breadth_first : boolean     If true (default), use a breadth-first approach to the problem.     Otherwise use a depth-first approach.  leaf_size : int     Specify the leaf size of the underlying tree.  See :class:`BallTree`     or :class:`KDTree` for details.  Default is 40.  metric_params : dict     Additional parameters to be passed to the tree for use with the     metric.  For more information, see the documentation of     :class:`BallTree` or :class:`KDTree`.", [
d("__init__(self, bandwidth, algorithm, kernel, metric, atol, rtol, breadth_first, leaf_size, metric_params)"),
d("_choose_algorithm(self, algorithm, metric)"),
d("fit(self, X, y)"),
d("score_samples(self, X)"),
d("score(self, X, y)"),
d("sample(self, n_samples, random_state)"),]),
c("TestMetrics()", "/neighbors/tests/test_dist_metrics.py; ", [
d("__init__(self, n1, n2, d, zero_frac, rseed, dtype)"),
d("test_cdist(self)"),
d("check_cdist(self, metric, kwargs, D_true)"),
d("check_cdist_bool(self, metric, D_true)"),
d("test_pdist(self)"),
d("check_pdist(self, metric, kwargs, D_true)"),
d("check_pdist_bool(self, metric, D_true)"),]),
c("_LazyDescr(object)", "/externals/six.py; ", [
d("__init__(self, name)"),
d("__get__(self, obj, tp)"),]),
c("MovedModule(_LazyDescr)", "/externals/six.py; ", [
d("__init__(self, name, old, new)"),
d("_resolve(self)"),]),
c("MovedAttribute(_LazyDescr)", "/externals/six.py; ", [
d("__init__(self, name, old_mod, new_mod, old_attr, new_attr)"),
d("_resolve(self)"),]),
c("_MovedItems()", "/externals/six.py; Lazy loading of moved objects", []),
c("Module_six_moves_urllib_parse()", "/externals/six.py; Lazy loading of moved objects in six.moves.urllib_parse", []),
c("Module_six_moves_urllib_error()", "/externals/six.py; Lazy loading of moved objects in six.moves.urllib_error", []),
c("Module_six_moves_urllib_request()", "/externals/six.py; Lazy loading of moved objects in six.moves.urllib_request", []),
c("Module_six_moves_urllib_response()", "/externals/six.py; Lazy loading of moved objects in six.moves.urllib_response", []),
c("Module_six_moves_urllib_robotparser()", "/externals/six.py; Lazy loading of moved objects in six.moves.urllib_robotparser", []),
c("Module_six_moves_urllib()", "/externals/six.py; Create a six.moves.urllib namespace that resembles the Python 3 namespace", []),
c("ArrayMemmapReducer(object)", "/externals/joblib/pool.py; Reducer callable to dump large arrays to memmap files.  Parameters ---------- max_nbytes: int     Threshold to trigger memmaping of large arrays to files created     a folder. temp_folder: str     Path of a folder where files for backing memmaped arrays are created. mmap_mode: 'r', 'r+' or 'c'     Mode for the created memmap datastructure. See the documentation of     numpy.memmap for more details. Note: 'w+' is coerced to 'r+'     automatically to avoid zeroing the data on unpickling. verbose: int, optional, 0 by default     If verbose > 0, memmap creations are logged.     If verbose > 1, both memmap creations, reuse and array pickling are     logged. context_id: int, optional, None by default     Set to a value identifying a call context to spare costly hashing of     the content of the input arrays when it is safe to assume that each     array will not be mutated by the parent process for the duration of the     dispatch process. This is the case when using the high level Parallel     API. It might not be the case when using the MemmapingPool API     directly. prewarm: bool, optional, False by default.     Force a read on newly memmaped array to make sure that OS pre-cache it     memory. This can be useful to avoid concurrent disk access when the     same data array is passed to different worker processes.", [
d("__init__(self, max_nbytes, temp_folder, mmap_mode, verbose, context_id, prewarm)"),
d("__call__(self, a)"),]),
c("CustomizablePickler(Pickler)", "/externals/joblib/pool.py; Pickler that accepts custom reducers.  HIGHEST_PROTOCOL is selected by default as this pickler is used to pickle ephemeral datastructures for interprocess communication hence no backward compatibility is required.  `reducers` is expected expected to be a dictionary with key/values being `(type, callable)` pairs where `callable` is a function that give an instance of `type` will return a tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the pickled `tuple_of_objects` as would return a `__reduce__` method. See the standard library documentation on pickling for more details.", [
d("__init__(self, writer, reducers, protocol)"),
d("register(self, type, reduce_func)"),]),
c("CustomizablePicklingQueue(object)", "/externals/joblib/pool.py; Locked Pipe implementation that uses a customizable pickler.  This class is an alternative to the multiprocessing implementation of SimpleQueue in order to make it possible to pass custom pickling reducers, for instance to avoid memory copy when passing memmory mapped datastructures.  `reducers` is expected expected to be a dictionary with key/values being `(type, callable)` pairs where `callable` is a function that give an instance of `type` will return a tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the pickled `tuple_of_objects` as would return a `__reduce__` method. See the standard library documentation on pickling for more details.", [
d("__init__(self, context, reducers)"),
d("__getstate__(self)"),
d("__setstate__(self, state)"),
d("empty(self)"),
d("_make_methods(self)"),]),
c("PicklingPool(Pool)", "/externals/joblib/pool.py; Pool implementation with customizable pickling reducers.  This is useful to control how data is shipped between processes and makes it possible to use shared memory without useless copies induces by the default pickling methods of the original objects passed as arguments to dispatch.  `forward_reducers` and `backward_reducers` are expected to be dictionaries with key/values being `(type, callable)` pairs where `callable` is a function that give an instance of `type` will return a tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the pickled `tuple_of_objects` as would return a `__reduce__` method. See the standard library documentation on pickling for more details.", [
d("__init__(self, processes, forward_reducers, backward_reducers)"),
d("_setup_queues(self)"),]),
c("MemmapingPool(PicklingPool)", "/externals/joblib/pool.py; Process pool that shares large arrays to avoid memory copy.  This drop-in replacement for `multiprocessing.pool.Pool` makes it possible to work efficiently with shared memory in a numpy context.  Existing instances of numpy.memmap are preserved: the child suprocesses will have access to the same shared memory in the original mode except for the 'w+' mode that is automatically transformed as 'r+' to avoid zeroing the original data upon instantiation.  Furthermore large arrays from the parent process are automatically dumped to a temporary folder on the filesystem such as child processes to access their content via memmaping (file system backed shared memory).  Note: it is important to call the terminate method to collect the temporary folder used by the pool.  Parameters ---------- processes: int, optional     Number of worker processes running concurrently in the pool. initializer: callable, optional     Callable executed on worker process creation. initargs: tuple, optional     Arguments passed to the initializer callable. temp_folder: str, optional     Folder to be used by the pool for memmaping large arrays     for sharing memory with worker processes. If None, this will try in     order:     - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,     - /dev/shm if the folder exists and is writable: this is a RAMdisk       filesystem available by default on modern Linux distributions,     - the default system temporary folder that can be overridden       with TMP, TMPDIR or TEMP environment variables, typically /tmp       under Unix operating systems. max_nbytes int or None, optional, 1e6 by default     Threshold on the size of arrays passed to the workers that     triggers automated memmory mapping in temp_folder.     Use None to disable memmaping of large arrays. forward_reducers: dictionary, optional     Reducers used to pickle objects passed from master to worker     processes: see below. backward_reducers: dictionary, optional     Reducers used to pickle return values from workers back to the     master process. verbose: int, optional     Make it possible to monitor how the communication of numpy arrays     with the subprocess is handled (pickling or memmaping) context_id: int, optional, None by default     Set to a value identifying a call context to spare costly hashing of     the content of the input arrays when it is safe to assume that each     array will not be mutated by the parent process for the duration of the     dispatch process. This is the case when using the high level Parallel     API. prewarm: bool or str, optional, 'auto' by default.     If True, force a read on newly memmaped array to make sure that OS pre-     cache it in memory. This can be useful to avoid concurrent disk access     when the same data array is passed to different worker processes.     If 'auto' (by default), prewarm is set to True, unless the Linux shared     memory partition /dev/shm is available and used as temp_folder.  `forward_reducers` and `backward_reducers` are expected to be dictionaries with key/values being `(type, callable)` pairs where `callable` is a function that give an instance of `type` will return a tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the pickled `tuple_of_objects` as would return a `__reduce__` method. See the standard library documentation on pickling for more details.", [
d("__init__(self, processes, temp_folder, max_nbytes, mmap_mode, forward_reducers, backward_reducers, verbose, context_id, prewarm)"),
d("terminate(self)"),]),
c("JoblibException(Exception)", "/externals/joblib/my_exceptions.py; A simple exception with an error message that you can get to.", [
d("__init__(self)"),
d("__reduce__(self)"),
d("__repr__(self)"),]),
c("TransportableException(JoblibException)", "/externals/joblib/my_exceptions.py; An exception containing all the info to wrap an original exception and recreate it.", [
d("__init__(self, message, etype)"),
d("__reduce__(self)"),]),
c("JobLibCollisionWarning(UserWarning)", "/externals/joblib/memory.py; Warn that there might be a collision between names of functions.     ", []),
c("MemorizedResult(Logger)", "/externals/joblib/memory.py; Object representing a cached value.  Attributes ---------- cachedir: string     path to root of joblib cache  func: function or string     function whose output is cached. The string case is intended only for     instanciation based on the output of repr() on another instance.     (namely eval(repr(memorized_instance)) works).  argument_hash: string     hash of the function arguments  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}     The memmapping mode used when loading from cache numpy arrays. See     numpy.load for the meaning of the different values.  verbose: int     verbosity level (0 means no message)  timestamp, metadata: string     for internal use only", [
d("__init__(self, cachedir, func, argument_hash, mmap_mode, verbose, timestamp, metadata)"),
d("get(self)"),
d("clear(self)"),
d("__repr__(self)"),
d("__reduce__(self)"),]),
c("NotMemorizedResult(object)", "/externals/joblib/memory.py; Class representing an arbitrary value.  This class is a replacement for MemorizedResult when there is no cache.", [
d("__init__(self, value)"),
d("get(self)"),
d("clear(self)"),
d("__repr__(self)"),
d("__getstate__(self)"),
d("__setstate__(self, state)"),]),
c("NotMemorizedFunc(object)", "/externals/joblib/memory.py; No-op object decorating a function.  This class replaces MemorizedFunc when there is no cache. It provides an identical API but does not write anything on disk.  Attributes ---------- func: callable     Original undecorated function.", [
d("__init__(self, func)"),
d("__call__(self)"),
d("call_and_shelve(self)"),
d("__reduce__(self)"),
d("__repr__(self)"),
d("clear(self, warn)"),]),
c("MemorizedFunc(Logger)", "/externals/joblib/memory.py; Callable object decorating a function for caching its return value each time it is called.  All values are cached on the filesystem, in a deep directory structure. Methods are provided to inspect the cache or clean it.  Attributes ---------- func: callable     The original, undecorated, function.  cachedir: string     Path to the base cache directory of the memory context.  ignore: list or None     List of variable names to ignore when choosing whether to     recompute.  mmap_mode: {None, 'r+', 'r', 'w+', 'c'}     The memmapping mode used when loading from cache     numpy arrays. See numpy.load for the meaning of the different     values.  compress: boolean, or integer     Whether to zip the stored data on disk. If an integer is     given, it should be between 1 and 9, and sets the amount     of compression. Note that compressed arrays cannot be     read by memmapping.  verbose: int, optional     The verbosity flag, controls messages that are issued as     the function is evaluated.", [
d("__init__(self, func, cachedir, ignore, mmap_mode, compress, verbose, timestamp)"),
d("_cached_call(self, args, kwargs)"),
d("call_and_shelve(self)"),
d("__call__(self)"),
d("__reduce__(self)"),
d("format_signature(self)"),
d("format_call(self)"),
d("_get_argument_hash(self)"),
d("_get_output_dir(self)"),
d("_get_func_dir(self, mkdir)"),
d("_hash_func(self)"),
d("_write_func_code(self, filename, func_code, first_line)"),
d("_check_previous_func_code(self, stacklevel)"),
d("clear(self, warn)"),
d("call(self)"),
d("_persist_output(self, output, dir)"),
d("_persist_input(self, output_dir, duration, args, kwargs, this_duration_limit)"),
d("load_output(self, output_dir)"),
d("__repr__(self)"),]),
c("Memory(Logger)", "/externals/joblib/memory.py; A context object for caching a function's return value each time it is called with the same input arguments.  All values are cached on the filesystem, in a deep directory structure.  see :ref:`memory_reference`", [
d("__init__(self, cachedir, mmap_mode, compress, verbose)"),
d("cache(self, func, ignore, verbose, mmap_mode)"),
d("clear(self, warn)"),
d("eval(self, func)"),
d("__repr__(self)"),
d("__reduce__(self)"),]),
c("NDArrayWrapper(object)", "/externals/joblib/numpy_pickle.py; An object to be persisted instead of numpy arrays.  The only thing this object does, is to carry the filename in which the array has been persisted, and the array subclass.", [
d("__init__(self, filename, subclass, allow_mmap)"),
d("read(self, unpickler)"),]),
c("ZNDArrayWrapper(NDArrayWrapper)", "/externals/joblib/numpy_pickle.py; An object to be persisted instead of numpy arrays.  This object store the Zfile filename in which the data array has been persisted, and the meta information to retrieve it.  The reason that we store the raw buffer data of the array and the meta information, rather than array representation routine (tostring) is that it enables us to use completely the strided model to avoid memory copies (a and a.T store as fast). In addition saving the heavy information separately can avoid creating large temporary buffers when unpickling data with large arrays.", [
d("__init__(self, filename, init_args, state)"),
d("read(self, unpickler)"),]),
c("NumpyPickler(Pickler)", "/externals/joblib/numpy_pickle.py; A pickler to persist of big data efficiently.  The main features of this object are:   * persistence of numpy arrays in separate .npy files, for which    I/O is fast.   * optional compression using Zlib, with a special care on avoid    temporaries.", [
d("__init__(self, filename, compress, cache_size)"),
d("_write_array(self, array, filename)"),
d("save(self, obj)"),
d("save_bytes(self, obj)"),
d("close(self)"),]),
c("NumpyUnpickler(Unpickler)", "/externals/joblib/numpy_pickle.py; A subclass of the Unpickler to unpickle our numpy pickles.     ", [
d("__init__(self, filename, file_handle, mmap_mode)"),
d("_open_pickle(self, file_handle)"),
d("load_build(self)"),]),
c("ZipNumpyUnpickler(NumpyUnpickler)", "/externals/joblib/numpy_pickle.py; A subclass of our Unpickler to unpickle on the fly from compressed storage.", [
d("__init__(self, filename, file_handle)"),
d("_open_pickle(self, file_handle)"),]),
c("Logger(object)", "/externals/joblib/logger.py; Base class for logging messages.     ", [
d("__init__(self, depth)"),
d("warn(self, msg)"),
d("debug(self, msg)"),
d("format(self, obj, indent)"),]),
c("PrintTime(object)", "/externals/joblib/logger.py; Print and log messages while keeping track of time.     ", [
d("__init__(self, logfile, logdir)"),
d("__call__(self, msg, total)"),]),
c("_ConsistentSet(object)", "/externals/joblib/hashing.py; Class used to ensure the hash of Sets is preserved whatever the order of its items.", [
d("__init__(self, set_sequence)"),]),
c("_MyHash(object)", "/externals/joblib/hashing.py; Class used to hash objects that won't normally pickle ", [
d("__init__(self)"),]),
c("Hasher(Pickler)", "/externals/joblib/hashing.py; A subclass of pickler, to do cryptographic hashing, rather than pickling.", [
d("__init__(self, hash_name)"),
d("hash(self, obj, return_digest)"),
d("save(self, obj)"),
d("save_global(self, obj, name, pack)"),
d("_batch_setitems(self, items)"),
d("save_set(self, set_items)"),]),
c("NumpyHasher(Hasher)", "/externals/joblib/hashing.py; Special case the hasher for when numpy is loaded.     ", [
d("__init__(self, hash_name, coerce_mmap)"),
d("save(self, obj)"),]),
c("BatchedCalls(object)", "/externals/joblib/parallel.py; Wrap a sequence of (func, args, kwargs) tuples as a single callable", [
d("__init__(self, iterator_slice)"),
d("__call__(self)"),
d("__len__(self)"),]),
c("WorkerInterrupt(Exception)", "/externals/joblib/parallel.py; An exception that is not KeyboardInterrupt to allow subprocesses to be interrupted.", []),
c("SafeFunction(object)", "/externals/joblib/parallel.py; Wraps a function to make it exception with full traceback in their representation. Useful for parallel computing with multiprocessing, for which exceptions cannot be captured.", [
d("__init__(self, func)"),
d("__call__(self)"),]),
c("ImmediateComputeBatch(object)", "/externals/joblib/parallel.py; Sequential computation of a batch of tasks.  This replicates the async computation API but actually does not delay the computations when joblib.Parallel runs in sequential mode.", [
d("__init__(self, batch)"),
d("get(self)"),]),
c("BatchCompletionCallBack(object)", "/externals/joblib/parallel.py; Callback used by joblib.Parallel's multiprocessing backend.  This callable is executed by the parent process whenever a worker process has returned the results of a batch of tasks.  It is used for progress reporting, to update estimate of the batch processing duration and to schedule the next batch of tasks to be processed.", [
d("__init__(self, dispatch_timestamp, batch_size, parallel)"),
d("__call__(self, out)"),]),
c("Parallel(Logger)", "/externals/joblib/parallel.py; Helper class for readable parallel mapping.  Parameters ----------- n_jobs: int, default: 1     The maximum number of concurrently running jobs, such as the number     of Python worker processes when backend='multiprocessing'     or the size of the thread-pool when backend='threading'.     If -1 all CPUs are used. If 1 is given, no parallel computing code     is used at all, which is useful for debugging. For n_jobs below -1,     (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all     CPUs but one are used. backend: str or None, default: 'multiprocessing'     Specify the parallelization backend implementation.     Supported backends are:       - 'multiprocessing' used by default, can induce some         communication and memory overhead when exchanging input and         output data with the with the worker Python processes.       - 'threading' is a very low-overhead backend but it suffers         from the Python Global Interpreter Lock if the called function         relies a lot on Python objects. 'threading' is mostly useful         when the execution bottleneck is a compiled extension that         explicitly releases the GIL (for instance a Cython loop wrapped         in a 'with nogil' block or an expensive call to a library such         as NumPy). verbose: int, optional     The verbosity level: if non zero, progress messages are     printed. Above 50, the output is sent to stdout.     The frequency of the messages increases with the verbosity level.     If it more than 10, all iterations are reported. pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}     The number of batches (of tasks) to be pre-dispatched.     Default is '2*n_jobs'. When batch_size='auto' this is reasonable     default and the multiprocessing workers shoud never starve. batch_size: int or 'auto', default: 'auto'     The number of atomic tasks to dispatch at once to each     worker. When individual evaluations are very fast, multiprocessing     can be slower than sequential computation because of the overhead.     Batching fast computations together can mitigate this.     The ``'auto'`` strategy keeps track of the time it takes for a batch     to complete, and dynamically adjusts the batch size to keep the time     on the order of half a second, using a heuristic. The initial batch     size is 1.     ``batch_size='auto'`` with ``backend='threading'`` will dispatch     batches of a single task at a time as the threading backend has     very little overhead and using larger batch size has not proved to     bring any gain in that case. temp_folder: str, optional     Folder to be used by the pool for memmaping large arrays     for sharing memory with worker processes. If None, this will try in     order:     - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable,     - /dev/shm if the folder exists and is writable: this is a RAMdisk       filesystem available by default on modern Linux distributions,     - the default system temporary folder that can be overridden       with TMP, TMPDIR or TEMP environment variables, typically /tmp       under Unix operating systems.     Only active when backend='multiprocessing'. max_nbytes int, str, or None, optional, 1M by default     Threshold on the size of arrays passed to the workers that     triggers automated memory mapping in temp_folder. Can be an int     in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.     Use None to disable memmaping of large arrays.     Only active when backend='multiprocessing'.  Notes -----  This object uses the multiprocessing module to compute in parallel the application of a function to many different arguments. The main functionality it brings in addition to using the raw multiprocessing API are (see examples for details):      * More readable code, in particular since it avoids       constructing list of arguments.      * Easier debugging:         - informative tracebacks even when the error happens on           the client side         - using 'n_jobs=1' enables to turn off parallel computing           for debugging without changing the codepath         - early capture of pickling errors      * An optional progress meter.      * Interruption of multiprocesses jobs with 'Ctrl-C'      * Flexible pickling control for the communication to and from       the worker processes.      * Ability to use shared memory efficiently with worker       processes for large numpy-based datastructures.  Examples --------  A simple example:  >>> from math import sqrt >>> from sklearn.externals.joblib import Parallel, delayed >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10)) [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]  Reshaping the output when the function has several return values:  >>> from math import modf >>> from sklearn.externals.joblib import Parallel, delayed >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10)) >>> res, i = zip(*r) >>> res (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5) >>> i (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)  The progress meter: the higher the value of `verbose`, the more messages::      >>> from time import sleep     >>> from sklearn.externals.joblib import Parallel, delayed     >>> r = Parallel(n_jobs=2, verbose=5)(delayed(sleep)(.1) for _ in range(10)) #doctest: +SKIP     [Parallel(n_jobs=2)]: Done   1 out of  10 | elapsed:    0.1s remaining:    0.9s     [Parallel(n_jobs=2)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s     [Parallel(n_jobs=2)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s     [Parallel(n_jobs=2)]: Done   9 out of  10 | elapsed:    0.5s remaining:    0.1s     [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.5s finished  Traceback example, note how the line of the error is indicated as well as the values of the parameter passed to the function that triggered the exception, even though the traceback happens in the child process::   >>> from heapq import nlargest  >>> from sklearn.externals.joblib import Parallel, delayed  >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP  #...  ---------------------------------------------------------------------------  Sub-process traceback:  ---------------------------------------------------------------------------  TypeError                                          Mon Nov 12 11:37:46 2012  PID: 12934                                    Python 2.7.3: /usr/bin/python  ...........................................................................  /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)      419         if n >= size:      420             return sorted(iterable, key=key, reverse=True)[:n]      421      422     # When key is none, use simpler decoration      423     if key is None:  --> 424         it = izip(iterable, count(0,-1))                    # decorate      425         result = _nlargest(n, it)      426         return map(itemgetter(0), result)                   # undecorate      427      428     # General case, slowest method   TypeError: izip argument #1 must support iteration  ___________________________________________________________________________   Using pre_dispatch in a producer/consumer situation, where the data is generated on the fly. Note how the producer is first called a 3 times before the parallel loop is initiated, and then called to generate new data on the fly. In this case the total number of iterations cannot be reported in the progress messages::   >>> from math import sqrt  >>> from sklearn.externals.joblib import Parallel, delayed   >>> def producer():  ...     for i in range(6):  ...         print('Produced %s' % i)  ...         yield i   >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(  ...                         delayed(sqrt)(i) for i in producer()) #doctest: +SKIP  Produced 0  Produced 1  Produced 2  [Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    0.0s  Produced 3  [Parallel(n_jobs=2)]: Done   2 jobs       | elapsed:    0.0s  Produced 4  [Parallel(n_jobs=2)]: Done   3 jobs       | elapsed:    0.0s  Produced 5  [Parallel(n_jobs=2)]: Done   4 jobs       | elapsed:    0.0s  [Parallel(n_jobs=2)]: Done   5 out of   6 | elapsed:    0.0s remaining:    0.0s  [Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed:    0.0s finished", [
d("__init__(self, n_jobs, backend, verbose, pre_dispatch, batch_size, temp_folder, max_nbytes, mmap_mode)"),
d("__enter__(self)"),
d("__exit__(self, exc_type, exc_value, traceback)"),
d("_effective_n_jobs(self)"),
d("_initialize_pool(self)"),
d("_terminate_pool(self)"),
d("_dispatch(self, batch)"),
d("dispatch_next(self)"),
d("dispatch_one_batch(self, iterator)"),
d("_print(self, msg, msg_args)"),
d("print_progress(self)"),
d("retrieve(self)"),
d("__call__(self, iterable)"),
d("__repr__(self)"),]),
]});